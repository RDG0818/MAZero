[2025-03-30 20:23:42,140][train][INFO][test_train_sync_serial.py><module>] ==> Path: /home/tori/Documents/research/MAZero/results/matrix/matgame3/debug/seed=1/2025-03-30 20:23:42
[2025-03-30 20:23:42,140][train][INFO][test_train_sync_serial.py><module>] ==> Param: {'seed': 1, 'discount': 0.997, 'use_wandb': False, 'num_agents': 2, 'image_channel': 1, 'obs_shape': (2, 1, 1), 'action_space_size': 3, 'max_moves': 10, 'test_max_moves': 10, 'selfplay_on_gpu': False, 'data_actors': 1, 'num_pmcts': 1, 'checkpoint_interval': 100, 'total_transitions': 2000, 'start_transitions': 256, 'use_priority': True, 'priority_prob_alpha': 0.6, 'priority_prob_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'use_max_priority': True, 'use_change_temperature': False, 'eps_start': 0.0, 'eps_end': 0.0, 'eps_annealing_time': 1000, 'use_priority_refresh': False, 'refresh_actors': 1, 'refresh_interval': 100, 'refresh_mini_size': 256, 'num_simulations': 50, 'pb_c_base': 19652, 'pb_c_init': 1.25, 'tree_value_stat_delta_lb': 0.01, 'root_dirichlet_alpha': 0.3, 'root_exploration_fraction': 0.25, 'sampled_action_times': 5, 'mcts_rho': 0.75, 'mcts_lambda': 0.8, 'train_on_gpu': True, 'training_steps': 2000, 'last_steps': 100, 'batch_size': 256, 'num_unroll_steps': 5, 'max_grad_norm': 10.0, 'reward_loss_coeff': 1.0, 'value_loss_coeff': 0.25, 'policy_loss_coeff': 1.0, 'consistency_coeff': 2.0, 'awac_lambda': 1.0, 'adv_clip': 3.0, 'lr': 0.02, 'lr_warm_step': 20.0, 'lr_decay_rate': 0.1, 'lr_decay_steps': 2000, 'opti_eps': 1e-05, 'weight_decay': 0, 'reanalyze_on_gpu': False, 'reanalyze_actors': 1, 'reanalyze_update_actors': 0, 'td_steps': 5, 'mini_infer_size': 64, 'use_root_value': False, 'use_pred_value': True, 'use_reanalyze_value': True, 'revisit_policy_search_rate': 1.0, 'use_off_correction': True, 'auto_td_steps': 600.0, 'target_model_interval': 100, 'test_interval': 200, 'test_episodes': 32, 'use_mcts_test': True, 'save_interval': 10000, 'log_interval': 100, 'image_based': False, 'gray_scale': False, 'cvt_string': False, 'frame_skip': 1, 'episode_life': False, 'use_augmentation': False, 'augmentation': ['shift', 'intensity'], 'clip_param': 0.2, 'gae_lambda': 0.95, 'ppo_loss_proportion': 0, 'stacked_observations': 1, 'hidden_state_size': 64, 'fc_representation_layers': [64, 64], 'fc_dynamic_layers': [64, 64], 'fc_reward_layers': [32], 'fc_value_layers': [32], 'fc_policy_layers': [32], 'proj_hid': 128, 'proj_out': 128, 'pred_hid': 64, 'pred_out': 128, 'use_vectorization': False}
[2025-03-30 20:23:43,074][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:10/256
[2025-03-30 20:23:43,339][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:20/256
[2025-03-30 20:23:43,581][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:30/256
[2025-03-30 20:23:43,833][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:40/256
[2025-03-30 20:23:44,097][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:50/256
[2025-03-30 20:23:44,351][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:60/256
[2025-03-30 20:23:44,596][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:70/256
[2025-03-30 20:23:44,861][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:80/256
[2025-03-30 20:23:45,122][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:90/256
[2025-03-30 20:23:45,368][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:100/256
[2025-03-30 20:23:45,624][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:110/256
[2025-03-30 20:23:45,869][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:120/256
[2025-03-30 20:23:46,119][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:130/256
[2025-03-30 20:23:46,403][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:140/256
[2025-03-30 20:23:46,654][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:150/256
[2025-03-30 20:23:46,902][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:160/256
[2025-03-30 20:23:47,167][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:170/256
[2025-03-30 20:23:47,414][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:180/256
[2025-03-30 20:23:47,671][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:190/256
[2025-03-30 20:23:47,937][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:200/256
[2025-03-30 20:23:48,197][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:210/256
[2025-03-30 20:23:48,460][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:220/256
[2025-03-30 20:23:48,715][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:230/256
[2025-03-30 20:23:48,973][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:240/256
[2025-03-30 20:23:49,231][train][DEBUG][train.py>train_sync_serial] ==> ReplayBufferSize:250/256
[2025-03-30 20:23:49,481][train][INFO][train.py>train_sync_serial] ==> Begin training...
[2025-03-30 20:23:50,551][train][INFO][log.py>_log] ==> #0          Episodes Collected: 26         Transitions Collected: 260        Batch Size: 256   | NewEpisode Model(mean:0         ) Reward(mean:-7.73, max:59.00, min:-70.00, std:26.34) | total_loss: 770.683 reward_loss: 349.498 policy_loss: 12.319  value_loss: 1639.515consistency_loss: -0.506  lr: 0.020000  batch_future_return: -4.625  batch_model_diff: 0.000   target_model_diff: 0.000   Tc_perstep: 6.659   Tp_perstep: 0.450   Tu_perstep: 0.450   Te_perstep: 0.169   
[2025-03-30 20:24:37,060][train][INFO][log.py>_log] ==> #100        Episodes Collected: 26         Transitions Collected: 260        Batch Size: 256   | total_loss: 448.592 reward_loss: 242.629 policy_loss: 4.310   value_loss: 829.225 consistency_loss: -2.827  lr: 0.020000  batch_future_return: -4.629  batch_model_diff: 100.000 target_model_diff: 100.000 Tc_perstep: 0.066   Tp_perstep: 0.451   Tu_perstep: 0.018   Te_perstep: 0.002   
[2025-03-30 20:25:22,694][train][INFO][log.py>_log] ==> #200        Episodes Collected: 26         Transitions Collected: 260        Batch Size: 256   | total_loss: 179.783 reward_loss: 92.145  policy_loss: 1.903   value_loss: 353.711 consistency_loss: -1.346  lr: 0.020000  batch_future_return: -4.586  batch_model_diff: 200.000 target_model_diff: 100.000 Tc_perstep: 0.033   Tp_perstep: 0.446   Tu_perstep: 0.016   Te_perstep: 0.002   
[2025-03-30 20:26:15,235][train][INFO][log.py>_log] ==> #300        Episodes Collected: 31         Transitions Collected: 310        Batch Size: 256   | NewEpisode Model(mean:280       ) Reward(mean:35.80, max:100.00, min:-40.00, std:51.21) | total_loss: 196.552 reward_loss: 70.659  policy_loss: 1.145   value_loss: 509.495 consistency_loss: -1.313  lr: 0.020000  batch_future_return: 0.090   batch_model_diff: 248.477 target_model_diff: 100.000 Tc_perstep: 0.026   Tp_perstep: 0.464   Tu_perstep: 0.015   Te_perstep: 0.001   
[2025-03-30 20:27:01,467][train][INFO][log.py>_log] ==> #400        Episodes Collected: 41         Transitions Collected: 410        Batch Size: 256   | NewEpisode Model(mean:355       ) Reward(mean:39.10, max:80.00, min:-22.00, std:34.97) | total_loss: 137.749 reward_loss: 42.033  policy_loss: 1.788   value_loss: 384.053 consistency_loss: -1.043  lr: 0.020000  batch_future_return: 5.914   batch_model_diff: 267.383 target_model_diff: 100.000 Tc_perstep: 0.026   Tp_perstep: 0.453   Tu_perstep: 0.015   Te_perstep: 0.001   
[2025-03-30 20:27:57,640][train][INFO][log.py>_log] ==> #500        Episodes Collected: 51         Transitions Collected: 510        Batch Size: 256   | NewEpisode Model(mean:455       ) Reward(mean:89.90, max:100.00, min:80.00, std:9.90 ) | total_loss: 257.791 reward_loss: 42.301  policy_loss: 0.923   value_loss: 874.671 consistency_loss: -2.050  lr: 0.020000  batch_future_return: 14.211  batch_model_diff: 300.742 target_model_diff: 100.000 Tc_perstep: 0.026   Tp_perstep: 0.467   Tu_perstep: 0.015   Te_perstep: 0.001   
[2025-03-30 20:28:50,779][train][INFO][log.py>_log] ==> #600        Episodes Collected: 61         Transitions Collected: 610        Batch Size: 256   | NewEpisode Model(mean:555       ) Reward(mean:79.70, max:100.00, min:50.00, std:17.56) | total_loss: 138.152 reward_loss: 4.941   policy_loss: 1.671   value_loss: 537.159 consistency_loss: -1.375  lr: 0.020000  batch_future_return: 19.113  batch_model_diff: 349.375 target_model_diff: 100.000 Tc_perstep: 0.026   Tp_perstep: 0.471   Tu_perstep: 0.015   Te_perstep: 0.001   
[2025-03-30 20:29:49,634][train][INFO][log.py>_log] ==> #700        Episodes Collected: 71         Transitions Collected: 710        Batch Size: 256   | NewEpisode Model(mean:655       ) Reward(mean:96.00, max:100.00, min:80.00, std:8.00 ) | total_loss: 124.262 reward_loss: 2.730   policy_loss: 0.616   value_loss: 494.659 consistency_loss: -1.374  lr: 0.020000  batch_future_return: 23.973  batch_model_diff: 391.758 target_model_diff: 100.000 Tc_perstep: 0.026   Tp_perstep: 0.482   Tu_perstep: 0.015   Te_perstep: 0.001   
[2025-03-30 20:30:43,018][train][INFO][log.py>_log] ==> #800        Episodes Collected: 81         Transitions Collected: 810        Batch Size: 256   | NewEpisode Model(mean:755       ) Reward(mean:89.80, max:100.00, min:40.00, std:18.55) | total_loss: 186.402 reward_loss: 22.808  policy_loss: 1.582   value_loss: 664.331 consistency_loss: -2.035  lr: 0.020000  batch_future_return: 24.633  batch_model_diff: 436.602 target_model_diff: 100.000 Tc_perstep: 0.026   Tp_perstep: 0.483   Tu_perstep: 0.014   Te_perstep: 0.001   
[2025-03-30 20:31:48,617][train][INFO][log.py>_log] ==> #900        Episodes Collected: 91         Transitions Collected: 910        Batch Size: 256   | NewEpisode Model(mean:855       ) Reward(mean:82.00, max:100.00, min:40.00, std:18.87) | total_loss: 138.180 reward_loss: 1.712   policy_loss: 0.777   value_loss: 560.853 consistency_loss: -2.261  lr: 0.020000  batch_future_return: 29.559  batch_model_diff: 484.453 target_model_diff: 100.000 Tc_perstep: 0.026   Tp_perstep: 0.498   Tu_perstep: 0.014   Te_perstep: 0.001   
[2025-03-30 20:32:47,248][train][INFO][log.py>_log] ==> #1000       Episodes Collected: 101        Transitions Collected: 1010       Batch Size: 256   | NewEpisode Model(mean:955       ) Reward(mean:87.90, max:100.00, min:60.00, std:15.93) | total_loss: 85.447  reward_loss: 0.353   policy_loss: 0.695   value_loss: 350.113 consistency_loss: -1.565  lr: 0.020000  batch_future_return: 31.812  batch_model_diff: 534.727 target_model_diff: 100.000 Tc_perstep: 0.026   Tp_perstep: 0.503   Tu_perstep: 0.014   Te_perstep: 0.001   
[2025-03-30 20:33:47,056][train][INFO][log.py>_log] ==> #1100       Episodes Collected: 111        Transitions Collected: 1110       Batch Size: 256   | NewEpisode Model(mean:1055      ) Reward(mean:82.80, max:100.00, min:60.00, std:12.58) | total_loss: 67.271  reward_loss: 0.263   policy_loss: 0.470   value_loss: 275.225 consistency_loss: -1.134  lr: 0.020000  batch_future_return: 27.879  batch_model_diff: 641.523 target_model_diff: 100.000 Tc_perstep: 0.026   Tp_perstep: 0.508   Tu_perstep: 0.014   Te_perstep: 0.001   
[2025-03-30 20:34:47,457][train][INFO][log.py>_log] ==> #1200       Episodes Collected: 121        Transitions Collected: 1210       Batch Size: 256   | NewEpisode Model(mean:1155      ) Reward(mean:87.90, max:100.00, min:60.00, std:13.18) | total_loss: 89.722  reward_loss: 0.181   policy_loss: 0.640   value_loss: 369.559 consistency_loss: -1.744  lr: 0.020000  batch_future_return: 25.602  batch_model_diff: 720.000 target_model_diff: 100.000 Tc_perstep: 0.026   Tp_perstep: 0.513   Tu_perstep: 0.014   Te_perstep: 0.001   
[2025-03-30 20:35:48,502][train][INFO][log.py>_log] ==> #1300       Episodes Collected: 131        Transitions Collected: 1310       Batch Size: 256   | NewEpisode Model(mean:1255      ) Reward(mean:86.00, max:100.00, min:50.00, std:16.25) | total_loss: 63.902  reward_loss: 7.916   policy_loss: 0.487   value_loss: 232.255 consistency_loss: -1.282  lr: 0.020000  batch_future_return: 32.762  batch_model_diff: 696.406 target_model_diff: 100.000 Tc_perstep: 0.025   Tp_perstep: 0.517   Tu_perstep: 0.014   Te_perstep: 0.001   
[2025-03-30 20:36:49,509][train][INFO][log.py>_log] ==> #1400       Episodes Collected: 141        Transitions Collected: 1410       Batch Size: 256   | NewEpisode Model(mean:1355      ) Reward(mean:87.00, max:100.00, min:40.00, std:17.92) | total_loss: 25.312  reward_loss: 0.639   policy_loss: 0.306   value_loss: 103.648 consistency_loss: -0.773  lr: 0.020000  batch_future_return: 33.820  batch_model_diff: 740.352 target_model_diff: 100.000 Tc_perstep: 0.025   Tp_perstep: 0.521   Tu_perstep: 0.014   Te_perstep: 0.001   
[2025-03-30 20:37:51,795][train][INFO][log.py>_log] ==> #1500       Episodes Collected: 151        Transitions Collected: 1510       Batch Size: 256   | NewEpisode Model(mean:1455      ) Reward(mean:82.90, max:100.00, min:60.00, std:14.20) | total_loss: 39.610  reward_loss: 0.155   policy_loss: 0.486   value_loss: 165.569 consistency_loss: -1.212  lr: 0.020000  batch_future_return: 33.988  batch_model_diff: 864.336 target_model_diff: 100.000 Tc_perstep: 0.025   Tp_perstep: 0.525   Tu_perstep: 0.014   Te_perstep: 0.001   
[2025-03-30 20:38:52,526][train][INFO][log.py>_log] ==> #1600       Episodes Collected: 161        Transitions Collected: 1610       Batch Size: 256   | NewEpisode Model(mean:1555      ) Reward(mean:84.90, max:100.00, min:60.00, std:15.69) | total_loss: 30.426  reward_loss: 0.830   policy_loss: 0.292   value_loss: 123.481 consistency_loss: -0.783  lr: 0.020000  batch_future_return: 30.500  batch_model_diff: 889.961 target_model_diff: 100.000 Tc_perstep: 0.025   Tp_perstep: 0.528   Tu_perstep: 0.014   Te_perstep: 0.001   
[2025-03-30 20:39:53,333][train][INFO][log.py>_log] ==> #1700       Episodes Collected: 171        Transitions Collected: 1710       Batch Size: 256   | NewEpisode Model(mean:1655      ) Reward(mean:86.70, max:100.00, min:60.00, std:12.55) | total_loss: 31.365  reward_loss: 0.302   policy_loss: 0.381   value_loss: 130.116 consistency_loss: -0.924  lr: 0.020000  batch_future_return: 32.008  batch_model_diff: 971.797 target_model_diff: 100.000 Tc_perstep: 0.025   Tp_perstep: 0.530   Tu_perstep: 0.014   Te_perstep: 0.001   
[2025-03-30 20:40:52,273][train][INFO][log.py>_log] ==> #1800       Episodes Collected: 181        Transitions Collected: 1810       Batch Size: 256   | NewEpisode Model(mean:1755      ) Reward(mean:91.90, max:100.00, min:80.00, std:9.72 ) | total_loss: 23.343  reward_loss: 1.317   policy_loss: 0.307   value_loss: 92.853  consistency_loss: -0.747  lr: 0.020000  batch_future_return: 32.711  batch_model_diff: 1019.414target_model_diff: 100.000 Tc_perstep: 0.025   Tp_perstep: 0.531   Tu_perstep: 0.014   Te_perstep: 0.001   
[2025-03-30 20:41:52,269][train][INFO][log.py>_log] ==> #1900       Episodes Collected: 191        Transitions Collected: 1910       Batch Size: 256   | NewEpisode Model(mean:1855      ) Reward(mean:86.80, max:100.00, min:79.00, std:9.16 ) | total_loss: 53.882  reward_loss: 5.265   policy_loss: 0.654   value_loss: 204.742 consistency_loss: -1.611  lr: 0.020000  batch_future_return: 31.680  batch_model_diff: 1087.617target_model_diff: 100.000 Tc_perstep: 0.025   Tp_perstep: 0.533   Tu_perstep: 0.014   Te_perstep: 0.001   
[2025-03-30 20:42:49,192][train][INFO][log.py>_log] ==> #2000       Episodes Collected: 200        Transitions Collected: 2000       Batch Size: 256   | NewEpisode Model(mean:1950      ) Reward(mean:92.11, max:100.00, min:69.00, std:11.55) | total_loss: 31.175  reward_loss: 0.329   policy_loss: 0.544   value_loss: 132.049 consistency_loss: -1.355  lr: 0.020000  batch_future_return: 33.949  batch_model_diff: 1089.648target_model_diff: 100.000 Tc_perstep: 0.025   Tp_perstep: 0.533   Tu_perstep: 0.014   Te_perstep: 0.001   
[2025-03-30 20:43:42,061][train][INFO][train.py>train_sync_serial] ==> Training over...
