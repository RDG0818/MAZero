[2025-03-31 09:26:01,785][train][INFO][test_train_sync_parallel.py><module>] ==> Path: /home/tori/Documents/research/MAZero/results/matrix/matgame3/debug/seed=1/2025-03-31 09:26:01
[2025-03-31 09:26:01,785][train][INFO][test_train_sync_parallel.py><module>] ==> Param: {'seed': 1, 'discount': 0.997, 'use_wandb': False, 'num_agents': 2, 'image_channel': 1, 'obs_shape': (2, 1, 1), 'action_space_size': 3, 'max_moves': 10, 'test_max_moves': 10, 'selfplay_on_gpu': False, 'data_actors': 1, 'num_pmcts': 1, 'checkpoint_interval': 100, 'total_transitions': 2000, 'start_transitions': 256, 'use_priority': True, 'priority_prob_alpha': 0.6, 'priority_prob_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'use_max_priority': True, 'use_change_temperature': False, 'eps_start': 0.0, 'eps_end': 0.0, 'eps_annealing_time': 1000, 'use_priority_refresh': False, 'refresh_actors': 1, 'refresh_interval': 100, 'refresh_mini_size': 256, 'num_simulations': 50, 'pb_c_base': 19652, 'pb_c_init': 1.25, 'tree_value_stat_delta_lb': 0.01, 'root_dirichlet_alpha': 0.3, 'root_exploration_fraction': 0.25, 'sampled_action_times': 5, 'mcts_rho': 0.75, 'mcts_lambda': 0.8, 'train_on_gpu': True, 'training_steps': 2000, 'last_steps': 100, 'batch_size': 256, 'num_unroll_steps': 5, 'max_grad_norm': 10.0, 'reward_loss_coeff': 1.0, 'value_loss_coeff': 0.25, 'policy_loss_coeff': 1.0, 'consistency_coeff': 2.0, 'awac_lambda': 1.0, 'adv_clip': 3.0, 'lr': 0.02, 'lr_warm_step': 20.0, 'lr_decay_rate': 0.1, 'lr_decay_steps': 2000, 'opti_eps': 1e-05, 'weight_decay': 0, 'reanalyze_on_gpu': False, 'reanalyze_actors': 10, 'reanalyze_update_actors': 0, 'td_steps': 5, 'mini_infer_size': 64, 'use_root_value': False, 'use_pred_value': True, 'use_reanalyze_value': True, 'revisit_policy_search_rate': 1.0, 'use_off_correction': True, 'auto_td_steps': 600.0, 'target_model_interval': 100, 'test_interval': 200, 'test_episodes': 32, 'use_mcts_test': True, 'save_interval': 10000, 'log_interval': 100, 'image_based': False, 'gray_scale': False, 'cvt_string': False, 'frame_skip': 1, 'episode_life': False, 'use_augmentation': False, 'augmentation': ['shift', 'intensity'], 'clip_param': 0.2, 'gae_lambda': 0.95, 'ppo_loss_proportion': 0, 'stacked_observations': 1, 'hidden_state_size': 64, 'fc_representation_layers': [64, 64], 'fc_dynamic_layers': [64, 64], 'fc_reward_layers': [32], 'fc_value_layers': [32], 'fc_policy_layers': [32], 'proj_hid': 128, 'proj_out': 128, 'pred_hid': 64, 'pred_out': 128, 'use_vectorization': False}
[2025-03-31 09:26:02,943][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:10/256
[2025-03-31 09:26:03,221][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:20/256
[2025-03-31 09:26:03,475][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:30/256
[2025-03-31 09:26:03,741][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:40/256
[2025-03-31 09:26:04,015][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:50/256
[2025-03-31 09:26:04,275][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:60/256
[2025-03-31 09:26:04,526][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:70/256
[2025-03-31 09:26:04,780][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:80/256
[2025-03-31 09:26:05,056][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:90/256
[2025-03-31 09:26:05,322][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:100/256
[2025-03-31 09:26:05,579][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:110/256
[2025-03-31 09:26:05,848][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:120/256
[2025-03-31 09:26:06,100][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:130/256
[2025-03-31 09:26:06,350][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:140/256
[2025-03-31 09:26:06,607][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:150/256
[2025-03-31 09:26:06,882][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:160/256
[2025-03-31 09:26:07,135][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:170/256
[2025-03-31 09:26:07,390][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:180/256
[2025-03-31 09:26:07,641][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:190/256
[2025-03-31 09:26:07,954][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:200/256
[2025-03-31 09:26:08,209][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:210/256
[2025-03-31 09:26:08,478][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:220/256
[2025-03-31 09:26:08,722][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:230/256
[2025-03-31 09:26:08,982][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:240/256
[2025-03-31 09:26:09,249][train][DEBUG][train.py>train_sync_parallel] ==> ReplayBufferSize:250/256
[2025-03-31 09:26:09,498][train][INFO][train.py>train_sync_parallel] ==> Begin training...
[2025-03-31 09:26:10,818][train][INFO][log.py>_log] ==> #0          Episodes Collected: 26         Transitions Collected: 260        Batch Size: 256   | NewEpisode Model(mean:0         ) Reward(mean:-7.73, max:59.00, min:-70.00, std:26.34) | total_loss: 770.683 reward_loss: 349.498 policy_loss: 12.319  value_loss: 1639.515consistency_loss: -0.506  lr: 0.020000  batch_future_return: -4.625  batch_model_diff: 0.000   target_model_diff: 0.000   Tc_perstep: 6.824   Tp_perstep: 0.797   Tu_perstep: 0.459   Te_perstep: 0.000   
[2025-03-31 09:26:19,296][train][INFO][log.py>_log] ==> #100        Episodes Collected: 26         Transitions Collected: 260        Batch Size: 256   | total_loss: 574.044 reward_loss: 181.360 policy_loss: 7.373   value_loss: 1580.017consistency_loss: -4.846  lr: 0.020000  batch_future_return: -4.473  batch_model_diff: 100.000 target_model_diff: 100.000 Tc_perstep: 0.068   Tp_perstep: 0.059   Tu_perstep: 0.037   Te_perstep: 0.000   
[2025-03-31 09:26:27,803][train][INFO][log.py>_log] ==> #200        Episodes Collected: 26         Transitions Collected: 260        Batch Size: 256   | total_loss: 574.416 reward_loss: 180.749 policy_loss: 7.522   value_loss: 1578.917consistency_loss: -4.292  lr: 0.020000  batch_future_return: -4.477  batch_model_diff: 200.000 target_model_diff: 100.000 Tc_perstep: 0.034   Tp_perstep: 0.056   Tu_perstep: 0.035   Te_perstep: 0.000   
[2025-03-31 09:26:39,967][train][INFO][log.py>_log] ==> #300        Episodes Collected: 31         Transitions Collected: 310        Batch Size: 256   | NewEpisode Model(mean:280       ) Reward(mean:92.00, max:100.00, min:80.00, std:9.80 ) | total_loss: 226.850 reward_loss: 24.391  policy_loss: 1.588   value_loss: 819.356 consistency_loss: -1.984  lr: 0.020000  batch_future_return: 5.855   batch_model_diff: 247.383 target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.065   Tu_perstep: 0.032   Te_perstep: 0.000   
[2025-03-31 09:26:53,730][train][INFO][log.py>_log] ==> #400        Episodes Collected: 41         Transitions Collected: 410        Batch Size: 256   | NewEpisode Model(mean:355       ) Reward(mean:90.60, max:100.00, min:60.00, std:12.96) | total_loss: 114.272 reward_loss: 3.397   policy_loss: 1.230   value_loss: 449.096 consistency_loss: -1.315  lr: 0.020000  batch_future_return: 15.316  batch_model_diff: 266.602 target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.072   Tu_perstep: 0.028   Te_perstep: 0.000   
[2025-03-31 09:27:08,556][train][INFO][log.py>_log] ==> #500        Episodes Collected: 51         Transitions Collected: 510        Batch Size: 256   | NewEpisode Model(mean:455       ) Reward(mean:88.70, max:100.00, min:59.00, std:13.06) | total_loss: 67.617  reward_loss: 1.792   policy_loss: 0.400   value_loss: 268.718 consistency_loss: -0.877  lr: 0.020000  batch_future_return: 20.938  batch_model_diff: 307.227 target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.078   Tu_perstep: 0.026   Te_perstep: 0.000   
[2025-03-31 09:27:22,934][train][INFO][log.py>_log] ==> #600        Episodes Collected: 61         Transitions Collected: 610        Batch Size: 256   | NewEpisode Model(mean:555       ) Reward(mean:86.80, max:100.00, min:70.00, std:10.20) | total_loss: 143.095 reward_loss: 5.459   policy_loss: 1.225   value_loss: 561.057 consistency_loss: -1.927  lr: 0.020000  batch_future_return: 22.922  batch_model_diff: 362.930 target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.081   Tu_perstep: 0.024   Te_perstep: 0.000   
[2025-03-31 09:27:38,287][train][INFO][log.py>_log] ==> #700        Episodes Collected: 71         Transitions Collected: 710        Batch Size: 256   | NewEpisode Model(mean:655       ) Reward(mean:96.90, max:100.00, min:80.00, std:6.36 ) | total_loss: 89.912  reward_loss: 1.437   policy_loss: 0.700   value_loss: 363.921 consistency_loss: -1.602  lr: 0.020000  batch_future_return: 28.508  batch_model_diff: 428.008 target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.085   Tu_perstep: 0.023   Te_perstep: 0.000   
[2025-03-31 09:27:53,478][train][INFO][log.py>_log] ==> #800        Episodes Collected: 81         Transitions Collected: 810        Batch Size: 256   | NewEpisode Model(mean:755       ) Reward(mean:87.80, max:100.00, min:60.00, std:13.39) | total_loss: 71.261  reward_loss: 0.739   policy_loss: 0.812   value_loss: 292.102 consistency_loss: -1.658  lr: 0.020000  batch_future_return: 30.305  batch_model_diff: 464.570 target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.087   Tu_perstep: 0.022   Te_perstep: 0.000   
[2025-03-31 09:28:08,992][train][INFO][log.py>_log] ==> #900        Episodes Collected: 91         Transitions Collected: 910        Batch Size: 256   | NewEpisode Model(mean:855       ) Reward(mean:86.00, max:100.00, min:40.00, std:18.00) | total_loss: 65.445  reward_loss: 1.315   policy_loss: 0.587   value_loss: 265.288 consistency_loss: -1.390  lr: 0.020000  batch_future_return: 28.172  batch_model_diff: 528.359 target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.090   Tu_perstep: 0.022   Te_perstep: 0.000   
[2025-03-31 09:28:24,670][train][INFO][log.py>_log] ==> #1000       Episodes Collected: 101        Transitions Collected: 1010       Batch Size: 256   | NewEpisode Model(mean:955       ) Reward(mean:83.80, max:100.00, min:59.00, std:15.16) | total_loss: 86.869  reward_loss: 0.327   policy_loss: 1.056   value_loss: 358.210 consistency_loss: -2.033  lr: 0.020000  batch_future_return: 33.320  batch_model_diff: 568.477 target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.092   Tu_perstep: 0.021   Te_perstep: 0.000   
[2025-03-31 09:28:40,063][train][INFO][log.py>_log] ==> #1100       Episodes Collected: 111        Transitions Collected: 1110       Batch Size: 256   | NewEpisode Model(mean:1055      ) Reward(mean:80.80, max:100.00, min:50.00, std:15.09) | total_loss: 29.791  reward_loss: 1.704   policy_loss: 0.384   value_loss: 117.491 consistency_loss: -0.835  lr: 0.020000  batch_future_return: 32.691  batch_model_diff: 626.367 target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.093   Tu_perstep: 0.021   Te_perstep: 0.000   
[2025-03-31 09:28:55,299][train][INFO][log.py>_log] ==> #1200       Episodes Collected: 121        Transitions Collected: 1210       Batch Size: 256   | NewEpisode Model(mean:1155      ) Reward(mean:87.80, max:100.00, min:60.00, std:13.09) | total_loss: 73.350  reward_loss: 5.392   policy_loss: 0.884   value_loss: 283.692 consistency_loss: -1.924  lr: 0.020000  batch_future_return: 31.484  batch_model_diff: 686.992 target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.094   Tu_perstep: 0.021   Te_perstep: 0.000   
[2025-03-31 09:29:10,840][train][INFO][log.py>_log] ==> #1300       Episodes Collected: 131        Transitions Collected: 1310       Batch Size: 256   | NewEpisode Model(mean:1255      ) Reward(mean:88.00, max:100.00, min:70.00, std:12.49) | total_loss: 72.223  reward_loss: 1.730   policy_loss: 1.000   value_loss: 296.100 consistency_loss: -2.266  lr: 0.020000  batch_future_return: 35.352  batch_model_diff: 726.953 target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.095   Tu_perstep: 0.020   Te_perstep: 0.000   
[2025-03-31 09:29:26,498][train][INFO][log.py>_log] ==> #1400       Episodes Collected: 141        Transitions Collected: 1410       Batch Size: 256   | NewEpisode Model(mean:1355      ) Reward(mean:86.90, max:100.00, min:40.00, std:17.85) | total_loss: 18.416  reward_loss: 1.073   policy_loss: 0.236   value_loss: 73.172  consistency_loss: -0.593  lr: 0.020000  batch_future_return: 36.965  batch_model_diff: 735.547 target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.096   Tu_perstep: 0.020   Te_perstep: 0.000   
[2025-03-31 09:29:42,485][train][INFO][log.py>_log] ==> #1500       Episodes Collected: 151        Transitions Collected: 1510       Batch Size: 256   | NewEpisode Model(mean:1455      ) Reward(mean:82.50, max:100.00, min:59.00, std:10.97) | total_loss: 37.538  reward_loss: 4.775   policy_loss: 0.496   value_loss: 138.862 consistency_loss: -1.224  lr: 0.020000  batch_future_return: 38.164  batch_model_diff: 773.242 target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.097   Tu_perstep: 0.020   Te_perstep: 0.000   
[2025-03-31 09:29:58,213][train][INFO][log.py>_log] ==> #1600       Episodes Collected: 161        Transitions Collected: 1610       Batch Size: 256   | NewEpisode Model(mean:1555      ) Reward(mean:83.90, max:100.00, min:60.00, std:17.46) | total_loss: 62.279  reward_loss: 0.150   policy_loss: 1.001   value_loss: 263.495 consistency_loss: -2.373  lr: 0.020000  batch_future_return: 36.086  batch_model_diff: 950.391 target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.097   Tu_perstep: 0.020   Te_perstep: 0.000   
[2025-03-31 09:30:14,013][train][INFO][log.py>_log] ==> #1700       Episodes Collected: 171        Transitions Collected: 1710       Batch Size: 256   | NewEpisode Model(mean:1655      ) Reward(mean:82.90, max:100.00, min:60.00, std:14.20) | total_loss: 42.103  reward_loss: 0.103   policy_loss: 0.797   value_loss: 178.906 consistency_loss: -1.762  lr: 0.020000  batch_future_return: 41.195  batch_model_diff: 939.336 target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.098   Tu_perstep: 0.020   Te_perstep: 0.000   
[2025-03-31 09:30:30,276][train][INFO][log.py>_log] ==> #1800       Episodes Collected: 181        Transitions Collected: 1810       Batch Size: 256   | NewEpisode Model(mean:1755      ) Reward(mean:91.90, max:100.00, min:80.00, std:9.72 ) | total_loss: 32.649  reward_loss: 1.134   policy_loss: 0.444   value_loss: 130.329 consistency_loss: -0.756  lr: 0.020000  batch_future_return: 36.699  batch_model_diff: 1002.500target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.099   Tu_perstep: 0.019   Te_perstep: 0.000   
[2025-03-31 09:30:46,061][train][INFO][log.py>_log] ==> #1900       Episodes Collected: 191        Transitions Collected: 1910       Batch Size: 256   | NewEpisode Model(mean:1855      ) Reward(mean:85.70, max:100.00, min:79.00, std:9.15 ) | total_loss: 14.860  reward_loss: 0.128   policy_loss: 0.358   value_loss: 63.973  consistency_loss: -0.809  lr: 0.020000  batch_future_return: 38.734  batch_model_diff: 1003.828target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.099   Tu_perstep: 0.019   Te_perstep: 0.000   
[2025-03-31 09:31:01,393][train][INFO][log.py>_log] ==> #2000       Episodes Collected: 200        Transitions Collected: 2000       Batch Size: 256   | NewEpisode Model(mean:1950      ) Reward(mean:92.22, max:100.00, min:70.00, std:11.33) | total_loss: 3.058   reward_loss: 0.007   policy_loss: 0.078   value_loss: 13.321  consistency_loss: -0.178  lr: 0.020000  batch_future_return: 39.297  batch_model_diff: 1024.102target_model_diff: 100.000 Tc_perstep: 0.027   Tp_perstep: 0.100   Tu_perstep: 0.019   Te_perstep: 0.000   
[2025-03-31 09:31:11,676][train][INFO][train.py>train_sync_parallel] ==> Training over...
