[2025-06-02 20:03:40,882][train][INFO][main.py><module>] ==> Path: /home/tori/Documents/research/MAZero_PIC/results/smac/3m/3m_seed0_rho0.75_lambda3_clip3.0/seed=0/2025-06-02 20:03:40
[2025-06-02 20:03:40,882][train][INFO][main.py><module>] ==> Param: {'seed': 0, 'discount': 0.99, 'use_wandb': False, 'num_agents': 3, 'image_channel': 1, 'obs_shape': (64, 1, 1), 'action_space_size': 9, 'max_moves': 60, 'test_max_moves': 60, 'selfplay_on_gpu': True, 'data_actors': 1, 'num_pmcts': 4, 'checkpoint_interval': 100, 'total_transitions': 2000000, 'start_transitions': 500, 'use_priority': True, 'priority_prob_alpha': 0.6, 'priority_prob_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'use_max_priority': True, 'use_change_temperature': False, 'eps_start': 0.0, 'eps_end': 0.0, 'eps_annealing_time': 1000, 'use_priority_refresh': False, 'refresh_actors': 1, 'refresh_interval': 100, 'refresh_mini_size': 256, 'num_simulations': 10, 'pb_c_base': 19652, 'pb_c_init': 1.25, 'tree_value_stat_delta_lb': 0.01, 'root_dirichlet_alpha': 0.3, 'root_exploration_fraction': 0.25, 'sampled_action_times': 1, 'mcts_rho': 0.75, 'mcts_lambda': 0.8, 'train_on_gpu': True, 'training_steps': 49000, 'last_steps': 1000, 'batch_size': 256, 'num_unroll_steps': 5, 'max_grad_norm': 5.0, 'reward_loss_coeff': 1.0, 'value_loss_coeff': 0.25, 'policy_loss_coeff': 1.0, 'consistency_coeff': 2.0, 'awac_lambda': 3.0, 'adv_clip': 3.0, 'lr': 0.0001, 'lr_warm_step': 490.0, 'lr_decay_rate': 0.1, 'lr_decay_steps': 49000, 'opti_eps': 1e-05, 'weight_decay': 0, 'reanalyze_on_gpu': True, 'reanalyze_actors': 4, 'reanalyze_update_actors': 0, 'td_steps': 5, 'mini_infer_size': 64, 'use_root_value': False, 'use_pred_value': True, 'use_reanalyze_value': True, 'revisit_policy_search_rate': 1.0, 'use_off_correction': True, 'auto_td_steps': 14700.0, 'target_model_interval': 200, 'test_interval': 500, 'test_episodes': 32, 'use_mcts_test': True, 'save_interval': 10000, 'log_interval': 100, 'image_based': False, 'gray_scale': False, 'cvt_string': False, 'frame_skip': 1, 'episode_life': False, 'use_augmentation': False, 'augmentation': ['shift', 'intensity'], 'clip_param': 0.2, 'gae_lambda': 0.95, 'ppo_loss_proportion': 0, 'stacked_observations': 1, 'hidden_state_size': 128, 'fc_representation_layers': [128, 128], 'fc_dynamic_layers': [128, 128], 'fc_reward_layers': [32], 'fc_value_layers': [32], 'fc_policy_layers': [32], 'proj_hid': 128, 'proj_out': 128, 'pred_hid': 64, 'pred_out': 128, 'use_vectorization': True}
[2025-06-02 20:03:42,162][train][DEBUG][train.py>_train] ==> ReplayBufferSize:0/500
[2025-06-02 20:03:45,166][train][DEBUG][train.py>_train] ==> ReplayBufferSize:0/500
[2025-06-02 20:03:48,170][train][DEBUG][train.py>_train] ==> ReplayBufferSize:0/500
[2025-06-02 20:03:51,174][train][DEBUG][train.py>_train] ==> ReplayBufferSize:0/500
[2025-06-02 20:03:54,178][train][DEBUG][train.py>_train] ==> ReplayBufferSize:0/500
[2025-06-02 20:03:57,182][train][DEBUG][train.py>_train] ==> ReplayBufferSize:0/500
[2025-06-02 20:04:00,186][train][DEBUG][train.py>_train] ==> ReplayBufferSize:0/500
[2025-06-02 20:04:03,190][train][DEBUG][train.py>_train] ==> ReplayBufferSize:95/500
[2025-06-02 20:04:06,194][train][DEBUG][train.py>_train] ==> ReplayBufferSize:234/500
[2025-06-02 20:04:09,198][train][DEBUG][train.py>_train] ==> ReplayBufferSize:391/500
[2025-06-02 20:04:12,202][train][DEBUG][train.py>_train] ==> ReplayBufferSize:585/500
[2025-06-02 20:04:12,202][train][INFO][train.py>_train] ==> Begin training...
[2025-06-02 20:04:14,927][train][INFO][log.py>_log] ==> #0          Episodes Collected: 27         Transitions Collected: 658        Batch Size: 256   | NewEpisode Model(mean:0         ) Reward(mean:1.40 , max:2.63 , min:0.00 , std:0.52 ) | total_loss: 42.905  reward_loss: 1.861   policy_loss: 37.001  value_loss: 11.364  consistency_loss: 0.602   lr: 0.000100  batch_future_return: 0.846   batch_model_diff: 0.000   target_model_diff: 0.000   Tp_perstep: 2.012   Tu_perstep: 0.706   
[2025-06-02 20:04:32,248][train][INFO][log.py>_log] ==> #100        Episodes Collected: 32         Transitions Collected: 790        Batch Size: 256   | NewEpisode Model(mean:0         ) Reward(mean:1.25 , max:2.63 , min:0.33 , std:0.82 ) | total_loss: 23.682  reward_loss: 0.956   policy_loss: 25.177  value_loss: 5.000   consistency_loss: -1.851  lr: 0.000100  batch_future_return: 0.948   batch_model_diff: 100.000 target_model_diff: 100.000 Tp_perstep: 0.147   Tu_perstep: 0.051   
[2025-06-02 20:04:50,834][train][INFO][log.py>_log] ==> #200        Episodes Collected: 47         Transitions Collected: 1208       Batch Size: 256   | NewEpisode Model(mean:85        ) Reward(mean:1.36 , max:2.30 , min:0.33 , std:0.46 ) | total_loss: 18.691  reward_loss: 0.536   policy_loss: 22.421  value_loss: 3.054   consistency_loss: -2.515  lr: 0.000100  batch_future_return: 0.789   batch_model_diff: 174.219 target_model_diff: 200.000 Tp_perstep: 0.140   Tu_perstep: 0.052   
[2025-06-02 20:04:59,716][train][WARNING][train.py>_train] ==> #250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:05:09,274][train][INFO][log.py>_log] ==> #300        Episodes Collected: 59         Transitions Collected: 1508       Batch Size: 256   | NewEpisode Model(mean:189       ) Reward(mean:1.15 , max:1.97 , min:0.33 , std:0.46 ) | total_loss: 14.594  reward_loss: 0.343   policy_loss: 19.191  value_loss: 2.136   consistency_loss: -2.737  lr: 0.000100  batch_future_return: 0.810   batch_model_diff: 234.375 target_model_diff: 100.000 Tp_perstep: 0.136   Tu_perstep: 0.053   
[2025-06-02 20:05:28,315][train][INFO][log.py>_log] ==> #400        Episodes Collected: 74         Transitions Collected: 1902       Batch Size: 256   | NewEpisode Model(mean:279       ) Reward(mean:1.12 , max:1.97 , min:0.33 , std:0.53 ) | total_loss: 11.097  reward_loss: 0.256   policy_loss: 15.524  value_loss: 1.568   consistency_loss: -2.537  lr: 0.000100  batch_future_return: 0.831   batch_model_diff: 304.297 target_model_diff: 200.000 Tp_perstep: 0.136   Tu_perstep: 0.054   
[2025-06-02 20:05:28,318][train][WARNING][train.py>_train] ==> #400 Batch Queue is excess (Reduce reanalyze actors).
[2025-06-02 20:05:47,581][train][INFO][log.py>_log] ==> #500        Episodes Collected: 90         Transitions Collected: 2310       Batch Size: 256   | NewEpisode Model(mean:384       ) Reward(mean:1.21 , max:2.30 , min:0.33 , std:0.59 ) | total_loss: 13.626  reward_loss: 0.327   policy_loss: 19.619  value_loss: 2.259   consistency_loss: -3.442  lr: 0.000100  batch_future_return: 0.815   batch_model_diff: 337.109 target_model_diff: 100.000 Tp_perstep: 0.136   Tu_perstep: 0.054   
[2025-06-02 20:05:57,045][train][WARNING][train.py>_train] ==> #550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:06:06,342][train][INFO][log.py>_log] ==> #600        Episodes Collected: 103        Transitions Collected: 2663       Batch Size: 256   | NewEpisode Model(mean:485       ) Reward(mean:0.78 , max:1.64 , min:0.33 , std:0.35 ) | total_loss: 12.223  reward_loss: 0.274   policy_loss: 18.575  value_loss: 2.034   consistency_loss: -3.567  lr: 0.000100  batch_future_return: 0.705   batch_model_diff: 402.344 target_model_diff: 200.000 Tp_perstep: 0.136   Tu_perstep: 0.054   
[2025-06-02 20:06:24,581][train][INFO][log.py>_log] ==> #700        Episodes Collected: 116        Transitions Collected: 3034       Batch Size: 256   | NewEpisode Model(mean:572       ) Reward(mean:0.89 , max:1.64 , min:0.00 , std:0.45 ) | total_loss: 11.954  reward_loss: 0.280   policy_loss: 18.369  value_loss: 2.037   consistency_loss: -3.602  lr: 0.000100  batch_future_return: 0.702   batch_model_diff: 427.344 target_model_diff: 100.000 Tp_perstep: 0.135   Tu_perstep: 0.054   
[2025-06-02 20:06:42,965][train][INFO][log.py>_log] ==> #800        Episodes Collected: 131        Transitions Collected: 3401       Batch Size: 256   | NewEpisode Model(mean:680       ) Reward(mean:1.03 , max:1.64 , min:0.33 , std:0.36 ) | total_loss: 11.655  reward_loss: 0.278   policy_loss: 18.298  value_loss: 2.002   consistency_loss: -3.711  lr: 0.000100  batch_future_return: 0.686   batch_model_diff: 522.266 target_model_diff: 200.000 Tp_perstep: 0.135   Tu_perstep: 0.054   
[2025-06-02 20:06:42,967][train][WARNING][train.py>_train] ==> #800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:07:02,321][train][INFO][log.py>_log] ==> #900        Episodes Collected: 146        Transitions Collected: 3824       Batch Size: 256   | NewEpisode Model(mean:786       ) Reward(mean:1.10 , max:2.30 , min:0.33 , std:0.56 ) | total_loss: 10.420  reward_loss: 0.267   policy_loss: 16.160  value_loss: 1.953   consistency_loss: -3.247  lr: 0.000100  batch_future_return: 0.710   batch_model_diff: 541.406 target_model_diff: 100.000 Tp_perstep: 0.136   Tu_perstep: 0.053   
[2025-06-02 20:07:20,842][train][INFO][log.py>_log] ==> #1000       Episodes Collected: 160        Transitions Collected: 4208       Batch Size: 256   | NewEpisode Model(mean:883       ) Reward(mean:0.94 , max:1.64 , min:0.33 , std:0.43 ) | total_loss: 12.508  reward_loss: 0.251   policy_loss: 19.364  value_loss: 2.324   consistency_loss: -3.844  lr: 0.000100  batch_future_return: 0.638   batch_model_diff: 605.078 target_model_diff: 200.000 Tp_perstep: 0.135   Tu_perstep: 0.053   
[2025-06-02 20:07:20,844][train][WARNING][train.py>_train] ==> #1000 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:07:40,293][train][INFO][log.py>_log] ==> #1100       Episodes Collected: 176        Transitions Collected: 4688       Batch Size: 256   | NewEpisode Model(mean:988       ) Reward(mean:1.09 , max:1.97 , min:0.33 , std:0.36 ) | total_loss: 8.040   reward_loss: 0.186   policy_loss: 12.661  value_loss: 1.568   consistency_loss: -2.599  lr: 0.000100  batch_future_return: 0.683   batch_model_diff: 657.812 target_model_diff: 100.000 Tp_perstep: 0.136   Tu_perstep: 0.053   
[2025-06-02 20:07:58,855][train][INFO][log.py>_log] ==> #1200       Episodes Collected: 186        Transitions Collected: 4980       Batch Size: 256   | NewEpisode Model(mean:1096      ) Reward(mean:0.82 , max:1.32 , min:0.00 , std:0.40 ) | total_loss: 9.793   reward_loss: 0.208   policy_loss: 15.394  value_loss: 1.885   consistency_loss: -3.140  lr: 0.000100  batch_future_return: 0.634   batch_model_diff: 660.547 target_model_diff: 200.000 Tp_perstep: 0.136   Tu_perstep: 0.053   
[2025-06-02 20:08:17,185][train][INFO][log.py>_log] ==> #1300       Episodes Collected: 200        Transitions Collected: 5417       Batch Size: 256   | NewEpisode Model(mean:1181      ) Reward(mean:0.92 , max:1.97 , min:0.00 , std:0.52 ) | total_loss: 9.403   reward_loss: 0.185   policy_loss: 14.945  value_loss: 1.901   consistency_loss: -3.101  lr: 0.000100  batch_future_return: 0.619   batch_model_diff: 706.641 target_model_diff: 100.000 Tp_perstep: 0.136   Tu_perstep: 0.053   
[2025-06-02 20:08:37,598][train][INFO][log.py>_log] ==> #1400       Episodes Collected: 216        Transitions Collected: 5902       Batch Size: 256   | NewEpisode Model(mean:1291      ) Reward(mean:1.25 , max:2.63 , min:0.00 , std:0.66 ) | total_loss: 8.111   reward_loss: 0.184   policy_loss: 13.200  value_loss: 1.687   consistency_loss: -2.847  lr: 0.000100  batch_future_return: 0.587   batch_model_diff: 804.688 target_model_diff: 200.000 Tp_perstep: 0.137   Tu_perstep: 0.053   
[2025-06-02 20:08:56,927][train][INFO][log.py>_log] ==> #1500       Episodes Collected: 231        Transitions Collected: 6293       Batch Size: 256   | NewEpisode Model(mean:1391      ) Reward(mean:1.07 , max:2.30 , min:0.66 , std:0.47 ) | total_loss: 7.487   reward_loss: 0.146   policy_loss: 12.171  value_loss: 1.584   consistency_loss: -2.613  lr: 0.000100  batch_future_return: 0.623   batch_model_diff: 754.297 target_model_diff: 100.000 Tp_perstep: 0.137   Tu_perstep: 0.052   
[2025-06-02 20:09:15,979][train][INFO][log.py>_log] ==> #1600       Episodes Collected: 245        Transitions Collected: 6742       Batch Size: 256   | NewEpisode Model(mean:1483      ) Reward(mean:0.89 , max:2.30 , min:0.33 , std:0.63 ) | total_loss: 8.179   reward_loss: 0.174   policy_loss: 13.402  value_loss: 1.688   consistency_loss: -2.909  lr: 0.000100  batch_future_return: 0.577   batch_model_diff: 892.578 target_model_diff: 200.000 Tp_perstep: 0.137   Tu_perstep: 0.052   
[2025-06-02 20:09:36,293][train][INFO][log.py>_log] ==> #1700       Episodes Collected: 260        Transitions Collected: 7177       Batch Size: 256   | NewEpisode Model(mean:1589      ) Reward(mean:0.90 , max:2.30 , min:0.33 , std:0.49 ) | total_loss: 5.980   reward_loss: 0.130   policy_loss: 9.572   value_loss: 1.237   consistency_loss: -2.016  lr: 0.000100  batch_future_return: 0.660   batch_model_diff: 931.641 target_model_diff: 100.000 Tp_perstep: 0.138   Tu_perstep: 0.052   
[2025-06-02 20:09:36,296][train][WARNING][train.py>_train] ==> #1700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:09:55,367][train][INFO][log.py>_log] ==> #1800       Episodes Collected: 274        Transitions Collected: 7594       Batch Size: 256   | NewEpisode Model(mean:1686      ) Reward(mean:1.15 , max:1.97 , min:0.00 , std:0.59 ) | total_loss: 6.004   reward_loss: 0.111   policy_loss: 9.580   value_loss: 1.259   consistency_loss: -2.001  lr: 0.000100  batch_future_return: 0.624   batch_model_diff: 908.203 target_model_diff: 200.000 Tp_perstep: 0.138   Tu_perstep: 0.052   
[2025-06-02 20:10:15,063][train][INFO][log.py>_log] ==> #1900       Episodes Collected: 290        Transitions Collected: 8041       Batch Size: 256   | NewEpisode Model(mean:1784      ) Reward(mean:1.29 , max:2.30 , min:0.66 , std:0.47 ) | total_loss: 5.254   reward_loss: 0.113   policy_loss: 8.563   value_loss: 1.183   consistency_loss: -1.858  lr: 0.000100  batch_future_return: 0.654   batch_model_diff: 1061.719target_model_diff: 100.000 Tp_perstep: 0.139   Tu_perstep: 0.052   
[2025-06-02 20:10:35,302][train][INFO][log.py>_log] ==> #2000       Episodes Collected: 309        Transitions Collected: 8536       Batch Size: 256   | NewEpisode Model(mean:1885      ) Reward(mean:1.04 , max:1.97 , min:0.33 , std:0.59 ) | total_loss: 4.524   reward_loss: 0.099   policy_loss: 7.372   value_loss: 1.014   consistency_loss: -1.600  lr: 0.000100  batch_future_return: 0.634   batch_model_diff: 1057.812target_model_diff: 200.000 Tp_perstep: 0.140   Tu_perstep: 0.052   
[2025-06-02 20:10:35,305][train][WARNING][train.py>_train] ==> #2000 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:10:54,315][train][INFO][log.py>_log] ==> #2100       Episodes Collected: 322        Transitions Collected: 8933       Batch Size: 256   | NewEpisode Model(mean:1979      ) Reward(mean:0.99 , max:2.30 , min:0.33 , std:0.50 ) | total_loss: 4.961   reward_loss: 0.115   policy_loss: 8.337   value_loss: 1.175   consistency_loss: -1.893  lr: 0.000100  batch_future_return: 0.633   batch_model_diff: 1105.469target_model_diff: 100.000 Tp_perstep: 0.140   Tu_perstep: 0.052   
[2025-06-02 20:11:13,595][train][INFO][log.py>_log] ==> #2200       Episodes Collected: 336        Transitions Collected: 9348       Batch Size: 256   | NewEpisode Model(mean:2078      ) Reward(mean:0.99 , max:1.64 , min:0.33 , std:0.50 ) | total_loss: 4.224   reward_loss: 0.099   policy_loss: 7.209   value_loss: 1.029   consistency_loss: -1.671  lr: 0.000100  batch_future_return: 0.646   batch_model_diff: 1209.375target_model_diff: 200.000 Tp_perstep: 0.140   Tu_perstep: 0.052   
[2025-06-02 20:11:13,597][train][WARNING][train.py>_train] ==> #2200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:11:23,879][train][WARNING][train.py>_train] ==> #2250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:11:33,640][train][INFO][log.py>_log] ==> #2300       Episodes Collected: 354        Transitions Collected: 9828       Batch Size: 256   | NewEpisode Model(mean:2183      ) Reward(mean:1.24 , max:2.63 , min:0.66 , std:0.56 ) | total_loss: 4.588   reward_loss: 0.113   policy_loss: 8.015   value_loss: 1.112   consistency_loss: -1.909  lr: 0.000100  batch_future_return: 0.582   batch_model_diff: 1198.828target_model_diff: 100.000 Tp_perstep: 0.140   Tu_perstep: 0.052   
[2025-06-02 20:11:33,642][train][WARNING][train.py>_train] ==> #2300 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:11:54,506][train][INFO][log.py>_log] ==> #2400       Episodes Collected: 371        Transitions Collected: 10299      Batch Size: 256   | NewEpisode Model(mean:2286      ) Reward(mean:1.28 , max:2.30 , min:0.33 , std:0.61 ) | total_loss: 4.096   reward_loss: 0.098   policy_loss: 7.008   value_loss: 0.987   consistency_loss: -1.628  lr: 0.000100  batch_future_return: 0.602   batch_model_diff: 1266.016target_model_diff: 200.000 Tp_perstep: 0.141   Tu_perstep: 0.052   
[2025-06-02 20:12:14,636][train][INFO][log.py>_log] ==> #2500       Episodes Collected: 388        Transitions Collected: 10783      Batch Size: 256   | NewEpisode Model(mean:2380      ) Reward(mean:0.93 , max:1.97 , min:0.33 , std:0.51 ) | total_loss: 1.787   reward_loss: 0.044   policy_loss: 3.163   value_loss: 0.468   consistency_loss: -0.768  lr: 0.000100  batch_future_return: 0.607   batch_model_diff: 1264.062target_model_diff: 100.000 Tp_perstep: 0.141   Tu_perstep: 0.052   
[2025-06-02 20:12:34,994][train][INFO][log.py>_log] ==> #2600       Episodes Collected: 407        Transitions Collected: 11308      Batch Size: 256   | NewEpisode Model(mean:2487      ) Reward(mean:1.04 , max:1.97 , min:0.00 , std:0.43 ) | total_loss: 4.970   reward_loss: 0.112   policy_loss: 8.337   value_loss: 1.206   consistency_loss: -1.890  lr: 0.000100  batch_future_return: 0.663   batch_model_diff: 1305.859target_model_diff: 200.000 Tp_perstep: 0.142   Tu_perstep: 0.052   
[2025-06-02 20:12:55,480][train][INFO][log.py>_log] ==> #2700       Episodes Collected: 422        Transitions Collected: 11726      Batch Size: 256   | NewEpisode Model(mean:2592      ) Reward(mean:0.81 , max:1.64 , min:0.33 , std:0.40 ) | total_loss: 4.805   reward_loss: 0.124   policy_loss: 8.071   value_loss: 1.147   consistency_loss: -1.838  lr: 0.000100  batch_future_return: 0.633   batch_model_diff: 1426.172target_model_diff: 100.000 Tp_perstep: 0.142   Tu_perstep: 0.052   
[2025-06-02 20:13:16,238][train][INFO][log.py>_log] ==> #2800       Episodes Collected: 440        Transitions Collected: 12251      Batch Size: 256   | NewEpisode Model(mean:2683      ) Reward(mean:1.28 , max:2.63 , min:0.33 , std:0.56 ) | total_loss: 3.692   reward_loss: 0.079   policy_loss: 6.449   value_loss: 0.906   consistency_loss: -1.531  lr: 0.000100  batch_future_return: 0.605   batch_model_diff: 1441.797target_model_diff: 200.000 Tp_perstep: 0.143   Tu_perstep: 0.051   
[2025-06-02 20:13:26,011][train][WARNING][train.py>_train] ==> #2850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:13:35,718][train][INFO][log.py>_log] ==> #2900       Episodes Collected: 455        Transitions Collected: 12694      Batch Size: 256   | NewEpisode Model(mean:2780      ) Reward(mean:1.05 , max:1.97 , min:0.33 , std:0.44 ) | total_loss: 4.042   reward_loss: 0.097   policy_loss: 6.922   value_loss: 1.003   consistency_loss: -1.614  lr: 0.000100  batch_future_return: 0.670   batch_model_diff: 1523.828target_model_diff: 100.000 Tp_perstep: 0.143   Tu_perstep: 0.051   
[2025-06-02 20:13:56,067][train][INFO][log.py>_log] ==> #3000       Episodes Collected: 471        Transitions Collected: 13144      Batch Size: 256   | NewEpisode Model(mean:2889      ) Reward(mean:1.19 , max:2.30 , min:0.33 , std:0.50 ) | total_loss: 2.947   reward_loss: 0.073   policy_loss: 5.218   value_loss: 0.764   consistency_loss: -1.267  lr: 0.000100  batch_future_return: 0.615   batch_model_diff: 1541.016target_model_diff: 200.000 Tp_perstep: 0.143   Tu_perstep: 0.051   
[2025-06-02 20:14:06,247][train][WARNING][train.py>_train] ==> #3050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:14:16,878][train][INFO][log.py>_log] ==> #3100       Episodes Collected: 489        Transitions Collected: 13682      Batch Size: 256   | NewEpisode Model(mean:2983      ) Reward(mean:1.21 , max:2.30 , min:0.66 , std:0.42 ) | total_loss: 3.446   reward_loss: 0.100   policy_loss: 6.035   value_loss: 0.878   consistency_loss: -1.454  lr: 0.000100  batch_future_return: 0.695   batch_model_diff: 1616.406target_model_diff: 100.000 Tp_perstep: 0.144   Tu_perstep: 0.051   
[2025-06-02 20:14:26,509][train][WARNING][train.py>_train] ==> #3150 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:14:37,085][train][INFO][log.py>_log] ==> #3200       Episodes Collected: 505        Transitions Collected: 14128      Batch Size: 256   | NewEpisode Model(mean:3087      ) Reward(mean:1.09 , max:1.97 , min:0.00 , std:0.41 ) | total_loss: 4.124   reward_loss: 0.112   policy_loss: 7.505   value_loss: 1.113   consistency_loss: -1.885  lr: 0.000100  batch_future_return: 0.623   batch_model_diff: 1708.203target_model_diff: 200.000 Tp_perstep: 0.144   Tu_perstep: 0.051   
[2025-06-02 20:14:56,934][train][INFO][log.py>_log] ==> #3300       Episodes Collected: 523        Transitions Collected: 14617      Batch Size: 256   | NewEpisode Model(mean:3182      ) Reward(mean:1.37 , max:4.33 , min:0.66 , std:0.83 ) | total_loss: 2.553   reward_loss: 0.069   policy_loss: 4.474   value_loss: 0.658   consistency_loss: -1.077  lr: 0.000100  batch_future_return: 0.632   batch_model_diff: 1664.453target_model_diff: 100.000 Tp_perstep: 0.144   Tu_perstep: 0.051   
[2025-06-02 20:15:06,997][train][WARNING][train.py>_train] ==> #3350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:15:17,642][train][INFO][log.py>_log] ==> #3400       Episodes Collected: 543        Transitions Collected: 15133      Batch Size: 256   | NewEpisode Model(mean:3290      ) Reward(mean:1.05 , max:1.97 , min:0.33 , std:0.51 ) | total_loss: 4.023   reward_loss: 0.119   policy_loss: 7.370   value_loss: 1.090   consistency_loss: -1.870  lr: 0.000100  batch_future_return: 0.608   batch_model_diff: 1631.250target_model_diff: 200.000 Tp_perstep: 0.145   Tu_perstep: 0.051   
[2025-06-02 20:15:17,644][train][WARNING][train.py>_train] ==> #3400 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:15:38,048][train][INFO][log.py>_log] ==> #3500       Episodes Collected: 559        Transitions Collected: 15602      Batch Size: 256   | NewEpisode Model(mean:3388      ) Reward(mean:1.01 , max:1.97 , min:0.33 , std:0.39 ) | total_loss: 5.258   reward_loss: 0.144   policy_loss: 9.134   value_loss: 1.322   consistency_loss: -2.175  lr: 0.000100  batch_future_return: 0.671   batch_model_diff: 1856.250target_model_diff: 100.000 Tp_perstep: 0.145   Tu_perstep: 0.051   
[2025-06-02 20:15:38,051][train][WARNING][train.py>_train] ==> #3500 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:15:48,260][train][WARNING][train.py>_train] ==> #3550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:15:58,941][train][INFO][log.py>_log] ==> #3600       Episodes Collected: 578        Transitions Collected: 16111      Batch Size: 256   | NewEpisode Model(mean:3488      ) Reward(mean:1.19 , max:1.64 , min:0.66 , std:0.36 ) | total_loss: 3.706   reward_loss: 0.088   policy_loss: 6.497   value_loss: 0.957   consistency_loss: -1.559  lr: 0.000100  batch_future_return: 0.689   batch_model_diff: 1847.656target_model_diff: 200.000 Tp_perstep: 0.145   Tu_perstep: 0.051   
[2025-06-02 20:16:19,198][train][INFO][log.py>_log] ==> #3700       Episodes Collected: 595        Transitions Collected: 16574      Batch Size: 256   | NewEpisode Model(mean:3587      ) Reward(mean:1.18 , max:1.64 , min:0.33 , std:0.41 ) | total_loss: 3.450   reward_loss: 0.116   policy_loss: 6.440   value_loss: 0.935   consistency_loss: -1.670  lr: 0.000100  batch_future_return: 0.649   batch_model_diff: 1719.531target_model_diff: 100.000 Tp_perstep: 0.145   Tu_perstep: 0.051   
[2025-06-02 20:16:40,005][train][INFO][log.py>_log] ==> #3800       Episodes Collected: 615        Transitions Collected: 17095      Batch Size: 256   | NewEpisode Model(mean:3689      ) Reward(mean:1.23 , max:2.30 , min:0.66 , std:0.50 ) | total_loss: 4.252   reward_loss: 0.110   policy_loss: 7.721   value_loss: 1.127   consistency_loss: -1.930  lr: 0.000100  batch_future_return: 0.623   batch_model_diff: 1851.953target_model_diff: 200.000 Tp_perstep: 0.146   Tu_perstep: 0.051   
[2025-06-02 20:17:00,815][train][INFO][log.py>_log] ==> #3900       Episodes Collected: 636        Transitions Collected: 17615      Batch Size: 256   | NewEpisode Model(mean:3792      ) Reward(mean:1.25 , max:2.30 , min:0.66 , std:0.50 ) | total_loss: 3.284   reward_loss: 0.078   policy_loss: 5.740   value_loss: 0.836   consistency_loss: -1.372  lr: 0.000100  batch_future_return: 0.649   batch_model_diff: 1815.625target_model_diff: 100.000 Tp_perstep: 0.146   Tu_perstep: 0.051   
[2025-06-02 20:17:10,956][train][WARNING][train.py>_train] ==> #3950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:17:21,575][train][INFO][log.py>_log] ==> #4000       Episodes Collected: 653        Transitions Collected: 18104      Batch Size: 256   | NewEpisode Model(mean:3893      ) Reward(mean:1.02 , max:1.97 , min:0.00 , std:0.50 ) | total_loss: 2.518   reward_loss: 0.065   policy_loss: 4.398   value_loss: 0.638   consistency_loss: -1.053  lr: 0.000100  batch_future_return: 0.655   batch_model_diff: 1776.953target_model_diff: 200.000 Tp_perstep: 0.146   Tu_perstep: 0.051   
[2025-06-02 20:17:42,257][train][INFO][log.py>_log] ==> #4100       Episodes Collected: 672        Transitions Collected: 18597      Batch Size: 256   | NewEpisode Model(mean:3991      ) Reward(mean:1.25 , max:2.63 , min:0.33 , std:0.71 ) | total_loss: 4.687   reward_loss: 0.124   policy_loss: 8.711   value_loss: 1.239   consistency_loss: -2.229  lr: 0.000100  batch_future_return: 0.626   batch_model_diff: 2012.109target_model_diff: 100.000 Tp_perstep: 0.147   Tu_perstep: 0.051   
[2025-06-02 20:18:03,084][train][INFO][log.py>_log] ==> #4200       Episodes Collected: 690        Transitions Collected: 19059      Batch Size: 256   | NewEpisode Model(mean:4090      ) Reward(mean:1.17 , max:1.97 , min:0.66 , std:0.48 ) | total_loss: 4.604   reward_loss: 0.119   policy_loss: 8.223   value_loss: 1.134   consistency_loss: -2.011  lr: 0.000100  batch_future_return: 0.662   batch_model_diff: 2037.891target_model_diff: 200.000 Tp_perstep: 0.147   Tu_perstep: 0.051   
[2025-06-02 20:18:23,938][train][INFO][log.py>_log] ==> #4300       Episodes Collected: 709        Transitions Collected: 19623      Batch Size: 256   | NewEpisode Model(mean:4184      ) Reward(mean:0.95 , max:2.30 , min:0.33 , std:0.48 ) | total_loss: 2.761   reward_loss: 0.071   policy_loss: 4.648   value_loss: 0.635   consistency_loss: -1.058  lr: 0.000100  batch_future_return: 0.674   batch_model_diff: 2026.953target_model_diff: 100.000 Tp_perstep: 0.147   Tu_perstep: 0.051   
[2025-06-02 20:18:44,656][train][INFO][log.py>_log] ==> #4400       Episodes Collected: 727        Transitions Collected: 20127      Batch Size: 256   | NewEpisode Model(mean:4291      ) Reward(mean:1.28 , max:1.97 , min:0.33 , std:0.56 ) | total_loss: 2.640   reward_loss: 0.058   policy_loss: 4.363   value_loss: 0.593   consistency_loss: -0.965  lr: 0.000100  batch_future_return: 0.676   batch_model_diff: 2136.719target_model_diff: 200.000 Tp_perstep: 0.147   Tu_perstep: 0.051   
[2025-06-02 20:18:44,658][train][WARNING][train.py>_train] ==> #4400 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:19:05,459][train][INFO][log.py>_log] ==> #4500       Episodes Collected: 745        Transitions Collected: 20626      Batch Size: 256   | NewEpisode Model(mean:4393      ) Reward(mean:0.97 , max:1.64 , min:0.33 , std:0.39 ) | total_loss: 4.524   reward_loss: 0.136   policy_loss: 8.062   value_loss: 1.182   consistency_loss: -1.985  lr: 0.000100  batch_future_return: 0.660   batch_model_diff: 2231.250target_model_diff: 100.000 Tp_perstep: 0.148   Tu_perstep: 0.051   
[2025-06-02 20:19:26,640][train][INFO][log.py>_log] ==> #4600       Episodes Collected: 763        Transitions Collected: 21139      Batch Size: 256   | NewEpisode Model(mean:4492      ) Reward(mean:1.30 , max:2.30 , min:0.66 , std:0.58 ) | total_loss: 1.929   reward_loss: 0.045   policy_loss: 3.561   value_loss: 0.527   consistency_loss: -0.905  lr: 0.000100  batch_future_return: 0.615   batch_model_diff: 2287.891target_model_diff: 200.000 Tp_perstep: 0.148   Tu_perstep: 0.051   
[2025-06-02 20:19:37,236][train][WARNING][train.py>_train] ==> #4650 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:19:47,381][train][INFO][log.py>_log] ==> #4700       Episodes Collected: 783        Transitions Collected: 21633      Batch Size: 256   | NewEpisode Model(mean:4588      ) Reward(mean:1.36 , max:2.63 , min:0.33 , std:0.55 ) | total_loss: 4.294   reward_loss: 0.120   policy_loss: 7.610   value_loss: 1.139   consistency_loss: -1.860  lr: 0.000100  batch_future_return: 0.634   batch_model_diff: 2234.766target_model_diff: 100.000 Tp_perstep: 0.148   Tu_perstep: 0.051   
[2025-06-02 20:19:47,383][train][WARNING][train.py>_train] ==> #4700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:20:08,549][train][INFO][log.py>_log] ==> #4800       Episodes Collected: 803        Transitions Collected: 22132      Batch Size: 256   | NewEpisode Model(mean:4686      ) Reward(mean:1.38 , max:2.30 , min:0.33 , std:0.59 ) | total_loss: 4.621   reward_loss: 0.131   policy_loss: 8.301   value_loss: 1.183   consistency_loss: -2.054  lr: 0.000100  batch_future_return: 0.663   batch_model_diff: 2094.531target_model_diff: 200.000 Tp_perstep: 0.149   Tu_perstep: 0.051   
[2025-06-02 20:20:29,816][train][INFO][log.py>_log] ==> #4900       Episodes Collected: 823        Transitions Collected: 22665      Batch Size: 256   | NewEpisode Model(mean:4783      ) Reward(mean:1.40 , max:3.29 , min:0.33 , std:0.73 ) | total_loss: 1.893   reward_loss: 0.056   policy_loss: 3.356   value_loss: 0.469   consistency_loss: -0.818  lr: 0.000100  batch_future_return: 0.691   batch_model_diff: 2242.188target_model_diff: 100.000 Tp_perstep: 0.149   Tu_perstep: 0.051   
[2025-06-02 20:20:50,143][train][INFO][log.py>_log] ==> #5000       Episodes Collected: 840        Transitions Collected: 23130      Batch Size: 256   | NewEpisode Model(mean:4887      ) Reward(mean:0.93 , max:1.64 , min:0.00 , std:0.38 ) | total_loss: 3.803   reward_loss: 0.098   policy_loss: 6.620   value_loss: 0.904   consistency_loss: -1.570  lr: 0.000100  batch_future_return: 0.652   batch_model_diff: 2190.625target_model_diff: 200.000 Tp_perstep: 0.149   Tu_perstep: 0.051   
[2025-06-02 20:21:00,798][train][WARNING][train.py>_train] ==> #5050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:21:11,528][train][INFO][log.py>_log] ==> #5100       Episodes Collected: 860        Transitions Collected: 23671      Batch Size: 256   | NewEpisode Model(mean:4984      ) Reward(mean:1.32 , max:2.30 , min:0.66 , std:0.43 ) | total_loss: 4.193   reward_loss: 0.116   policy_loss: 7.522   value_loss: 1.074   consistency_loss: -1.857  lr: 0.000100  batch_future_return: 0.669   batch_model_diff: 2315.234target_model_diff: 100.000 Tp_perstep: 0.149   Tu_perstep: 0.051   
[2025-06-02 20:21:32,311][train][INFO][log.py>_log] ==> #5200       Episodes Collected: 876        Transitions Collected: 24175      Batch Size: 256   | NewEpisode Model(mean:5088      ) Reward(mean:1.03 , max:2.30 , min:0.33 , std:0.54 ) | total_loss: 3.636   reward_loss: 0.093   policy_loss: 6.361   value_loss: 0.917   consistency_loss: -1.523  lr: 0.000100  batch_future_return: 0.637   batch_model_diff: 2250.391target_model_diff: 200.000 Tp_perstep: 0.149   Tu_perstep: 0.051   
[2025-06-02 20:21:53,553][train][INFO][log.py>_log] ==> #5300       Episodes Collected: 895        Transitions Collected: 24773      Batch Size: 256   | NewEpisode Model(mean:5187      ) Reward(mean:1.02 , max:1.97 , min:0.00 , std:0.52 ) | total_loss: 4.177   reward_loss: 0.101   policy_loss: 7.075   value_loss: 1.020   consistency_loss: -1.627  lr: 0.000100  batch_future_return: 0.714   batch_model_diff: 2492.969target_model_diff: 100.000 Tp_perstep: 0.150   Tu_perstep: 0.050   
[2025-06-02 20:21:53,555][train][WARNING][train.py>_train] ==> #5300 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:22:04,667][train][WARNING][train.py>_train] ==> #5350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:22:15,366][train][INFO][log.py>_log] ==> #5400       Episodes Collected: 914        Transitions Collected: 25287      Batch Size: 256   | NewEpisode Model(mean:5293      ) Reward(mean:1.37 , max:2.96 , min:0.66 , std:0.59 ) | total_loss: 4.375   reward_loss: 0.127   policy_loss: 7.908   value_loss: 1.141   consistency_loss: -1.972  lr: 0.000100  batch_future_return: 0.687   batch_model_diff: 2553.906target_model_diff: 200.000 Tp_perstep: 0.150   Tu_perstep: 0.050   
[2025-06-02 20:22:36,665][train][INFO][log.py>_log] ==> #5500       Episodes Collected: 932        Transitions Collected: 25815      Batch Size: 256   | NewEpisode Model(mean:5389      ) Reward(mean:1.06 , max:2.63 , min:0.33 , std:0.58 ) | total_loss: 4.071   reward_loss: 0.102   policy_loss: 6.774   value_loss: 0.933   consistency_loss: -1.519  lr: 0.000100  batch_future_return: 0.737   batch_model_diff: 2616.406target_model_diff: 100.000 Tp_perstep: 0.150   Tu_perstep: 0.050   
[2025-06-02 20:22:47,342][train][WARNING][train.py>_train] ==> #5550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:22:57,937][train][INFO][log.py>_log] ==> #5600       Episodes Collected: 953        Transitions Collected: 26372      Batch Size: 256   | NewEpisode Model(mean:5491      ) Reward(mean:1.10 , max:1.64 , min:0.33 , std:0.38 ) | total_loss: 3.144   reward_loss: 0.078   policy_loss: 5.470   value_loss: 0.810   consistency_loss: -1.303  lr: 0.000100  batch_future_return: 0.659   batch_model_diff: 2413.672target_model_diff: 200.000 Tp_perstep: 0.151   Tu_perstep: 0.050   
[2025-06-02 20:22:57,939][train][WARNING][train.py>_train] ==> #5600 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:23:19,724][train][INFO][log.py>_log] ==> #5700       Episodes Collected: 972        Transitions Collected: 26906      Batch Size: 256   | NewEpisode Model(mean:5589      ) Reward(mean:1.21 , max:2.30 , min:0.33 , std:0.40 ) | total_loss: 2.780   reward_loss: 0.062   policy_loss: 4.715   value_loss: 0.658   consistency_loss: -1.081  lr: 0.000100  batch_future_return: 0.672   batch_model_diff: 2771.875target_model_diff: 100.000 Tp_perstep: 0.151   Tu_perstep: 0.050   
[2025-06-02 20:23:30,377][train][WARNING][train.py>_train] ==> #5750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:23:41,057][train][INFO][log.py>_log] ==> #5800       Episodes Collected: 993        Transitions Collected: 27472      Batch Size: 256   | NewEpisode Model(mean:5687      ) Reward(mean:1.02 , max:2.30 , min:0.33 , std:0.43 ) | total_loss: 4.141   reward_loss: 0.104   policy_loss: 7.406   value_loss: 1.042   consistency_loss: -1.815  lr: 0.000100  batch_future_return: 0.604   batch_model_diff: 2586.328target_model_diff: 200.000 Tp_perstep: 0.151   Tu_perstep: 0.050   
[2025-06-02 20:24:03,888][train][INFO][log.py>_log] ==> #5900       Episodes Collected: 1015       Transitions Collected: 28024      Batch Size: 256   | NewEpisode Model(mean:5792      ) Reward(mean:1.03 , max:1.97 , min:0.00 , std:0.50 ) | total_loss: 2.802   reward_loss: 0.078   policy_loss: 5.162   value_loss: 0.765   consistency_loss: -1.315  lr: 0.000100  batch_future_return: 0.573   batch_model_diff: 2724.219target_model_diff: 100.000 Tp_perstep: 0.152   Tu_perstep: 0.050   
[2025-06-02 20:24:25,235][train][INFO][log.py>_log] ==> #6000       Episodes Collected: 1036       Transitions Collected: 28574      Batch Size: 256   | NewEpisode Model(mean:5883      ) Reward(mean:1.32 , max:2.30 , min:0.33 , std:0.55 ) | total_loss: 3.611   reward_loss: 0.096   policy_loss: 6.461   value_loss: 0.933   consistency_loss: -1.589  lr: 0.000100  batch_future_return: 0.643   batch_model_diff: 2687.500target_model_diff: 200.000 Tp_perstep: 0.152   Tu_perstep: 0.050   
[2025-06-02 20:24:25,238][train][WARNING][train.py>_train] ==> #6000 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:24:47,320][train][INFO][log.py>_log] ==> #6100       Episodes Collected: 1057       Transitions Collected: 29158      Batch Size: 256   | NewEpisode Model(mean:5985      ) Reward(mean:1.36 , max:2.30 , min:0.66 , std:0.42 ) | total_loss: 3.161   reward_loss: 0.083   policy_loss: 5.638   value_loss: 0.811   consistency_loss: -1.381  lr: 0.000100  batch_future_return: 0.654   batch_model_diff: 2678.906target_model_diff: 100.000 Tp_perstep: 0.152   Tu_perstep: 0.050   
[2025-06-02 20:24:47,322][train][WARNING][train.py>_train] ==> #6100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:25:09,514][train][INFO][log.py>_log] ==> #6200       Episodes Collected: 1079       Transitions Collected: 29766      Batch Size: 256   | NewEpisode Model(mean:6089      ) Reward(mean:1.26 , max:2.30 , min:0.66 , std:0.54 ) | total_loss: 3.747   reward_loss: 0.090   policy_loss: 6.500   value_loss: 0.946   consistency_loss: -1.540  lr: 0.000100  batch_future_return: 0.692   batch_model_diff: 2853.906target_model_diff: 200.000 Tp_perstep: 0.152   Tu_perstep: 0.050   
[2025-06-02 20:25:31,557][train][INFO][log.py>_log] ==> #6300       Episodes Collected: 1100       Transitions Collected: 30351      Batch Size: 256   | NewEpisode Model(mean:6192      ) Reward(mean:1.17 , max:2.96 , min:0.33 , std:0.62 ) | total_loss: 4.854   reward_loss: 0.138   policy_loss: 8.762   value_loss: 1.230   consistency_loss: -2.177  lr: 0.000100  batch_future_return: 0.609   batch_model_diff: 2848.438target_model_diff: 100.000 Tp_perstep: 0.153   Tu_perstep: 0.050   
[2025-06-02 20:25:52,471][train][INFO][log.py>_log] ==> #6400       Episodes Collected: 1120       Transitions Collected: 30850      Batch Size: 256   | NewEpisode Model(mean:6292      ) Reward(mean:1.25 , max:2.30 , min:0.00 , std:0.55 ) | total_loss: 3.943   reward_loss: 0.095   policy_loss: 6.722   value_loss: 0.978   consistency_loss: -1.559  lr: 0.000100  batch_future_return: 0.688   batch_model_diff: 3142.578target_model_diff: 200.000 Tp_perstep: 0.153   Tu_perstep: 0.050   
[2025-06-02 20:25:52,473][train][WARNING][train.py>_train] ==> #6400 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:26:14,294][train][INFO][log.py>_log] ==> #6500       Episodes Collected: 1141       Transitions Collected: 31416      Batch Size: 256   | NewEpisode Model(mean:6390      ) Reward(mean:1.13 , max:1.97 , min:0.33 , std:0.46 ) | total_loss: 2.587   reward_loss: 0.066   policy_loss: 4.577   value_loss: 0.661   consistency_loss: -1.111  lr: 0.000100  batch_future_return: 0.674   batch_model_diff: 2740.625target_model_diff: 100.000 Tp_perstep: 0.153   Tu_perstep: 0.050   
[2025-06-02 20:26:35,636][train][INFO][log.py>_log] ==> #6600       Episodes Collected: 1160       Transitions Collected: 31933      Batch Size: 256   | NewEpisode Model(mean:6489      ) Reward(mean:1.25 , max:1.97 , min:0.33 , std:0.43 ) | total_loss: 4.029   reward_loss: 0.114   policy_loss: 7.244   value_loss: 1.033   consistency_loss: -1.794  lr: 0.000100  batch_future_return: 0.708   batch_model_diff: 2925.000target_model_diff: 200.000 Tp_perstep: 0.153   Tu_perstep: 0.050   
[2025-06-02 20:26:46,695][train][WARNING][train.py>_train] ==> #6650 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:26:57,403][train][INFO][log.py>_log] ==> #6700       Episodes Collected: 1180       Transitions Collected: 32521      Batch Size: 256   | NewEpisode Model(mean:6587      ) Reward(mean:1.15 , max:1.97 , min:0.33 , std:0.48 ) | total_loss: 4.032   reward_loss: 0.099   policy_loss: 6.932   value_loss: 0.994   consistency_loss: -1.624  lr: 0.000100  batch_future_return: 0.667   batch_model_diff: 2937.109target_model_diff: 100.000 Tp_perstep: 0.154   Tu_perstep: 0.050   
[2025-06-02 20:26:57,405][train][WARNING][train.py>_train] ==> #6700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:27:08,108][train][WARNING][train.py>_train] ==> #6750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:27:18,965][train][INFO][log.py>_log] ==> #6800       Episodes Collected: 1201       Transitions Collected: 33079      Batch Size: 256   | NewEpisode Model(mean:6690      ) Reward(mean:1.05 , max:2.30 , min:0.00 , std:0.57 ) | total_loss: 2.712   reward_loss: 0.068   policy_loss: 4.887   value_loss: 0.687   consistency_loss: -1.207  lr: 0.000100  batch_future_return: 0.602   batch_model_diff: 3155.469target_model_diff: 200.000 Tp_perstep: 0.154   Tu_perstep: 0.050   
[2025-06-02 20:27:29,488][train][WARNING][train.py>_train] ==> #6850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:27:41,111][train][INFO][log.py>_log] ==> #6900       Episodes Collected: 1223       Transitions Collected: 33634      Batch Size: 256   | NewEpisode Model(mean:6795      ) Reward(mean:1.27 , max:2.30 , min:0.66 , std:0.49 ) | total_loss: 5.136   reward_loss: 0.103   policy_loss: 8.882   value_loss: 1.218   consistency_loss: -2.077  lr: 0.000100  batch_future_return: 0.588   batch_model_diff: 3308.984target_model_diff: 100.000 Tp_perstep: 0.154   Tu_perstep: 0.050   
[2025-06-02 20:28:02,452][train][INFO][log.py>_log] ==> #7000       Episodes Collected: 1241       Transitions Collected: 34145      Batch Size: 256   | NewEpisode Model(mean:6889      ) Reward(mean:1.22 , max:2.30 , min:0.00 , std:0.64 ) | total_loss: 3.519   reward_loss: 0.111   policy_loss: 6.548   value_loss: 0.980   consistency_loss: -1.692  lr: 0.000100  batch_future_return: 0.656   batch_model_diff: 3147.266target_model_diff: 200.000 Tp_perstep: 0.154   Tu_perstep: 0.050   
[2025-06-02 20:28:23,793][train][INFO][log.py>_log] ==> #7100       Episodes Collected: 1261       Transitions Collected: 34694      Batch Size: 256   | NewEpisode Model(mean:6987      ) Reward(mean:0.99 , max:1.97 , min:0.33 , std:0.44 ) | total_loss: 3.880   reward_loss: 0.104   policy_loss: 6.861   value_loss: 0.990   consistency_loss: -1.667  lr: 0.000100  batch_future_return: 0.674   batch_model_diff: 3376.172target_model_diff: 100.000 Tp_perstep: 0.154   Tu_perstep: 0.050   
[2025-06-02 20:28:23,795][train][WARNING][train.py>_train] ==> #7100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:28:46,452][train][INFO][log.py>_log] ==> #7200       Episodes Collected: 1284       Transitions Collected: 35301      Batch Size: 256   | NewEpisode Model(mean:7086      ) Reward(mean:1.16 , max:2.30 , min:0.00 , std:0.54 ) | total_loss: 3.948   reward_loss: 0.093   policy_loss: 6.748   value_loss: 0.960   consistency_loss: -1.566  lr: 0.000100  batch_future_return: 0.668   batch_model_diff: 3441.797target_model_diff: 200.000 Tp_perstep: 0.155   Tu_perstep: 0.050   
[2025-06-02 20:29:08,201][train][INFO][log.py>_log] ==> #7300       Episodes Collected: 1306       Transitions Collected: 35893      Batch Size: 256   | NewEpisode Model(mean:7188      ) Reward(mean:1.36 , max:2.96 , min:0.00 , std:0.79 ) | total_loss: 3.058   reward_loss: 0.062   policy_loss: 5.186   value_loss: 0.753   consistency_loss: -1.189  lr: 0.000100  batch_future_return: 0.691   batch_model_diff: 3192.578target_model_diff: 100.000 Tp_perstep: 0.155   Tu_perstep: 0.050   
[2025-06-02 20:29:08,204][train][WARNING][train.py>_train] ==> #7300 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:29:18,853][train][WARNING][train.py>_train] ==> #7350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:29:30,529][train][INFO][log.py>_log] ==> #7400       Episodes Collected: 1326       Transitions Collected: 36421      Batch Size: 256   | NewEpisode Model(mean:7292      ) Reward(mean:1.33 , max:2.30 , min:0.33 , std:0.48 ) | total_loss: 3.603   reward_loss: 0.105   policy_loss: 6.355   value_loss: 0.903   consistency_loss: -1.541  lr: 0.000100  batch_future_return: 0.712   batch_model_diff: 3482.422target_model_diff: 200.000 Tp_perstep: 0.155   Tu_perstep: 0.050   
[2025-06-02 20:29:41,131][train][WARNING][train.py>_train] ==> #7450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:29:52,242][train][INFO][log.py>_log] ==> #7500       Episodes Collected: 1345       Transitions Collected: 36945      Batch Size: 256   | NewEpisode Model(mean:7387      ) Reward(mean:1.25 , max:2.63 , min:0.33 , std:0.60 ) | total_loss: 4.121   reward_loss: 0.112   policy_loss: 7.472   value_loss: 1.074   consistency_loss: -1.866  lr: 0.000100  batch_future_return: 0.652   batch_model_diff: 3239.453target_model_diff: 100.000 Tp_perstep: 0.155   Tu_perstep: 0.050   
[2025-06-02 20:30:13,566][train][INFO][log.py>_log] ==> #7600       Episodes Collected: 1365       Transitions Collected: 37529      Batch Size: 256   | NewEpisode Model(mean:7487      ) Reward(mean:1.32 , max:2.96 , min:0.00 , std:0.65 ) | total_loss: 2.133   reward_loss: 0.051   policy_loss: 3.710   value_loss: 0.551   consistency_loss: -0.883  lr: 0.000100  batch_future_return: 0.686   batch_model_diff: 3398.438target_model_diff: 200.000 Tp_perstep: 0.155   Tu_perstep: 0.050   
[2025-06-02 20:30:24,278][train][WARNING][train.py>_train] ==> #7650 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:30:35,912][train][INFO][log.py>_log] ==> #7700       Episodes Collected: 1386       Transitions Collected: 38137      Batch Size: 256   | NewEpisode Model(mean:7587      ) Reward(mean:1.27 , max:1.97 , min:0.66 , std:0.46 ) | total_loss: 2.302   reward_loss: 0.063   policy_loss: 4.028   value_loss: 0.573   consistency_loss: -0.966  lr: 0.000100  batch_future_return: 0.663   batch_model_diff: 3539.844target_model_diff: 100.000 Tp_perstep: 0.156   Tu_perstep: 0.050   
[2025-06-02 20:30:58,230][train][INFO][log.py>_log] ==> #7800       Episodes Collected: 1409       Transitions Collected: 38722      Batch Size: 256   | NewEpisode Model(mean:7690      ) Reward(mean:1.55 , max:4.66 , min:0.33 , std:0.83 ) | total_loss: 3.119   reward_loss: 0.067   policy_loss: 5.289   value_loss: 0.748   consistency_loss: -1.212  lr: 0.000100  batch_future_return: 0.654   batch_model_diff: 3631.250target_model_diff: 200.000 Tp_perstep: 0.156   Tu_perstep: 0.050   
[2025-06-02 20:31:19,591][train][INFO][log.py>_log] ==> #7900       Episodes Collected: 1428       Transitions Collected: 39265      Batch Size: 256   | NewEpisode Model(mean:7790      ) Reward(mean:1.18 , max:2.63 , min:0.33 , std:0.59 ) | total_loss: 3.066   reward_loss: 0.080   policy_loss: 5.365   value_loss: 0.758   consistency_loss: -1.284  lr: 0.000100  batch_future_return: 0.651   batch_model_diff: 3464.062target_model_diff: 100.000 Tp_perstep: 0.156   Tu_perstep: 0.050   
[2025-06-02 20:31:19,593][train][WARNING][train.py>_train] ==> #7900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:31:41,916][train][INFO][log.py>_log] ==> #8000       Episodes Collected: 1449       Transitions Collected: 39822      Batch Size: 256   | NewEpisode Model(mean:7890      ) Reward(mean:1.32 , max:2.63 , min:0.00 , std:0.60 ) | total_loss: 4.175   reward_loss: 0.105   policy_loss: 7.273   value_loss: 1.001   consistency_loss: -1.726  lr: 0.000100  batch_future_return: 0.697   batch_model_diff: 3591.016target_model_diff: 200.000 Tp_perstep: 0.156   Tu_perstep: 0.050   
[2025-06-02 20:31:52,956][train][WARNING][train.py>_train] ==> #8050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:32:04,102][train][INFO][log.py>_log] ==> #8100       Episodes Collected: 1474       Transitions Collected: 40446      Batch Size: 256   | NewEpisode Model(mean:7988      ) Reward(mean:1.18 , max:2.63 , min:0.66 , std:0.50 ) | total_loss: 2.244   reward_loss: 0.050   policy_loss: 3.770   value_loss: 0.548   consistency_loss: -0.856  lr: 0.000100  batch_future_return: 0.667   batch_model_diff: 3876.953target_model_diff: 100.000 Tp_perstep: 0.156   Tu_perstep: 0.050   
[2025-06-02 20:32:04,104][train][WARNING][train.py>_train] ==> #8100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:32:16,019][train][WARNING][train.py>_train] ==> #8150 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:32:27,643][train][INFO][log.py>_log] ==> #8200       Episodes Collected: 1497       Transitions Collected: 41054      Batch Size: 256   | NewEpisode Model(mean:8092      ) Reward(mean:1.26 , max:2.63 , min:0.66 , std:0.53 ) | total_loss: 2.179   reward_loss: 0.063   policy_loss: 3.851   value_loss: 0.564   consistency_loss: -0.938  lr: 0.000100  batch_future_return: 0.653   batch_model_diff: 3852.344target_model_diff: 200.000 Tp_perstep: 0.157   Tu_perstep: 0.050   
[2025-06-02 20:32:38,700][train][WARNING][train.py>_train] ==> #8250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:32:50,188][train][INFO][log.py>_log] ==> #8300       Episodes Collected: 1517       Transitions Collected: 41645      Batch Size: 256   | NewEpisode Model(mean:8188      ) Reward(mean:1.35 , max:5.32 , min:0.33 , std:1.04 ) | total_loss: 2.764   reward_loss: 0.079   policy_loss: 5.299   value_loss: 0.798   consistency_loss: -1.407  lr: 0.000100  batch_future_return: 0.622   batch_model_diff: 3621.094target_model_diff: 100.000 Tp_perstep: 0.157   Tu_perstep: 0.050   
[2025-06-02 20:33:11,752][train][INFO][log.py>_log] ==> #8400       Episodes Collected: 1539       Transitions Collected: 42232      Batch Size: 256   | NewEpisode Model(mean:8287      ) Reward(mean:1.12 , max:2.30 , min:0.33 , std:0.48 ) | total_loss: 2.715   reward_loss: 0.071   policy_loss: 4.770   value_loss: 0.671   consistency_loss: -1.147  lr: 0.000100  batch_future_return: 0.652   batch_model_diff: 3850.000target_model_diff: 200.000 Tp_perstep: 0.157   Tu_perstep: 0.050   
[2025-06-02 20:33:22,832][train][WARNING][train.py>_train] ==> #8450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:33:33,939][train][INFO][log.py>_log] ==> #8500       Episodes Collected: 1559       Transitions Collected: 42801      Batch Size: 256   | NewEpisode Model(mean:8391      ) Reward(mean:0.99 , max:1.97 , min:0.33 , std:0.48 ) | total_loss: 2.177   reward_loss: 0.053   policy_loss: 3.804   value_loss: 0.534   consistency_loss: -0.907  lr: 0.000100  batch_future_return: 0.629   batch_model_diff: 3582.031target_model_diff: 100.000 Tp_perstep: 0.157   Tu_perstep: 0.050   
[2025-06-02 20:33:55,875][train][INFO][log.py>_log] ==> #8600       Episodes Collected: 1582       Transitions Collected: 43389      Batch Size: 256   | NewEpisode Model(mean:8491      ) Reward(mean:1.26 , max:1.97 , min:0.33 , std:0.34 ) | total_loss: 3.651   reward_loss: 0.088   policy_loss: 6.161   value_loss: 0.855   consistency_loss: -1.406  lr: 0.000100  batch_future_return: 0.674   batch_model_diff: 3562.109target_model_diff: 200.000 Tp_perstep: 0.157   Tu_perstep: 0.050   
[2025-06-02 20:34:06,478][train][WARNING][train.py>_train] ==> #8650 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:34:17,999][train][INFO][log.py>_log] ==> #8700       Episodes Collected: 1602       Transitions Collected: 43946      Batch Size: 256   | NewEpisode Model(mean:8595      ) Reward(mean:1.28 , max:2.96 , min:0.00 , std:0.73 ) | total_loss: 2.873   reward_loss: 0.082   policy_loss: 4.847   value_loss: 0.674   consistency_loss: -1.112  lr: 0.000100  batch_future_return: 0.714   batch_model_diff: 3790.625target_model_diff: 100.000 Tp_perstep: 0.158   Tu_perstep: 0.050   
[2025-06-02 20:34:29,511][train][WARNING][train.py>_train] ==> #8750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:34:40,703][train][INFO][log.py>_log] ==> #8800       Episodes Collected: 1626       Transitions Collected: 44565      Batch Size: 256   | NewEpisode Model(mean:8691      ) Reward(mean:1.18 , max:2.30 , min:0.33 , std:0.48 ) | total_loss: 4.172   reward_loss: 0.122   policy_loss: 7.131   value_loss: 1.008   consistency_loss: -1.666  lr: 0.000100  batch_future_return: 0.743   batch_model_diff: 3851.562target_model_diff: 200.000 Tp_perstep: 0.158   Tu_perstep: 0.050   
[2025-06-02 20:34:51,855][train][WARNING][train.py>_train] ==> #8850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:35:03,502][train][INFO][log.py>_log] ==> #8900       Episodes Collected: 1647       Transitions Collected: 45151      Batch Size: 256   | NewEpisode Model(mean:8792      ) Reward(mean:1.05 , max:1.97 , min:0.33 , std:0.43 ) | total_loss: 2.310   reward_loss: 0.063   policy_loss: 4.291   value_loss: 0.612   consistency_loss: -1.099  lr: 0.000100  batch_future_return: 0.574   batch_model_diff: 3903.516target_model_diff: 100.000 Tp_perstep: 0.158   Tu_perstep: 0.050   
[2025-06-02 20:35:15,499][train][WARNING][train.py>_train] ==> #8950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:35:26,705][train][INFO][log.py>_log] ==> #9000       Episodes Collected: 1672       Transitions Collected: 45815      Batch Size: 256   | NewEpisode Model(mean:8888      ) Reward(mean:1.26 , max:2.30 , min:0.00 , std:0.53 ) | total_loss: 3.277   reward_loss: 0.078   policy_loss: 5.616   value_loss: 0.810   consistency_loss: -1.310  lr: 0.000100  batch_future_return: 0.705   batch_model_diff: 3505.469target_model_diff: 200.000 Tp_perstep: 0.158   Tu_perstep: 0.050   
[2025-06-02 20:35:37,394][train][WARNING][train.py>_train] ==> #9050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:35:49,436][train][INFO][log.py>_log] ==> #9100       Episodes Collected: 1696       Transitions Collected: 46434      Batch Size: 256   | NewEpisode Model(mean:8991      ) Reward(mean:1.47 , max:2.96 , min:0.66 , std:0.46 ) | total_loss: 3.096   reward_loss: 0.071   policy_loss: 5.259   value_loss: 0.759   consistency_loss: -1.212  lr: 0.000100  batch_future_return: 0.676   batch_model_diff: 4228.906target_model_diff: 100.000 Tp_perstep: 0.159   Tu_perstep: 0.050   
[2025-06-02 20:36:00,080][train][WARNING][train.py>_train] ==> #9150 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:36:12,149][train][INFO][log.py>_log] ==> #9200       Episodes Collected: 1716       Transitions Collected: 47003      Batch Size: 256   | NewEpisode Model(mean:9093      ) Reward(mean:1.45 , max:3.29 , min:0.33 , std:0.70 ) | total_loss: 3.028   reward_loss: 0.087   policy_loss: 5.645   value_loss: 0.824   consistency_loss: -1.455  lr: 0.000100  batch_future_return: 0.595   batch_model_diff: 3975.781target_model_diff: 200.000 Tp_perstep: 0.159   Tu_perstep: 0.050   
[2025-06-02 20:36:34,869][train][INFO][log.py>_log] ==> #9300       Episodes Collected: 1740       Transitions Collected: 47658      Batch Size: 256   | NewEpisode Model(mean:9190      ) Reward(mean:1.21 , max:2.63 , min:0.00 , std:0.66 ) | total_loss: 2.528   reward_loss: 0.077   policy_loss: 4.544   value_loss: 0.664   consistency_loss: -1.129  lr: 0.000100  batch_future_return: 0.667   batch_model_diff: 4001.953target_model_diff: 100.000 Tp_perstep: 0.159   Tu_perstep: 0.050   
[2025-06-02 20:36:57,632][train][INFO][log.py>_log] ==> #9400       Episodes Collected: 1763       Transitions Collected: 48294      Batch Size: 256   | NewEpisode Model(mean:9292      ) Reward(mean:1.34 , max:2.30 , min:0.33 , std:0.61 ) | total_loss: 2.128   reward_loss: 0.058   policy_loss: 3.839   value_loss: 0.560   consistency_loss: -0.955  lr: 0.000100  batch_future_return: 0.653   batch_model_diff: 4022.266target_model_diff: 200.000 Tp_perstep: 0.159   Tu_perstep: 0.050   
[2025-06-02 20:37:08,679][train][WARNING][train.py>_train] ==> #9450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:37:19,818][train][INFO][log.py>_log] ==> #9500       Episodes Collected: 1785       Transitions Collected: 48871      Batch Size: 256   | NewEpisode Model(mean:9395      ) Reward(mean:1.23 , max:2.96 , min:0.66 , std:0.55 ) | total_loss: 1.979   reward_loss: 0.054   policy_loss: 3.471   value_loss: 0.490   consistency_loss: -0.834  lr: 0.000100  batch_future_return: 0.663   batch_model_diff: 4136.719target_model_diff: 100.000 Tp_perstep: 0.159   Tu_perstep: 0.050   
[2025-06-02 20:37:41,732][train][INFO][log.py>_log] ==> #9600       Episodes Collected: 1804       Transitions Collected: 49402      Batch Size: 256   | NewEpisode Model(mean:9492      ) Reward(mean:1.11 , max:1.64 , min:0.00 , std:0.46 ) | total_loss: 1.960   reward_loss: 0.054   policy_loss: 3.632   value_loss: 0.508   consistency_loss: -0.926  lr: 0.000100  batch_future_return: 0.579   batch_model_diff: 4264.062target_model_diff: 200.000 Tp_perstep: 0.159   Tu_perstep: 0.050   
[2025-06-02 20:38:04,506][train][INFO][log.py>_log] ==> #9700       Episodes Collected: 1826       Transitions Collected: 50024      Batch Size: 256   | NewEpisode Model(mean:9589      ) Reward(mean:1.26 , max:2.30 , min:0.33 , std:0.55 ) | total_loss: 3.664   reward_loss: 0.089   policy_loss: 6.360   value_loss: 0.919   consistency_loss: -1.508  lr: 0.000100  batch_future_return: 0.659   batch_model_diff: 4194.141target_model_diff: 100.000 Tp_perstep: 0.160   Tu_perstep: 0.050   
[2025-06-02 20:38:26,442][train][INFO][log.py>_log] ==> #9800       Episodes Collected: 1850       Transitions Collected: 50619      Batch Size: 256   | NewEpisode Model(mean:9689      ) Reward(mean:1.34 , max:2.63 , min:0.33 , std:0.58 ) | total_loss: 1.130   reward_loss: 0.030   policy_loss: 2.112   value_loss: 0.322   consistency_loss: -0.547  lr: 0.000100  batch_future_return: 0.609   batch_model_diff: 4432.812target_model_diff: 200.000 Tp_perstep: 0.160   Tu_perstep: 0.050   
[2025-06-02 20:38:37,602][train][WARNING][train.py>_train] ==> #9850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:38:48,773][train][INFO][log.py>_log] ==> #9900       Episodes Collected: 1871       Transitions Collected: 51167      Batch Size: 256   | NewEpisode Model(mean:9792      ) Reward(mean:1.11 , max:2.63 , min:0.33 , std:0.50 ) | total_loss: 4.233   reward_loss: 0.097   policy_loss: 7.390   value_loss: 1.062   consistency_loss: -1.760  lr: 0.000100  batch_future_return: 0.663   batch_model_diff: 4309.375target_model_diff: 100.000 Tp_perstep: 0.160   Tu_perstep: 0.050   
[2025-06-02 20:39:10,946][train][INFO][log.py>_log] ==> #10000      Episodes Collected: 1891       Transitions Collected: 51733      Batch Size: 256   | NewEpisode Model(mean:9891      ) Reward(mean:1.07 , max:1.97 , min:0.33 , std:0.40 ) | total_loss: 2.568   reward_loss: 0.080   policy_loss: 4.798   value_loss: 0.702   consistency_loss: -1.243  lr: 0.000100  batch_future_return: 0.685   batch_model_diff: 4164.062target_model_diff: 200.000 Tp_perstep: 0.160   Tu_perstep: 0.050   
[2025-06-02 20:39:10,948][train][WARNING][train.py>_train] ==> #10000 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:39:33,199][train][INFO][log.py>_log] ==> #10100      Episodes Collected: 1914       Transitions Collected: 52334      Batch Size: 256   | NewEpisode Model(mean:9987      ) Reward(mean:1.16 , max:2.30 , min:0.33 , std:0.56 ) | total_loss: 2.372   reward_loss: 0.080   policy_loss: 4.274   value_loss: 0.608   consistency_loss: -1.067  lr: 0.000100  batch_future_return: 0.698   batch_model_diff: 4798.828target_model_diff: 100.000 Tp_perstep: 0.160   Tu_perstep: 0.050   
[2025-06-02 20:39:33,201][train][WARNING][train.py>_train] ==> #10100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:39:55,602][train][INFO][log.py>_log] ==> #10200      Episodes Collected: 1939       Transitions Collected: 52958      Batch Size: 256   | NewEpisode Model(mean:10089     ) Reward(mean:1.26 , max:1.97 , min:0.66 , std:0.32 ) | total_loss: 3.411   reward_loss: 0.097   policy_loss: 5.814   value_loss: 0.846   consistency_loss: -1.356  lr: 0.000100  batch_future_return: 0.741   batch_model_diff: 4433.203target_model_diff: 200.000 Tp_perstep: 0.160   Tu_perstep: 0.050   
[2025-06-02 20:39:55,604][train][WARNING][train.py>_train] ==> #10200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:40:17,880][train][INFO][log.py>_log] ==> #10300      Episodes Collected: 1960       Transitions Collected: 53520      Batch Size: 256   | NewEpisode Model(mean:10194     ) Reward(mean:1.13 , max:1.97 , min:0.66 , std:0.43 ) | total_loss: 1.887   reward_loss: 0.050   policy_loss: 3.515   value_loss: 0.519   consistency_loss: -0.904  lr: 0.000100  batch_future_return: 0.584   batch_model_diff: 4427.344target_model_diff: 100.000 Tp_perstep: 0.160   Tu_perstep: 0.050   
[2025-06-02 20:40:17,882][train][WARNING][train.py>_train] ==> #10300 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:40:29,032][train][WARNING][train.py>_train] ==> #10350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:40:40,618][train][INFO][log.py>_log] ==> #10400      Episodes Collected: 1985       Transitions Collected: 54161      Batch Size: 256   | NewEpisode Model(mean:10290     ) Reward(mean:1.47 , max:3.62 , min:0.66 , std:0.65 ) | total_loss: 3.424   reward_loss: 0.108   policy_loss: 6.143   value_loss: 0.882   consistency_loss: -1.524  lr: 0.000100  batch_future_return: 0.676   batch_model_diff: 4546.875target_model_diff: 200.000 Tp_perstep: 0.161   Tu_perstep: 0.050   
[2025-06-02 20:40:40,620][train][WARNING][train.py>_train] ==> #10400 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:40:52,231][train][WARNING][train.py>_train] ==> #10450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:41:03,877][train][INFO][log.py>_log] ==> #10500      Episodes Collected: 2008       Transitions Collected: 54802      Batch Size: 256   | NewEpisode Model(mean:10393     ) Reward(mean:1.24 , max:2.30 , min:0.33 , std:0.58 ) | total_loss: 2.779   reward_loss: 0.070   policy_loss: 4.732   value_loss: 0.692   consistency_loss: -1.098  lr: 0.000100  batch_future_return: 0.699   batch_model_diff: 4753.125target_model_diff: 100.000 Tp_perstep: 0.161   Tu_perstep: 0.050   
[2025-06-02 20:41:14,982][train][WARNING][train.py>_train] ==> #10550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:41:26,612][train][INFO][log.py>_log] ==> #10600      Episodes Collected: 2029       Transitions Collected: 55373      Batch Size: 256   | NewEpisode Model(mean:10496     ) Reward(mean:1.28 , max:2.63 , min:0.66 , std:0.51 ) | total_loss: 4.011   reward_loss: 0.104   policy_loss: 7.103   value_loss: 1.049   consistency_loss: -1.730  lr: 0.000100  batch_future_return: 0.683   batch_model_diff: 4602.344target_model_diff: 200.000 Tp_perstep: 0.161   Tu_perstep: 0.050   
[2025-06-02 20:41:37,287][train][WARNING][train.py>_train] ==> #10650 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:41:48,990][train][INFO][log.py>_log] ==> #10700      Episodes Collected: 2051       Transitions Collected: 55967      Batch Size: 256   | NewEpisode Model(mean:10592     ) Reward(mean:1.15 , max:2.30 , min:0.33 , std:0.49 ) | total_loss: 4.366   reward_loss: 0.113   policy_loss: 7.646   value_loss: 1.119   consistency_loss: -1.836  lr: 0.000100  batch_future_return: 0.716   batch_model_diff: 4291.406target_model_diff: 100.000 Tp_perstep: 0.161   Tu_perstep: 0.050   
[2025-06-02 20:41:59,789][train][WARNING][train.py>_train] ==> #10750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:42:11,379][train][INFO][log.py>_log] ==> #10800      Episodes Collected: 2075       Transitions Collected: 56587      Batch Size: 256   | NewEpisode Model(mean:10692     ) Reward(mean:1.26 , max:2.63 , min:0.33 , std:0.70 ) | total_loss: 2.736   reward_loss: 0.070   policy_loss: 4.646   value_loss: 0.677   consistency_loss: -1.075  lr: 0.000100  batch_future_return: 0.731   batch_model_diff: 4614.453target_model_diff: 200.000 Tp_perstep: 0.161   Tu_perstep: 0.050   
[2025-06-02 20:42:22,493][train][WARNING][train.py>_train] ==> #10850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:42:34,138][train][INFO][log.py>_log] ==> #10900      Episodes Collected: 2096       Transitions Collected: 57173      Batch Size: 256   | NewEpisode Model(mean:10793     ) Reward(mean:1.35 , max:2.30 , min:0.33 , std:0.45 ) | total_loss: 1.160   reward_loss: 0.033   policy_loss: 2.073   value_loss: 0.314   consistency_loss: -0.512  lr: 0.000100  batch_future_return: 0.632   batch_model_diff: 4581.641target_model_diff: 100.000 Tp_perstep: 0.161   Tu_perstep: 0.050   
[2025-06-02 20:42:34,141][train][WARNING][train.py>_train] ==> #10900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:42:56,989][train][INFO][log.py>_log] ==> #11000      Episodes Collected: 2120       Transitions Collected: 57803      Batch Size: 256   | NewEpisode Model(mean:10890     ) Reward(mean:1.44 , max:4.00 , min:0.33 , std:0.74 ) | total_loss: 3.589   reward_loss: 0.098   policy_loss: 6.161   value_loss: 0.909   consistency_loss: -1.449  lr: 0.000100  batch_future_return: 0.743   batch_model_diff: 4801.172target_model_diff: 200.000 Tp_perstep: 0.162   Tu_perstep: 0.050   
[2025-06-02 20:43:19,859][train][INFO][log.py>_log] ==> #11100      Episodes Collected: 2144       Transitions Collected: 58393      Batch Size: 256   | NewEpisode Model(mean:10991     ) Reward(mean:1.62 , max:2.63 , min:0.99 , std:0.49 ) | total_loss: 2.364   reward_loss: 0.071   policy_loss: 4.286   value_loss: 0.634   consistency_loss: -1.076  lr: 0.000100  batch_future_return: 0.673   batch_model_diff: 4183.984target_model_diff: 100.000 Tp_perstep: 0.162   Tu_perstep: 0.050   
[2025-06-02 20:43:41,962][train][INFO][log.py>_log] ==> #11200      Episodes Collected: 2169       Transitions Collected: 59044      Batch Size: 256   | NewEpisode Model(mean:11091     ) Reward(mean:1.13 , max:2.63 , min:0.33 , std:0.53 ) | total_loss: 2.640   reward_loss: 0.064   policy_loss: 4.514   value_loss: 0.638   consistency_loss: -1.049  lr: 0.000100  batch_future_return: 0.692   batch_model_diff: 4974.609target_model_diff: 200.000 Tp_perstep: 0.162   Tu_perstep: 0.050   
[2025-06-02 20:43:41,964][train][WARNING][train.py>_train] ==> #11200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:44:06,131][train][INFO][log.py>_log] ==> #11300      Episodes Collected: 2193       Transitions Collected: 59669      Batch Size: 256   | NewEpisode Model(mean:11195     ) Reward(mean:1.12 , max:2.30 , min:0.33 , std:0.42 ) | total_loss: 3.238   reward_loss: 0.089   policy_loss: 5.767   value_loss: 0.855   consistency_loss: -1.416  lr: 0.000100  batch_future_return: 0.667   batch_model_diff: 4913.672target_model_diff: 100.000 Tp_perstep: 0.162   Tu_perstep: 0.050   
[2025-06-02 20:44:17,749][train][WARNING][train.py>_train] ==> #11350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:44:29,989][train][INFO][log.py>_log] ==> #11400      Episodes Collected: 2217       Transitions Collected: 60352      Batch Size: 256   | NewEpisode Model(mean:11292     ) Reward(mean:1.25 , max:2.96 , min:0.33 , std:0.56 ) | total_loss: 1.005   reward_loss: 0.025   policy_loss: 1.746   value_loss: 0.253   consistency_loss: -0.414  lr: 0.000100  batch_future_return: 0.730   batch_model_diff: 5138.672target_model_diff: 200.000 Tp_perstep: 0.162   Tu_perstep: 0.050   
[2025-06-02 20:44:52,344][train][INFO][log.py>_log] ==> #11500      Episodes Collected: 2242       Transitions Collected: 60958      Batch Size: 256   | NewEpisode Model(mean:11390     ) Reward(mean:1.46 , max:3.62 , min:0.99 , std:0.56 ) | total_loss: 1.185   reward_loss: 0.034   policy_loss: 2.047   value_loss: 0.302   consistency_loss: -0.485  lr: 0.000100  batch_future_return: 0.717   batch_model_diff: 5071.875target_model_diff: 100.000 Tp_perstep: 0.162   Tu_perstep: 0.050   
[2025-06-02 20:44:52,346][train][WARNING][train.py>_train] ==> #11500 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:45:03,443][train][WARNING][train.py>_train] ==> #11550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:45:14,656][train][INFO][log.py>_log] ==> #11600      Episodes Collected: 2262       Transitions Collected: 61544      Batch Size: 256   | NewEpisode Model(mean:11492     ) Reward(mean:1.36 , max:2.63 , min:0.33 , std:0.59 ) | total_loss: 2.491   reward_loss: 0.069   policy_loss: 4.423   value_loss: 0.640   consistency_loss: -1.080  lr: 0.000100  batch_future_return: 0.629   batch_model_diff: 5012.891target_model_diff: 200.000 Tp_perstep: 0.162   Tu_perstep: 0.050   
[2025-06-02 20:45:14,658][train][WARNING][train.py>_train] ==> #11600 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:45:37,463][train][INFO][log.py>_log] ==> #11700      Episodes Collected: 2286       Transitions Collected: 62150      Batch Size: 256   | NewEpisode Model(mean:11590     ) Reward(mean:1.29 , max:2.63 , min:0.33 , std:0.56 ) | total_loss: 2.582   reward_loss: 0.053   policy_loss: 4.322   value_loss: 0.599   consistency_loss: -0.971  lr: 0.000100  batch_future_return: 0.636   batch_model_diff: 5050.781target_model_diff: 100.000 Tp_perstep: 0.163   Tu_perstep: 0.050   
[2025-06-02 20:45:37,466][train][WARNING][train.py>_train] ==> #11700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:45:48,684][train][WARNING][train.py>_train] ==> #11750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:46:00,332][train][INFO][log.py>_log] ==> #11800      Episodes Collected: 2306       Transitions Collected: 62718      Batch Size: 256   | NewEpisode Model(mean:11691     ) Reward(mean:1.23 , max:2.63 , min:0.33 , std:0.57 ) | total_loss: 3.648   reward_loss: 0.094   policy_loss: 6.054   value_loss: 0.862   consistency_loss: -1.358  lr: 0.000100  batch_future_return: 0.723   batch_model_diff: 5148.047target_model_diff: 200.000 Tp_perstep: 0.163   Tu_perstep: 0.050   
[2025-06-02 20:46:23,087][train][INFO][log.py>_log] ==> #11900      Episodes Collected: 2329       Transitions Collected: 63411      Batch Size: 256   | NewEpisode Model(mean:11789     ) Reward(mean:1.07 , max:1.64 , min:0.33 , std:0.32 ) | total_loss: 1.914   reward_loss: 0.061   policy_loss: 3.414   value_loss: 0.524   consistency_loss: -0.846  lr: 0.000100  batch_future_return: 0.731   batch_model_diff: 4893.359target_model_diff: 100.000 Tp_perstep: 0.163   Tu_perstep: 0.050   
[2025-06-02 20:46:35,141][train][WARNING][train.py>_train] ==> #11950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:46:47,245][train][INFO][log.py>_log] ==> #12000      Episodes Collected: 2354       Transitions Collected: 64092      Batch Size: 256   | NewEpisode Model(mean:11894     ) Reward(mean:1.30 , max:2.96 , min:0.66 , std:0.50 ) | total_loss: 2.003   reward_loss: 0.068   policy_loss: 3.802   value_loss: 0.578   consistency_loss: -1.006  lr: 0.000100  batch_future_return: 0.700   batch_model_diff: 5232.422target_model_diff: 200.000 Tp_perstep: 0.163   Tu_perstep: 0.050   
[2025-06-02 20:46:58,354][train][WARNING][train.py>_train] ==> #12050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:47:11,025][train][INFO][log.py>_log] ==> #12100      Episodes Collected: 2382       Transitions Collected: 64784      Batch Size: 256   | NewEpisode Model(mean:11994     ) Reward(mean:1.28 , max:2.63 , min:0.66 , std:0.51 ) | total_loss: 4.495   reward_loss: 0.109   policy_loss: 7.429   value_loss: 1.025   consistency_loss: -1.650  lr: 0.000100  batch_future_return: 0.767   batch_model_diff: 5109.375target_model_diff: 100.000 Tp_perstep: 0.163   Tu_perstep: 0.050   
[2025-06-02 20:47:22,236][train][WARNING][train.py>_train] ==> #12150 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:47:33,870][train][INFO][log.py>_log] ==> #12200      Episodes Collected: 2404       Transitions Collected: 65367      Batch Size: 256   | NewEpisode Model(mean:12098     ) Reward(mean:1.30 , max:1.97 , min:0.33 , std:0.38 ) | total_loss: 3.491   reward_loss: 0.102   policy_loss: 6.146   value_loss: 0.876   consistency_loss: -1.487  lr: 0.000100  batch_future_return: 0.675   batch_model_diff: 5152.344target_model_diff: 200.000 Tp_perstep: 0.163   Tu_perstep: 0.050   
[2025-06-02 20:47:33,872][train][WARNING][train.py>_train] ==> #12200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:47:57,232][train][INFO][log.py>_log] ==> #12300      Episodes Collected: 2424       Transitions Collected: 65990      Batch Size: 256   | NewEpisode Model(mean:12192     ) Reward(mean:1.25 , max:2.63 , min:0.33 , std:0.56 ) | total_loss: 0.929   reward_loss: 0.026   policy_loss: 1.634   value_loss: 0.236   consistency_loss: -0.395  lr: 0.000100  batch_future_return: 0.669   batch_model_diff: 5112.109target_model_diff: 100.000 Tp_perstep: 0.164   Tu_perstep: 0.050   
[2025-06-02 20:48:20,579][train][INFO][log.py>_log] ==> #12400      Episodes Collected: 2446       Transitions Collected: 66606      Batch Size: 256   | NewEpisode Model(mean:12292     ) Reward(mean:1.18 , max:1.97 , min:0.33 , std:0.42 ) | total_loss: 2.093   reward_loss: 0.056   policy_loss: 3.648   value_loss: 0.531   consistency_loss: -0.872  lr: 0.000100  batch_future_return: 0.696   batch_model_diff: 5041.406target_model_diff: 200.000 Tp_perstep: 0.164   Tu_perstep: 0.050   
[2025-06-02 20:48:32,232][train][WARNING][train.py>_train] ==> #12450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:48:43,903][train][INFO][log.py>_log] ==> #12500      Episodes Collected: 2469       Transitions Collected: 67263      Batch Size: 256   | NewEpisode Model(mean:12389     ) Reward(mean:1.30 , max:3.29 , min:0.00 , std:0.71 ) | total_loss: 2.393   reward_loss: 0.063   policy_loss: 4.115   value_loss: 0.599   consistency_loss: -0.967  lr: 0.000100  batch_future_return: 0.700   batch_model_diff: 4910.938target_model_diff: 100.000 Tp_perstep: 0.164   Tu_perstep: 0.050   
[2025-06-02 20:48:55,125][train][WARNING][train.py>_train] ==> #12550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:49:07,208][train][INFO][log.py>_log] ==> #12600      Episodes Collected: 2495       Transitions Collected: 67943      Batch Size: 256   | NewEpisode Model(mean:12489     ) Reward(mean:1.21 , max:2.30 , min:0.33 , std:0.50 ) | total_loss: 1.977   reward_loss: 0.057   policy_loss: 3.443   value_loss: 0.514   consistency_loss: -0.826  lr: 0.000100  batch_future_return: 0.720   batch_model_diff: 5001.953target_model_diff: 200.000 Tp_perstep: 0.164   Tu_perstep: 0.050   
[2025-06-02 20:49:07,210][train][WARNING][train.py>_train] ==> #12600 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:49:19,165][train][WARNING][train.py>_train] ==> #12650 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:49:31,232][train][INFO][log.py>_log] ==> #12700      Episodes Collected: 2521       Transitions Collected: 68651      Batch Size: 256   | NewEpisode Model(mean:12593     ) Reward(mean:1.11 , max:2.30 , min:0.33 , std:0.55 ) | total_loss: 3.168   reward_loss: 0.084   policy_loss: 5.311   value_loss: 0.768   consistency_loss: -1.210  lr: 0.000100  batch_future_return: 0.735   batch_model_diff: 5186.328target_model_diff: 100.000 Tp_perstep: 0.164   Tu_perstep: 0.050   
[2025-06-02 20:49:31,234][train][WARNING][train.py>_train] ==> #12700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:49:43,744][train][WARNING][train.py>_train] ==> #12750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:49:55,698][train][INFO][log.py>_log] ==> #12800      Episodes Collected: 2544       Transitions Collected: 69312      Batch Size: 256   | NewEpisode Model(mean:12695     ) Reward(mean:1.22 , max:2.63 , min:0.66 , std:0.52 ) | total_loss: 5.357   reward_loss: 0.140   policy_loss: 9.038   value_loss: 1.280   consistency_loss: -2.070  lr: 0.000100  batch_future_return: 0.703   batch_model_diff: 5140.234target_model_diff: 200.000 Tp_perstep: 0.165   Tu_perstep: 0.050   
[2025-06-02 20:49:55,700][train][WARNING][train.py>_train] ==> #12800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:50:19,019][train][INFO][log.py>_log] ==> #12900      Episodes Collected: 2567       Transitions Collected: 69943      Batch Size: 256   | NewEpisode Model(mean:12787     ) Reward(mean:1.04 , max:1.97 , min:0.33 , std:0.43 ) | total_loss: 3.656   reward_loss: 0.091   policy_loss: 6.238   value_loss: 0.907   consistency_loss: -1.450  lr: 0.000100  batch_future_return: 0.706   batch_model_diff: 5128.516target_model_diff: 100.000 Tp_perstep: 0.165   Tu_perstep: 0.050   
[2025-06-02 20:50:30,565][train][WARNING][train.py>_train] ==> #12950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:50:42,198][train][INFO][log.py>_log] ==> #13000      Episodes Collected: 2592       Transitions Collected: 70620      Batch Size: 256   | NewEpisode Model(mean:12891     ) Reward(mean:1.39 , max:2.63 , min:0.33 , std:0.62 ) | total_loss: 2.162   reward_loss: 0.059   policy_loss: 3.702   value_loss: 0.528   consistency_loss: -0.866  lr: 0.000100  batch_future_return: 0.710   batch_model_diff: 5116.797target_model_diff: 200.000 Tp_perstep: 0.165   Tu_perstep: 0.050   
[2025-06-02 20:50:42,200][train][WARNING][train.py>_train] ==> #13000 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:51:05,378][train][INFO][log.py>_log] ==> #13100      Episodes Collected: 2614       Transitions Collected: 71218      Batch Size: 256   | NewEpisode Model(mean:12995     ) Reward(mean:1.36 , max:2.30 , min:0.33 , std:0.57 ) | total_loss: 2.213   reward_loss: 0.057   policy_loss: 3.814   value_loss: 0.565   consistency_loss: -0.900  lr: 0.000100  batch_future_return: 0.690   batch_model_diff: 5098.047target_model_diff: 100.000 Tp_perstep: 0.165   Tu_perstep: 0.050   
[2025-06-02 20:51:29,107][train][INFO][log.py>_log] ==> #13200      Episodes Collected: 2640       Transitions Collected: 71896      Batch Size: 256   | NewEpisode Model(mean:13092     ) Reward(mean:1.20 , max:1.97 , min:0.33 , std:0.56 ) | total_loss: 2.022   reward_loss: 0.050   policy_loss: 3.401   value_loss: 0.485   consistency_loss: -0.775  lr: 0.000100  batch_future_return: 0.751   batch_model_diff: 4842.188target_model_diff: 200.000 Tp_perstep: 0.165   Tu_perstep: 0.050   
[2025-06-02 20:51:29,109][train][WARNING][train.py>_train] ==> #13200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:51:41,191][train][WARNING][train.py>_train] ==> #13250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:51:52,799][train][INFO][log.py>_log] ==> #13300      Episodes Collected: 2664       Transitions Collected: 72552      Batch Size: 256   | NewEpisode Model(mean:13195     ) Reward(mean:1.22 , max:3.29 , min:0.33 , std:0.71 ) | total_loss: 4.448   reward_loss: 0.117   policy_loss: 7.381   value_loss: 1.046   consistency_loss: -1.655  lr: 0.000100  batch_future_return: 0.783   batch_model_diff: 5178.125target_model_diff: 100.000 Tp_perstep: 0.165   Tu_perstep: 0.050   
[2025-06-02 20:52:04,460][train][WARNING][train.py>_train] ==> #13350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:52:16,981][train][INFO][log.py>_log] ==> #13400      Episodes Collected: 2689       Transitions Collected: 73231      Batch Size: 256   | NewEpisode Model(mean:13292     ) Reward(mean:1.35 , max:2.63 , min:0.33 , std:0.50 ) | total_loss: 3.503   reward_loss: 0.102   policy_loss: 6.151   value_loss: 0.885   consistency_loss: -1.485  lr: 0.000100  batch_future_return: 0.656   batch_model_diff: 5676.172target_model_diff: 200.000 Tp_perstep: 0.166   Tu_perstep: 0.050   
[2025-06-02 20:52:28,554][train][WARNING][train.py>_train] ==> #13450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:52:40,464][train][INFO][log.py>_log] ==> #13500      Episodes Collected: 2714       Transitions Collected: 73869      Batch Size: 256   | NewEpisode Model(mean:13389     ) Reward(mean:1.17 , max:1.97 , min:0.33 , std:0.42 ) | total_loss: 2.993   reward_loss: 0.078   policy_loss: 5.184   value_loss: 0.774   consistency_loss: -1.232  lr: 0.000100  batch_future_return: 0.723   batch_model_diff: 5335.156target_model_diff: 100.000 Tp_perstep: 0.166   Tu_perstep: 0.050   
[2025-06-02 20:52:51,979][train][WARNING][train.py>_train] ==> #13550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:53:03,954][train][INFO][log.py>_log] ==> #13600      Episodes Collected: 2738       Transitions Collected: 74576      Batch Size: 256   | NewEpisode Model(mean:13490     ) Reward(mean:1.25 , max:1.97 , min:0.66 , std:0.38 ) | total_loss: 2.819   reward_loss: 0.079   policy_loss: 4.802   value_loss: 0.695   consistency_loss: -1.117  lr: 0.000100  batch_future_return: 0.691   batch_model_diff: 5169.531target_model_diff: 200.000 Tp_perstep: 0.166   Tu_perstep: 0.050   
[2025-06-02 20:53:03,957][train][WARNING][train.py>_train] ==> #13600 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:53:27,174][train][INFO][log.py>_log] ==> #13700      Episodes Collected: 2760       Transitions Collected: 75186      Batch Size: 256   | NewEpisode Model(mean:13596     ) Reward(mean:1.18 , max:2.30 , min:0.00 , std:0.49 ) | total_loss: 4.346   reward_loss: 0.124   policy_loss: 7.459   value_loss: 1.119   consistency_loss: -1.758  lr: 0.000100  batch_future_return: 0.731   batch_model_diff: 5460.156target_model_diff: 100.000 Tp_perstep: 0.166   Tu_perstep: 0.050   
[2025-06-02 20:53:27,177][train][WARNING][train.py>_train] ==> #13700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:53:50,471][train][INFO][log.py>_log] ==> #13800      Episodes Collected: 2784       Transitions Collected: 75782      Batch Size: 256   | NewEpisode Model(mean:13690     ) Reward(mean:1.37 , max:2.30 , min:0.66 , std:0.46 ) | total_loss: 2.707   reward_loss: 0.077   policy_loss: 4.654   value_loss: 0.689   consistency_loss: -1.098  lr: 0.000100  batch_future_return: 0.701   batch_model_diff: 5237.891target_model_diff: 200.000 Tp_perstep: 0.166   Tu_perstep: 0.050   
[2025-06-02 20:54:14,229][train][INFO][log.py>_log] ==> #13900      Episodes Collected: 2808       Transitions Collected: 76448      Batch Size: 256   | NewEpisode Model(mean:13790     ) Reward(mean:1.07 , max:2.30 , min:0.00 , std:0.51 ) | total_loss: 3.261   reward_loss: 0.080   policy_loss: 5.695   value_loss: 0.816   consistency_loss: -1.359  lr: 0.000100  batch_future_return: 0.640   batch_model_diff: 5348.438target_model_diff: 100.000 Tp_perstep: 0.166   Tu_perstep: 0.050   
[2025-06-02 20:54:37,630][train][INFO][log.py>_log] ==> #14000      Episodes Collected: 2835       Transitions Collected: 77166      Batch Size: 256   | NewEpisode Model(mean:13890     ) Reward(mean:1.19 , max:2.63 , min:0.33 , std:0.54 ) | total_loss: 2.424   reward_loss: 0.069   policy_loss: 4.353   value_loss: 0.649   consistency_loss: -1.080  lr: 0.000100  batch_future_return: 0.701   batch_model_diff: 5343.359target_model_diff: 200.000 Tp_perstep: 0.167   Tu_perstep: 0.050   
[2025-06-02 20:55:01,289][train][INFO][log.py>_log] ==> #14100      Episodes Collected: 2858       Transitions Collected: 77797      Batch Size: 256   | NewEpisode Model(mean:13995     ) Reward(mean:1.20 , max:2.63 , min:0.33 , std:0.56 ) | total_loss: 0.877   reward_loss: 0.025   policy_loss: 1.526   value_loss: 0.217   consistency_loss: -0.364  lr: 0.000100  batch_future_return: 0.685   batch_model_diff: 5657.422target_model_diff: 100.000 Tp_perstep: 0.167   Tu_perstep: 0.050   
[2025-06-02 20:55:01,290][train][WARNING][train.py>_train] ==> #14100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:55:12,941][train][WARNING][train.py>_train] ==> #14150 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:55:24,607][train][INFO][log.py>_log] ==> #14200      Episodes Collected: 2883       Transitions Collected: 78442      Batch Size: 256   | NewEpisode Model(mean:14092     ) Reward(mean:1.25 , max:1.97 , min:0.33 , std:0.53 ) | total_loss: 3.191   reward_loss: 0.080   policy_loss: 5.299   value_loss: 0.749   consistency_loss: -1.188  lr: 0.000100  batch_future_return: 0.736   batch_model_diff: 5836.328target_model_diff: 200.000 Tp_perstep: 0.167   Tu_perstep: 0.050   
[2025-06-02 20:55:24,609][train][WARNING][train.py>_train] ==> #14200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:55:36,216][train][WARNING][train.py>_train] ==> #14250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:55:47,869][train][INFO][log.py>_log] ==> #14300      Episodes Collected: 2905       Transitions Collected: 79029      Batch Size: 256   | NewEpisode Model(mean:14190     ) Reward(mean:1.42 , max:2.63 , min:0.33 , std:0.62 ) | total_loss: 3.088   reward_loss: 0.088   policy_loss: 5.508   value_loss: 0.812   consistency_loss: -1.355  lr: 0.000100  batch_future_return: 0.633   batch_model_diff: 5486.328target_model_diff: 100.000 Tp_perstep: 0.167   Tu_perstep: 0.050   
[2025-06-02 20:56:11,158][train][INFO][log.py>_log] ==> #14400      Episodes Collected: 2927       Transitions Collected: 79672      Batch Size: 256   | NewEpisode Model(mean:14288     ) Reward(mean:1.05 , max:1.64 , min:0.00 , std:0.49 ) | total_loss: 3.652   reward_loss: 0.104   policy_loss: 6.431   value_loss: 0.942   consistency_loss: -1.559  lr: 0.000100  batch_future_return: 0.679   batch_model_diff: 5746.875target_model_diff: 200.000 Tp_perstep: 0.167   Tu_perstep: 0.050   
[2025-06-02 20:56:22,289][train][WARNING][train.py>_train] ==> #14450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:56:34,407][train][INFO][log.py>_log] ==> #14500      Episodes Collected: 2949       Transitions Collected: 80335      Batch Size: 256   | NewEpisode Model(mean:14389     ) Reward(mean:1.26 , max:1.97 , min:0.00 , std:0.49 ) | total_loss: 2.137   reward_loss: 0.056   policy_loss: 3.660   value_loss: 0.548   consistency_loss: -0.858  lr: 0.000100  batch_future_return: 0.702   batch_model_diff: 5525.391target_model_diff: 100.000 Tp_perstep: 0.167   Tu_perstep: 0.050   
[2025-06-02 20:56:34,409][train][WARNING][train.py>_train] ==> #14500 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:56:46,471][train][WARNING][train.py>_train] ==> #14550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:56:58,548][train][INFO][log.py>_log] ==> #14600      Episodes Collected: 2972       Transitions Collected: 81012      Batch Size: 256   | NewEpisode Model(mean:14491     ) Reward(mean:1.16 , max:1.97 , min:0.33 , std:0.37 ) | total_loss: 1.810   reward_loss: 0.044   policy_loss: 3.079   value_loss: 0.456   consistency_loss: -0.714  lr: 0.000100  batch_future_return: 0.687   batch_model_diff: 5649.609target_model_diff: 200.000 Tp_perstep: 0.167   Tu_perstep: 0.050   
[2025-06-02 20:57:10,190][train][WARNING][train.py>_train] ==> #14650 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:57:21,875][train][INFO][log.py>_log] ==> #14700      Episodes Collected: 2996       Transitions Collected: 81678      Batch Size: 256   | NewEpisode Model(mean:14591     ) Reward(mean:1.38 , max:2.63 , min:0.33 , std:0.50 ) | total_loss: 1.901   reward_loss: 0.048   policy_loss: 3.241   value_loss: 0.478   consistency_loss: -0.754  lr: 0.000100  batch_future_return: 0.710   batch_model_diff: 5696.875target_model_diff: 100.000 Tp_perstep: 0.167   Tu_perstep: 0.050   
[2025-06-02 20:57:21,878][train][WARNING][train.py>_train] ==> #14700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:57:34,097][train][WARNING][train.py>_train] ==> #14750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:57:45,699][train][INFO][log.py>_log] ==> #14800      Episodes Collected: 3021       Transitions Collected: 82341      Batch Size: 256   | NewEpisode Model(mean:14693     ) Reward(mean:1.35 , max:2.63 , min:0.00 , std:0.66 ) | total_loss: 2.344   reward_loss: 0.064   policy_loss: 4.208   value_loss: 0.648   consistency_loss: -1.045  lr: 0.000100  batch_future_return: 0.672   batch_model_diff: 6307.812target_model_diff: 200.000 Tp_perstep: 0.168   Tu_perstep: 0.050   
[2025-06-02 20:58:09,534][train][INFO][log.py>_log] ==> #14900      Episodes Collected: 3046       Transitions Collected: 83006      Batch Size: 256   | NewEpisode Model(mean:14793     ) Reward(mean:1.38 , max:2.96 , min:0.33 , std:0.62 ) | total_loss: 2.847   reward_loss: 0.075   policy_loss: 4.795   value_loss: 0.686   consistency_loss: -1.097  lr: 0.000100  batch_future_return: 0.647   batch_model_diff: 5900.000target_model_diff: 100.000 Tp_perstep: 0.168   Tu_perstep: 0.050   
[2025-06-02 20:58:09,536][train][WARNING][train.py>_train] ==> #14900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:58:33,227][train][INFO][log.py>_log] ==> #15000      Episodes Collected: 3071       Transitions Collected: 83651      Batch Size: 256   | NewEpisode Model(mean:14890     ) Reward(mean:1.34 , max:2.63 , min:0.33 , std:0.54 ) | total_loss: 3.889   reward_loss: 0.101   policy_loss: 7.050   value_loss: 1.047   consistency_loss: -1.762  lr: 0.000100  batch_future_return: 0.653   batch_model_diff: 6196.484target_model_diff: 200.000 Tp_perstep: 0.168   Tu_perstep: 0.050   
[2025-06-02 20:58:33,230][train][WARNING][train.py>_train] ==> #15000 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:58:57,072][train][INFO][log.py>_log] ==> #15100      Episodes Collected: 3097       Transitions Collected: 84355      Batch Size: 256   | NewEpisode Model(mean:14989     ) Reward(mean:1.28 , max:2.30 , min:0.33 , std:0.52 ) | total_loss: 2.188   reward_loss: 0.072   policy_loss: 4.036   value_loss: 0.606   consistency_loss: -1.036  lr: 0.000100  batch_future_return: 0.646   batch_model_diff: 6058.203target_model_diff: 100.000 Tp_perstep: 0.168   Tu_perstep: 0.050   
[2025-06-02 20:59:21,262][train][INFO][log.py>_log] ==> #15200      Episodes Collected: 3121       Transitions Collected: 84999      Batch Size: 256   | NewEpisode Model(mean:15093     ) Reward(mean:1.14 , max:1.97 , min:0.00 , std:0.40 ) | total_loss: 3.466   reward_loss: 0.085   policy_loss: 6.052   value_loss: 0.933   consistency_loss: -1.452  lr: 0.000100  batch_future_return: 0.724   batch_model_diff: 5564.062target_model_diff: 200.000 Tp_perstep: 0.168   Tu_perstep: 0.050   
[2025-06-02 20:59:44,879][train][INFO][log.py>_log] ==> #15300      Episodes Collected: 3147       Transitions Collected: 85691      Batch Size: 256   | NewEpisode Model(mean:15188     ) Reward(mean:1.19 , max:2.63 , min:0.33 , std:0.55 ) | total_loss: 3.345   reward_loss: 0.091   policy_loss: 5.854   value_loss: 0.893   consistency_loss: -1.411  lr: 0.000100  batch_future_return: 0.669   batch_model_diff: 5834.766target_model_diff: 100.000 Tp_perstep: 0.168   Tu_perstep: 0.050   
[2025-06-02 20:59:44,882][train][WARNING][train.py>_train] ==> #15300 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 20:59:56,932][train][WARNING][train.py>_train] ==> #15350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:00:09,495][train][INFO][log.py>_log] ==> #15400      Episodes Collected: 3172       Transitions Collected: 86390      Batch Size: 256   | NewEpisode Model(mean:15293     ) Reward(mean:1.29 , max:2.96 , min:0.33 , std:0.57 ) | total_loss: 3.517   reward_loss: 0.102   policy_loss: 6.350   value_loss: 0.937   consistency_loss: -1.585  lr: 0.000100  batch_future_return: 0.636   batch_model_diff: 5832.422target_model_diff: 200.000 Tp_perstep: 0.169   Tu_perstep: 0.049   
[2025-06-02 21:00:09,497][train][WARNING][train.py>_train] ==> #15400 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:00:21,127][train][WARNING][train.py>_train] ==> #15450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:00:33,314][train][INFO][log.py>_log] ==> #15500      Episodes Collected: 3198       Transitions Collected: 87070      Batch Size: 256   | NewEpisode Model(mean:15391     ) Reward(mean:1.34 , max:2.30 , min:0.00 , std:0.50 ) | total_loss: 2.223   reward_loss: 0.049   policy_loss: 3.886   value_loss: 0.565   consistency_loss: -0.927  lr: 0.000100  batch_future_return: 0.597   batch_model_diff: 6032.422target_model_diff: 100.000 Tp_perstep: 0.169   Tu_perstep: 0.049   
[2025-06-02 21:00:33,317][train][WARNING][train.py>_train] ==> #15500 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:00:56,772][train][INFO][log.py>_log] ==> #15600      Episodes Collected: 3221       Transitions Collected: 87683      Batch Size: 256   | NewEpisode Model(mean:15492     ) Reward(mean:1.26 , max:1.97 , min:0.66 , std:0.43 ) | total_loss: 4.057   reward_loss: 0.120   policy_loss: 6.915   value_loss: 1.017   consistency_loss: -1.616  lr: 0.000100  batch_future_return: 0.706   batch_model_diff: 6010.156target_model_diff: 200.000 Tp_perstep: 0.169   Tu_perstep: 0.049   
[2025-06-02 21:01:08,366][train][WARNING][train.py>_train] ==> #15650 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:01:20,558][train][INFO][log.py>_log] ==> #15700      Episodes Collected: 3244       Transitions Collected: 88366      Batch Size: 256   | NewEpisode Model(mean:15590     ) Reward(mean:1.22 , max:2.30 , min:0.00 , std:0.52 ) | total_loss: 2.673   reward_loss: 0.064   policy_loss: 4.446   value_loss: 0.656   consistency_loss: -1.000  lr: 0.000100  batch_future_return: 0.727   batch_model_diff: 6265.234target_model_diff: 100.000 Tp_perstep: 0.169   Tu_perstep: 0.049   
[2025-06-02 21:01:20,560][train][WARNING][train.py>_train] ==> #15700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:01:32,209][train][WARNING][train.py>_train] ==> #15750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:01:44,315][train][INFO][log.py>_log] ==> #15800      Episodes Collected: 3268       Transitions Collected: 89030      Batch Size: 256   | NewEpisode Model(mean:15689     ) Reward(mean:1.18 , max:2.30 , min:0.33 , std:0.57 ) | total_loss: 2.261   reward_loss: 0.064   policy_loss: 3.875   value_loss: 0.571   consistency_loss: -0.910  lr: 0.000100  batch_future_return: 0.687   batch_model_diff: 6084.375target_model_diff: 200.000 Tp_perstep: 0.169   Tu_perstep: 0.049   
[2025-06-02 21:02:08,003][train][INFO][log.py>_log] ==> #15900      Episodes Collected: 3292       Transitions Collected: 89662      Batch Size: 256   | NewEpisode Model(mean:15791     ) Reward(mean:1.37 , max:2.63 , min:0.00 , std:0.63 ) | total_loss: 1.583   reward_loss: 0.038   policy_loss: 2.627   value_loss: 0.383   consistency_loss: -0.589  lr: 0.000100  batch_future_return: 0.745   batch_model_diff: 6095.703target_model_diff: 100.000 Tp_perstep: 0.169   Tu_perstep: 0.049   
[2025-06-02 21:02:08,006][train][WARNING][train.py>_train] ==> #15900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:02:32,019][train][INFO][log.py>_log] ==> #16000      Episodes Collected: 3316       Transitions Collected: 90304      Batch Size: 256   | NewEpisode Model(mean:15889     ) Reward(mean:1.33 , max:2.96 , min:0.66 , std:0.51 ) | total_loss: 1.816   reward_loss: 0.056   policy_loss: 3.183   value_loss: 0.493   consistency_loss: -0.773  lr: 0.000100  batch_future_return: 0.698   batch_model_diff: 6345.312target_model_diff: 200.000 Tp_perstep: 0.169   Tu_perstep: 0.049   
[2025-06-02 21:02:32,021][train][WARNING][train.py>_train] ==> #16000 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:02:55,863][train][INFO][log.py>_log] ==> #16100      Episodes Collected: 3341       Transitions Collected: 91048      Batch Size: 256   | NewEpisode Model(mean:15987     ) Reward(mean:1.29 , max:2.30 , min:0.33 , std:0.47 ) | total_loss: 2.033   reward_loss: 0.047   policy_loss: 3.817   value_loss: 0.601   consistency_loss: -0.991  lr: 0.000100  batch_future_return: 0.591   batch_model_diff: 6050.000target_model_diff: 100.000 Tp_perstep: 0.169   Tu_perstep: 0.049   
[2025-06-02 21:02:55,865][train][WARNING][train.py>_train] ==> #16100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:03:20,029][train][INFO][log.py>_log] ==> #16200      Episodes Collected: 3363       Transitions Collected: 91689      Batch Size: 256   | NewEpisode Model(mean:16091     ) Reward(mean:1.37 , max:2.63 , min:0.33 , std:0.58 ) | total_loss: 2.199   reward_loss: 0.058   policy_loss: 3.607   value_loss: 0.527   consistency_loss: -0.799  lr: 0.000100  batch_future_return: 0.735   batch_model_diff: 6915.625target_model_diff: 200.000 Tp_perstep: 0.170   Tu_perstep: 0.049   
[2025-06-02 21:03:20,032][train][WARNING][train.py>_train] ==> #16200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:03:32,028][train][WARNING][train.py>_train] ==> #16250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:03:44,141][train][INFO][log.py>_log] ==> #16300      Episodes Collected: 3391       Transitions Collected: 92426      Batch Size: 256   | NewEpisode Model(mean:16188     ) Reward(mean:1.33 , max:1.97 , min:0.66 , std:0.44 ) | total_loss: 2.644   reward_loss: 0.070   policy_loss: 4.662   value_loss: 0.672   consistency_loss: -1.128  lr: 0.000100  batch_future_return: 0.642   batch_model_diff: 6682.812target_model_diff: 100.000 Tp_perstep: 0.170   Tu_perstep: 0.049   
[2025-06-02 21:04:08,179][train][INFO][log.py>_log] ==> #16400      Episodes Collected: 3413       Transitions Collected: 93064      Batch Size: 256   | NewEpisode Model(mean:16293     ) Reward(mean:1.12 , max:2.30 , min:0.33 , std:0.47 ) | total_loss: 4.539   reward_loss: 0.128   policy_loss: 7.826   value_loss: 1.170   consistency_loss: -1.853  lr: 0.000100  batch_future_return: 0.690   batch_model_diff: 6456.641target_model_diff: 200.000 Tp_perstep: 0.170   Tu_perstep: 0.049   
[2025-06-02 21:04:31,931][train][INFO][log.py>_log] ==> #16500      Episodes Collected: 3439       Transitions Collected: 93771      Batch Size: 256   | NewEpisode Model(mean:16391     ) Reward(mean:1.11 , max:1.97 , min:0.33 , std:0.43 ) | total_loss: 3.962   reward_loss: 0.103   policy_loss: 6.639   value_loss: 1.008   consistency_loss: -1.516  lr: 0.000100  batch_future_return: 0.744   batch_model_diff: 6279.297target_model_diff: 100.000 Tp_perstep: 0.170   Tu_perstep: 0.049   
[2025-06-02 21:04:43,513][train][WARNING][train.py>_train] ==> #16550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:04:55,645][train][INFO][log.py>_log] ==> #16600      Episodes Collected: 3463       Transitions Collected: 94401      Batch Size: 256   | NewEpisode Model(mean:16493     ) Reward(mean:1.42 , max:2.63 , min:0.00 , std:0.62 ) | total_loss: 3.157   reward_loss: 0.095   policy_loss: 5.626   value_loss: 0.867   consistency_loss: -1.390  lr: 0.000100  batch_future_return: 0.678   batch_model_diff: 6633.594target_model_diff: 200.000 Tp_perstep: 0.170   Tu_perstep: 0.049   
[2025-06-02 21:05:19,335][train][INFO][log.py>_log] ==> #16700      Episodes Collected: 3488       Transitions Collected: 95052      Batch Size: 256   | NewEpisode Model(mean:16590     ) Reward(mean:1.25 , max:2.63 , min:0.00 , std:0.54 ) | total_loss: 1.821   reward_loss: 0.046   policy_loss: 3.165   value_loss: 0.489   consistency_loss: -0.756  lr: 0.000100  batch_future_return: 0.620   batch_model_diff: 6357.031target_model_diff: 100.000 Tp_perstep: 0.170   Tu_perstep: 0.049   
[2025-06-02 21:05:43,552][train][INFO][log.py>_log] ==> #16800      Episodes Collected: 3512       Transitions Collected: 95748      Batch Size: 256   | NewEpisode Model(mean:16691     ) Reward(mean:1.16 , max:1.97 , min:0.33 , std:0.43 ) | total_loss: 1.500   reward_loss: 0.046   policy_loss: 2.879   value_loss: 0.465   consistency_loss: -0.771  lr: 0.000100  batch_future_return: 0.588   batch_model_diff: 6435.938target_model_diff: 200.000 Tp_perstep: 0.170   Tu_perstep: 0.049   
[2025-06-02 21:06:07,419][train][INFO][log.py>_log] ==> #16900      Episodes Collected: 3538       Transitions Collected: 96445      Batch Size: 256   | NewEpisode Model(mean:16790     ) Reward(mean:1.11 , max:2.30 , min:0.00 , std:0.63 ) | total_loss: 2.788   reward_loss: 0.069   policy_loss: 4.739   value_loss: 0.713   consistency_loss: -1.099  lr: 0.000100  batch_future_return: 0.685   batch_model_diff: 6657.812target_model_diff: 100.000 Tp_perstep: 0.170   Tu_perstep: 0.049   
[2025-06-02 21:06:18,635][train][WARNING][train.py>_train] ==> #16950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:06:30,939][train][INFO][log.py>_log] ==> #17000      Episodes Collected: 3560       Transitions Collected: 97047      Batch Size: 256   | NewEpisode Model(mean:16891     ) Reward(mean:1.30 , max:2.30 , min:0.33 , std:0.51 ) | total_loss: 2.077   reward_loss: 0.051   policy_loss: 3.500   value_loss: 0.514   consistency_loss: -0.801  lr: 0.000100  batch_future_return: 0.708   batch_model_diff: 6615.625target_model_diff: 200.000 Tp_perstep: 0.170   Tu_perstep: 0.049   
[2025-06-02 21:06:30,940][train][WARNING][train.py>_train] ==> #17000 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:06:43,023][train][WARNING][train.py>_train] ==> #17050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:06:55,068][train][INFO][log.py>_log] ==> #17100      Episodes Collected: 3586       Transitions Collected: 97777      Batch Size: 256   | NewEpisode Model(mean:16989     ) Reward(mean:1.04 , max:2.30 , min:0.33 , std:0.46 ) | total_loss: 2.818   reward_loss: 0.083   policy_loss: 5.174   value_loss: 0.787   consistency_loss: -1.318  lr: 0.000100  batch_future_return: 0.592   batch_model_diff: 6226.562target_model_diff: 100.000 Tp_perstep: 0.171   Tu_perstep: 0.049   
[2025-06-02 21:07:19,258][train][INFO][log.py>_log] ==> #17200      Episodes Collected: 3613       Transitions Collected: 98453      Batch Size: 256   | NewEpisode Model(mean:17092     ) Reward(mean:1.52 , max:3.62 , min:0.00 , std:0.75 ) | total_loss: 3.576   reward_loss: 0.100   policy_loss: 6.569   value_loss: 1.015   consistency_loss: -1.673  lr: 0.000100  batch_future_return: 0.611   batch_model_diff: 6505.469target_model_diff: 200.000 Tp_perstep: 0.171   Tu_perstep: 0.049   
[2025-06-02 21:07:30,783][train][WARNING][train.py>_train] ==> #17250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:07:42,924][train][INFO][log.py>_log] ==> #17300      Episodes Collected: 3635       Transitions Collected: 99084      Batch Size: 256   | NewEpisode Model(mean:17193     ) Reward(mean:1.30 , max:2.30 , min:0.33 , std:0.55 ) | total_loss: 2.449   reward_loss: 0.059   policy_loss: 4.271   value_loss: 0.656   consistency_loss: -1.022  lr: 0.000100  batch_future_return: 0.697   batch_model_diff: 7085.156target_model_diff: 100.000 Tp_perstep: 0.171   Tu_perstep: 0.049   
[2025-06-02 21:08:06,676][train][INFO][log.py>_log] ==> #17400      Episodes Collected: 3660       Transitions Collected: 99750      Batch Size: 256   | NewEpisode Model(mean:17290     ) Reward(mean:1.32 , max:1.97 , min:0.33 , std:0.39 ) | total_loss: 2.347   reward_loss: 0.057   policy_loss: 4.073   value_loss: 0.613   consistency_loss: -0.968  lr: 0.000100  batch_future_return: 0.730   batch_model_diff: 6446.484target_model_diff: 200.000 Tp_perstep: 0.171   Tu_perstep: 0.049   
[2025-06-02 21:08:30,545][train][INFO][log.py>_log] ==> #17500      Episodes Collected: 3685       Transitions Collected: 100418     Batch Size: 256   | NewEpisode Model(mean:17390     ) Reward(mean:1.38 , max:2.30 , min:0.66 , std:0.45 ) | total_loss: 3.336   reward_loss: 0.080   policy_loss: 5.618   value_loss: 0.846   consistency_loss: -1.287  lr: 0.000100  batch_future_return: 0.705   batch_model_diff: 6971.484target_model_diff: 100.000 Tp_perstep: 0.171   Tu_perstep: 0.049   
[2025-06-02 21:08:30,547][train][WARNING][train.py>_train] ==> #17500 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:08:55,368][train][INFO][log.py>_log] ==> #17600      Episodes Collected: 3714       Transitions Collected: 101152     Batch Size: 256   | NewEpisode Model(mean:17490     ) Reward(mean:1.36 , max:2.30 , min:0.33 , std:0.53 ) | total_loss: 3.281   reward_loss: 0.075   policy_loss: 5.495   value_loss: 0.805   consistency_loss: -1.246  lr: 0.000100  batch_future_return: 0.645   batch_model_diff: 6732.812target_model_diff: 200.000 Tp_perstep: 0.171   Tu_perstep: 0.049   
[2025-06-02 21:09:18,625][train][INFO][log.py>_log] ==> #17700      Episodes Collected: 3738       Transitions Collected: 101791     Batch Size: 256   | NewEpisode Model(mean:17593     ) Reward(mean:1.22 , max:1.97 , min:0.00 , std:0.54 ) | total_loss: 2.394   reward_loss: 0.072   policy_loss: 4.352   value_loss: 0.663   consistency_loss: -1.098  lr: 0.000100  batch_future_return: 0.646   batch_model_diff: 6796.094target_model_diff: 100.000 Tp_perstep: 0.171   Tu_perstep: 0.049   
[2025-06-02 21:09:18,627][train][WARNING][train.py>_train] ==> #17700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:09:42,700][train][INFO][log.py>_log] ==> #17800      Episodes Collected: 3765       Transitions Collected: 102464     Batch Size: 256   | NewEpisode Model(mean:17694     ) Reward(mean:1.32 , max:1.97 , min:0.33 , std:0.50 ) | total_loss: 3.148   reward_loss: 0.078   policy_loss: 5.167   value_loss: 0.759   consistency_loss: -1.144  lr: 0.000100  batch_future_return: 0.731   batch_model_diff: 6933.203target_model_diff: 200.000 Tp_perstep: 0.171   Tu_perstep: 0.049   
[2025-06-02 21:09:42,702][train][WARNING][train.py>_train] ==> #17800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:10:06,995][train][INFO][log.py>_log] ==> #17900      Episodes Collected: 3790       Transitions Collected: 103144     Batch Size: 256   | NewEpisode Model(mean:17791     ) Reward(mean:1.37 , max:2.30 , min:0.00 , std:0.59 ) | total_loss: 3.014   reward_loss: 0.076   policy_loss: 5.008   value_loss: 0.758   consistency_loss: -1.129  lr: 0.000100  batch_future_return: 0.742   batch_model_diff: 7164.453target_model_diff: 100.000 Tp_perstep: 0.172   Tu_perstep: 0.049   
[2025-06-02 21:10:06,998][train][WARNING][train.py>_train] ==> #17900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:10:19,039][train][WARNING][train.py>_train] ==> #17950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:10:30,666][train][INFO][log.py>_log] ==> #18000      Episodes Collected: 3810       Transitions Collected: 103729     Batch Size: 256   | NewEpisode Model(mean:17885     ) Reward(mean:1.33 , max:2.63 , min:0.00 , std:0.58 ) | total_loss: 2.820   reward_loss: 0.077   policy_loss: 4.786   value_loss: 0.716   consistency_loss: -1.111  lr: 0.000100  batch_future_return: 0.700   batch_model_diff: 7310.156target_model_diff: 200.000 Tp_perstep: 0.172   Tu_perstep: 0.049   
[2025-06-02 21:10:30,669][train][WARNING][train.py>_train] ==> #18000 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:10:42,322][train][WARNING][train.py>_train] ==> #18050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:10:54,936][train][INFO][log.py>_log] ==> #18100      Episodes Collected: 3835       Transitions Collected: 104500     Batch Size: 256   | NewEpisode Model(mean:17986     ) Reward(mean:1.33 , max:2.63 , min:0.33 , std:0.58 ) | total_loss: 1.979   reward_loss: 0.051   policy_loss: 3.286   value_loss: 0.491   consistency_loss: -0.740  lr: 0.000100  batch_future_return: 0.734   batch_model_diff: 7009.375target_model_diff: 100.000 Tp_perstep: 0.172   Tu_perstep: 0.049   
[2025-06-02 21:11:19,129][train][INFO][log.py>_log] ==> #18200      Episodes Collected: 3861       Transitions Collected: 105200     Batch Size: 256   | NewEpisode Model(mean:18092     ) Reward(mean:1.32 , max:1.97 , min:0.33 , std:0.49 ) | total_loss: 2.749   reward_loss: 0.075   policy_loss: 4.653   value_loss: 0.683   consistency_loss: -1.075  lr: 0.000100  batch_future_return: 0.724   batch_model_diff: 6894.531target_model_diff: 200.000 Tp_perstep: 0.172   Tu_perstep: 0.049   
[2025-06-02 21:11:42,723][train][INFO][log.py>_log] ==> #18300      Episodes Collected: 3885       Transitions Collected: 105857     Batch Size: 256   | NewEpisode Model(mean:18195     ) Reward(mean:1.33 , max:2.63 , min:0.66 , std:0.48 ) | total_loss: 1.183   reward_loss: 0.034   policy_loss: 2.107   value_loss: 0.311   consistency_loss: -0.517  lr: 0.000100  batch_future_return: 0.636   batch_model_diff: 7134.375target_model_diff: 100.000 Tp_perstep: 0.172   Tu_perstep: 0.049   
[2025-06-02 21:12:06,258][train][INFO][log.py>_log] ==> #18400      Episodes Collected: 3907       Transitions Collected: 106513     Batch Size: 256   | NewEpisode Model(mean:18292     ) Reward(mean:1.37 , max:2.63 , min:0.66 , std:0.50 ) | total_loss: 1.212   reward_loss: 0.033   policy_loss: 2.165   value_loss: 0.336   consistency_loss: -0.535  lr: 0.000100  batch_future_return: 0.682   batch_model_diff: 7492.188target_model_diff: 200.000 Tp_perstep: 0.172   Tu_perstep: 0.049   
[2025-06-02 21:12:18,311][train][WARNING][train.py>_train] ==> #18450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:12:30,483][train][INFO][log.py>_log] ==> #18500      Episodes Collected: 3931       Transitions Collected: 107163     Batch Size: 256   | NewEpisode Model(mean:18394     ) Reward(mean:1.34 , max:2.30 , min:0.66 , std:0.48 ) | total_loss: 2.801   reward_loss: 0.080   policy_loss: 4.746   value_loss: 0.738   consistency_loss: -1.104  lr: 0.000100  batch_future_return: 0.753   batch_model_diff: 7208.984target_model_diff: 100.000 Tp_perstep: 0.172   Tu_perstep: 0.049   
[2025-06-02 21:12:54,108][train][INFO][log.py>_log] ==> #18600      Episodes Collected: 3957       Transitions Collected: 107865     Batch Size: 256   | NewEpisode Model(mean:18489     ) Reward(mean:1.13 , max:1.97 , min:0.33 , std:0.40 ) | total_loss: 3.051   reward_loss: 0.091   policy_loss: 5.087   value_loss: 0.768   consistency_loss: -1.160  lr: 0.000100  batch_future_return: 0.744   batch_model_diff: 7635.156target_model_diff: 200.000 Tp_perstep: 0.172   Tu_perstep: 0.049   
[2025-06-02 21:12:54,110][train][WARNING][train.py>_train] ==> #18600 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:13:17,577][train][INFO][log.py>_log] ==> #18700      Episodes Collected: 3978       Transitions Collected: 108489     Batch Size: 256   | NewEpisode Model(mean:18595     ) Reward(mean:1.27 , max:2.30 , min:0.33 , std:0.48 ) | total_loss: 0.973   reward_loss: 0.026   policy_loss: 1.806   value_loss: 0.293   consistency_loss: -0.466  lr: 0.000100  batch_future_return: 0.611   batch_model_diff: 7456.250target_model_diff: 100.000 Tp_perstep: 0.172   Tu_perstep: 0.049   
[2025-06-02 21:13:17,581][train][WARNING][train.py>_train] ==> #18700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:13:41,463][train][INFO][log.py>_log] ==> #18800      Episodes Collected: 4005       Transitions Collected: 109173     Batch Size: 256   | NewEpisode Model(mean:18693     ) Reward(mean:1.39 , max:2.63 , min:0.33 , std:0.57 ) | total_loss: 1.125   reward_loss: 0.031   policy_loss: 2.051   value_loss: 0.327   consistency_loss: -0.519  lr: 0.000100  batch_future_return: 0.646   batch_model_diff: 7035.547target_model_diff: 200.000 Tp_perstep: 0.172   Tu_perstep: 0.049   
[2025-06-02 21:13:41,465][train][WARNING][train.py>_train] ==> #18800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:14:05,629][train][INFO][log.py>_log] ==> #18900      Episodes Collected: 4030       Transitions Collected: 109876     Batch Size: 256   | NewEpisode Model(mean:18793     ) Reward(mean:1.26 , max:2.30 , min:0.33 , std:0.57 ) | total_loss: 2.147   reward_loss: 0.056   policy_loss: 3.782   value_loss: 0.579   consistency_loss: -0.918  lr: 0.000100  batch_future_return: 0.685   batch_model_diff: 7961.328target_model_diff: 100.000 Tp_perstep: 0.172   Tu_perstep: 0.049   
[2025-06-02 21:14:05,632][train][WARNING][train.py>_train] ==> #18900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:14:29,914][train][INFO][log.py>_log] ==> #19000      Episodes Collected: 4055       Transitions Collected: 110555     Batch Size: 256   | NewEpisode Model(mean:18895     ) Reward(mean:1.25 , max:2.30 , min:0.66 , std:0.47 ) | total_loss: 1.758   reward_loss: 0.043   policy_loss: 2.938   value_loss: 0.442   consistency_loss: -0.667  lr: 0.000100  batch_future_return: 0.688   batch_model_diff: 7677.344target_model_diff: 200.000 Tp_perstep: 0.173   Tu_perstep: 0.049   
[2025-06-02 21:14:54,052][train][INFO][log.py>_log] ==> #19100      Episodes Collected: 4080       Transitions Collected: 111203     Batch Size: 256   | NewEpisode Model(mean:18993     ) Reward(mean:1.21 , max:2.63 , min:0.00 , std:0.56 ) | total_loss: 3.441   reward_loss: 0.078   policy_loss: 5.608   value_loss: 0.816   consistency_loss: -1.224  lr: 0.000100  batch_future_return: 0.727   batch_model_diff: 7428.906target_model_diff: 100.000 Tp_perstep: 0.173   Tu_perstep: 0.049   
[2025-06-02 21:15:17,753][train][INFO][log.py>_log] ==> #19200      Episodes Collected: 4103       Transitions Collected: 111883     Batch Size: 256   | NewEpisode Model(mean:19089     ) Reward(mean:1.29 , max:2.96 , min:0.33 , std:0.58 ) | total_loss: 1.795   reward_loss: 0.045   policy_loss: 2.951   value_loss: 0.428   consistency_loss: -0.654  lr: 0.000100  batch_future_return: 0.720   batch_model_diff: 7112.500target_model_diff: 200.000 Tp_perstep: 0.173   Tu_perstep: 0.049   
[2025-06-02 21:15:17,755][train][WARNING][train.py>_train] ==> #19200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:15:42,459][train][INFO][log.py>_log] ==> #19300      Episodes Collected: 4126       Transitions Collected: 112574     Batch Size: 256   | NewEpisode Model(mean:19187     ) Reward(mean:1.20 , max:2.96 , min:0.66 , std:0.54 ) | total_loss: 3.182   reward_loss: 0.089   policy_loss: 5.367   value_loss: 0.797   consistency_loss: -1.236  lr: 0.000100  batch_future_return: 0.711   batch_model_diff: 7588.281target_model_diff: 100.000 Tp_perstep: 0.173   Tu_perstep: 0.049   
[2025-06-02 21:16:06,688][train][INFO][log.py>_log] ==> #19400      Episodes Collected: 4155       Transitions Collected: 113315     Batch Size: 256   | NewEpisode Model(mean:19290     ) Reward(mean:1.60 , max:2.96 , min:0.00 , std:0.64 ) | total_loss: 2.918   reward_loss: 0.072   policy_loss: 4.876   value_loss: 0.757   consistency_loss: -1.110  lr: 0.000100  batch_future_return: 0.768   batch_model_diff: 7523.828target_model_diff: 200.000 Tp_perstep: 0.173   Tu_perstep: 0.049   
[2025-06-02 21:16:30,880][train][INFO][log.py>_log] ==> #19500      Episodes Collected: 4178       Transitions Collected: 113980     Batch Size: 256   | NewEpisode Model(mean:19392     ) Reward(mean:1.24 , max:2.63 , min:0.33 , std:0.48 ) | total_loss: 2.806   reward_loss: 0.076   policy_loss: 4.768   value_loss: 0.732   consistency_loss: -1.110  lr: 0.000100  batch_future_return: 0.719   batch_model_diff: 7654.297target_model_diff: 100.000 Tp_perstep: 0.173   Tu_perstep: 0.049   
[2025-06-02 21:16:30,882][train][WARNING][train.py>_train] ==> #19500 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:16:54,581][train][INFO][log.py>_log] ==> #19600      Episodes Collected: 4203       Transitions Collected: 114658     Batch Size: 256   | NewEpisode Model(mean:19489     ) Reward(mean:1.30 , max:1.97 , min:0.66 , std:0.38 ) | total_loss: 3.761   reward_loss: 0.104   policy_loss: 6.286   value_loss: 0.967   consistency_loss: -1.435  lr: 0.000100  batch_future_return: 0.693   batch_model_diff: 7256.250target_model_diff: 200.000 Tp_perstep: 0.173   Tu_perstep: 0.049   
[2025-06-02 21:16:54,584][train][WARNING][train.py>_train] ==> #19600 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:17:18,350][train][INFO][log.py>_log] ==> #19700      Episodes Collected: 4227       Transitions Collected: 115310     Batch Size: 256   | NewEpisode Model(mean:19592     ) Reward(mean:1.49 , max:2.96 , min:0.33 , std:0.70 ) | total_loss: 3.541   reward_loss: 0.094   policy_loss: 5.885   value_loss: 0.852   consistency_loss: -1.325  lr: 0.000100  batch_future_return: 0.705   batch_model_diff: 7826.953target_model_diff: 100.000 Tp_perstep: 0.173   Tu_perstep: 0.049   
[2025-06-02 21:17:41,621][train][INFO][log.py>_log] ==> #19800      Episodes Collected: 4250       Transitions Collected: 115942     Batch Size: 256   | NewEpisode Model(mean:19692     ) Reward(mean:1.29 , max:2.63 , min:0.33 , std:0.54 ) | total_loss: 2.985   reward_loss: 0.065   policy_loss: 4.679   value_loss: 0.680   consistency_loss: -0.965  lr: 0.000100  batch_future_return: 0.767   batch_model_diff: 7960.938target_model_diff: 200.000 Tp_perstep: 0.173   Tu_perstep: 0.049   
[2025-06-02 21:17:41,623][train][WARNING][train.py>_train] ==> #19800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:17:53,394][train][WARNING][train.py>_train] ==> #19850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:18:05,448][train][INFO][log.py>_log] ==> #19900      Episodes Collected: 4275       Transitions Collected: 116588     Batch Size: 256   | NewEpisode Model(mean:19790     ) Reward(mean:1.28 , max:2.30 , min:0.33 , std:0.46 ) | total_loss: 3.391   reward_loss: 0.081   policy_loss: 5.623   value_loss: 0.834   consistency_loss: -1.261  lr: 0.000100  batch_future_return: 0.750   batch_model_diff: 7716.406target_model_diff: 100.000 Tp_perstep: 0.173   Tu_perstep: 0.049   
[2025-06-02 21:18:17,074][train][WARNING][train.py>_train] ==> #19950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:18:29,162][train][INFO][log.py>_log] ==> #20000      Episodes Collected: 4299       Transitions Collected: 117212     Batch Size: 256   | NewEpisode Model(mean:19889     ) Reward(mean:1.12 , max:1.97 , min:0.33 , std:0.43 ) | total_loss: 1.971   reward_loss: 0.057   policy_loss: 3.309   value_loss: 0.496   consistency_loss: -0.759  lr: 0.000100  batch_future_return: 0.706   batch_model_diff: 8378.125target_model_diff: 200.000 Tp_perstep: 0.173   Tu_perstep: 0.049   
[2025-06-02 21:18:53,100][train][INFO][log.py>_log] ==> #20100      Episodes Collected: 4322       Transitions Collected: 117934     Batch Size: 256   | NewEpisode Model(mean:19987     ) Reward(mean:1.42 , max:2.30 , min:0.33 , std:0.49 ) | total_loss: 1.060   reward_loss: 0.025   policy_loss: 1.793   value_loss: 0.270   consistency_loss: -0.413  lr: 0.000100  batch_future_return: 0.632   batch_model_diff: 8073.828target_model_diff: 100.000 Tp_perstep: 0.174   Tu_perstep: 0.049   
[2025-06-02 21:19:05,204][train][WARNING][train.py>_train] ==> #20150 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:19:17,317][train][INFO][log.py>_log] ==> #20200      Episodes Collected: 4347       Transitions Collected: 118621     Batch Size: 256   | NewEpisode Model(mean:20092     ) Reward(mean:1.41 , max:2.96 , min:0.33 , std:0.55 ) | total_loss: 2.663   reward_loss: 0.065   policy_loss: 4.570   value_loss: 0.682   consistency_loss: -1.071  lr: 0.000100  batch_future_return: 0.685   batch_model_diff: 7964.062target_model_diff: 200.000 Tp_perstep: 0.174   Tu_perstep: 0.049   
[2025-06-02 21:19:41,228][train][INFO][log.py>_log] ==> #20300      Episodes Collected: 4370       Transitions Collected: 119290     Batch Size: 256   | NewEpisode Model(mean:20194     ) Reward(mean:1.47 , max:2.96 , min:0.33 , std:0.63 ) | total_loss: 1.684   reward_loss: 0.040   policy_loss: 2.779   value_loss: 0.401   consistency_loss: -0.617  lr: 0.000100  batch_future_return: 0.683   batch_model_diff: 8327.344target_model_diff: 100.000 Tp_perstep: 0.174   Tu_perstep: 0.049   
[2025-06-02 21:20:04,313][train][INFO][log.py>_log] ==> #20400      Episodes Collected: 4392       Transitions Collected: 119903     Batch Size: 256   | NewEpisode Model(mean:20290     ) Reward(mean:1.20 , max:2.30 , min:0.66 , std:0.43 ) | total_loss: 2.040   reward_loss: 0.042   policy_loss: 3.391   value_loss: 0.505   consistency_loss: -0.760  lr: 0.000100  batch_future_return: 0.728   batch_model_diff: 8348.438target_model_diff: 200.000 Tp_perstep: 0.174   Tu_perstep: 0.049   
[2025-06-02 21:20:16,596][train][WARNING][train.py>_train] ==> #20450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:20:28,666][train][INFO][log.py>_log] ==> #20500      Episodes Collected: 4418       Transitions Collected: 120621     Batch Size: 256   | NewEpisode Model(mean:20390     ) Reward(mean:1.26 , max:2.63 , min:0.33 , std:0.53 ) | total_loss: 2.674   reward_loss: 0.087   policy_loss: 4.711   value_loss: 0.715   consistency_loss: -1.151  lr: 0.000100  batch_future_return: 0.650   batch_model_diff: 8573.438target_model_diff: 100.000 Tp_perstep: 0.174   Tu_perstep: 0.049   
[2025-06-02 21:20:52,574][train][INFO][log.py>_log] ==> #20600      Episodes Collected: 4443       Transitions Collected: 121260     Batch Size: 256   | NewEpisode Model(mean:20491     ) Reward(mean:1.25 , max:1.97 , min:0.33 , std:0.50 ) | total_loss: 2.295   reward_loss: 0.072   policy_loss: 3.952   value_loss: 0.609   consistency_loss: -0.941  lr: 0.000100  batch_future_return: 0.726   batch_model_diff: 7869.531target_model_diff: 200.000 Tp_perstep: 0.174   Tu_perstep: 0.049   
[2025-06-02 21:21:16,402][train][INFO][log.py>_log] ==> #20700      Episodes Collected: 4469       Transitions Collected: 121952     Batch Size: 256   | NewEpisode Model(mean:20593     ) Reward(mean:1.47 , max:3.29 , min:0.33 , std:0.71 ) | total_loss: 3.262   reward_loss: 0.089   policy_loss: 5.381   value_loss: 0.821   consistency_loss: -1.206  lr: 0.000100  batch_future_return: 0.726   batch_model_diff: 8345.703target_model_diff: 100.000 Tp_perstep: 0.174   Tu_perstep: 0.049   
[2025-06-02 21:21:28,044][train][WARNING][train.py>_train] ==> #20750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:21:40,288][train][INFO][log.py>_log] ==> #20800      Episodes Collected: 4495       Transitions Collected: 122642     Batch Size: 256   | NewEpisode Model(mean:20693     ) Reward(mean:1.28 , max:1.97 , min:0.66 , std:0.36 ) | total_loss: 1.824   reward_loss: 0.045   policy_loss: 2.949   value_loss: 0.428   consistency_loss: -0.639  lr: 0.000100  batch_future_return: 0.764   batch_model_diff: 7795.703target_model_diff: 200.000 Tp_perstep: 0.174   Tu_perstep: 0.049   
[2025-06-02 21:21:51,869][train][WARNING][train.py>_train] ==> #20850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:22:04,051][train][INFO][log.py>_log] ==> #20900      Episodes Collected: 4518       Transitions Collected: 123252     Batch Size: 256   | NewEpisode Model(mean:20794     ) Reward(mean:1.33 , max:2.96 , min:0.66 , std:0.59 ) | total_loss: 3.163   reward_loss: 0.087   policy_loss: 5.398   value_loss: 0.822   consistency_loss: -1.264  lr: 0.000100  batch_future_return: 0.700   batch_model_diff: 8240.234target_model_diff: 100.000 Tp_perstep: 0.174   Tu_perstep: 0.049   
[2025-06-02 21:22:28,309][train][INFO][log.py>_log] ==> #21000      Episodes Collected: 4544       Transitions Collected: 123960     Batch Size: 256   | NewEpisode Model(mean:20892     ) Reward(mean:1.39 , max:2.63 , min:0.33 , std:0.61 ) | total_loss: 2.063   reward_loss: 0.053   policy_loss: 3.487   value_loss: 0.513   consistency_loss: -0.803  lr: 0.000100  batch_future_return: 0.729   batch_model_diff: 7466.406target_model_diff: 200.000 Tp_perstep: 0.174   Tu_perstep: 0.049   
[2025-06-02 21:22:40,025][train][WARNING][train.py>_train] ==> #21050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:22:52,287][train][INFO][log.py>_log] ==> #21100      Episodes Collected: 4568       Transitions Collected: 124625     Batch Size: 256   | NewEpisode Model(mean:20992     ) Reward(mean:1.32 , max:2.63 , min:0.33 , std:0.45 ) | total_loss: 2.062   reward_loss: 0.052   policy_loss: 3.450   value_loss: 0.515   consistency_loss: -0.784  lr: 0.000100  batch_future_return: 0.681   batch_model_diff: 8275.391target_model_diff: 100.000 Tp_perstep: 0.174   Tu_perstep: 0.049   
[2025-06-02 21:23:16,037][train][INFO][log.py>_log] ==> #21200      Episodes Collected: 4592       Transitions Collected: 125306     Batch Size: 256   | NewEpisode Model(mean:21090     ) Reward(mean:1.33 , max:1.97 , min:0.33 , std:0.43 ) | total_loss: 1.908   reward_loss: 0.048   policy_loss: 3.232   value_loss: 0.484   consistency_loss: -0.746  lr: 0.000100  batch_future_return: 0.692   batch_model_diff: 8315.625target_model_diff: 200.000 Tp_perstep: 0.174   Tu_perstep: 0.049   
[2025-06-02 21:23:16,038][train][WARNING][train.py>_train] ==> #21200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:23:39,861][train][INFO][log.py>_log] ==> #21300      Episodes Collected: 4615       Transitions Collected: 125970     Batch Size: 256   | NewEpisode Model(mean:21193     ) Reward(mean:1.20 , max:2.63 , min:0.33 , std:0.61 ) | total_loss: 2.056   reward_loss: 0.052   policy_loss: 3.486   value_loss: 0.530   consistency_loss: -0.808  lr: 0.000100  batch_future_return: 0.704   batch_model_diff: 8278.906target_model_diff: 100.000 Tp_perstep: 0.174   Tu_perstep: 0.049   
[2025-06-02 21:23:39,863][train][WARNING][train.py>_train] ==> #21300 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:24:04,589][train][INFO][log.py>_log] ==> #21400      Episodes Collected: 4643       Transitions Collected: 126662     Batch Size: 256   | NewEpisode Model(mean:21293     ) Reward(mean:1.55 , max:2.63 , min:0.33 , std:0.53 ) | total_loss: 2.351   reward_loss: 0.062   policy_loss: 3.808   value_loss: 0.586   consistency_loss: -0.833  lr: 0.000100  batch_future_return: 0.767   batch_model_diff: 9068.359target_model_diff: 200.000 Tp_perstep: 0.175   Tu_perstep: 0.049   
[2025-06-02 21:24:27,976][train][INFO][log.py>_log] ==> #21500      Episodes Collected: 4667       Transitions Collected: 127302     Batch Size: 256   | NewEpisode Model(mean:21391     ) Reward(mean:1.37 , max:2.63 , min:0.33 , std:0.53 ) | total_loss: 3.292   reward_loss: 0.081   policy_loss: 5.277   value_loss: 0.774   consistency_loss: -1.129  lr: 0.000100  batch_future_return: 0.715   batch_model_diff: 8206.250target_model_diff: 100.000 Tp_perstep: 0.175   Tu_perstep: 0.049   
[2025-06-02 21:24:52,244][train][INFO][log.py>_log] ==> #21600      Episodes Collected: 4693       Transitions Collected: 127995     Batch Size: 256   | NewEpisode Model(mean:21490     ) Reward(mean:1.37 , max:1.97 , min:0.66 , std:0.41 ) | total_loss: 2.457   reward_loss: 0.069   policy_loss: 4.179   value_loss: 0.636   consistency_loss: -0.975  lr: 0.000100  batch_future_return: 0.693   batch_model_diff: 8789.844target_model_diff: 200.000 Tp_perstep: 0.175   Tu_perstep: 0.049   
[2025-06-02 21:24:52,247][train][WARNING][train.py>_train] ==> #21600 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:25:16,146][train][INFO][log.py>_log] ==> #21700      Episodes Collected: 4721       Transitions Collected: 128716     Batch Size: 256   | NewEpisode Model(mean:21589     ) Reward(mean:1.42 , max:2.63 , min:0.33 , std:0.56 ) | total_loss: 3.221   reward_loss: 0.085   policy_loss: 5.273   value_loss: 0.788   consistency_loss: -1.167  lr: 0.000100  batch_future_return: 0.743   batch_model_diff: 8838.281target_model_diff: 100.000 Tp_perstep: 0.175   Tu_perstep: 0.049   
[2025-06-02 21:25:16,149][train][WARNING][train.py>_train] ==> #21700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:25:27,860][train][WARNING][train.py>_train] ==> #21750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:25:40,072][train][INFO][log.py>_log] ==> #21800      Episodes Collected: 4742       Transitions Collected: 129338     Batch Size: 256   | NewEpisode Model(mean:21696     ) Reward(mean:1.27 , max:2.30 , min:0.00 , std:0.68 ) | total_loss: 0.597   reward_loss: 0.015   policy_loss: 1.007   value_loss: 0.152   consistency_loss: -0.231  lr: 0.000100  batch_future_return: 0.672   batch_model_diff: 8953.516target_model_diff: 200.000 Tp_perstep: 0.175   Tu_perstep: 0.049   
[2025-06-02 21:25:51,616][train][WARNING][train.py>_train] ==> #21850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:26:03,741][train][INFO][log.py>_log] ==> #21900      Episodes Collected: 4768       Transitions Collected: 130024     Batch Size: 256   | NewEpisode Model(mean:21792     ) Reward(mean:1.38 , max:2.30 , min:0.33 , std:0.51 ) | total_loss: 2.398   reward_loss: 0.061   policy_loss: 3.970   value_loss: 0.599   consistency_loss: -0.892  lr: 0.000100  batch_future_return: 0.720   batch_model_diff: 8911.719target_model_diff: 100.000 Tp_perstep: 0.175   Tu_perstep: 0.049   
[2025-06-02 21:26:03,743][train][WARNING][train.py>_train] ==> #21900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:26:15,329][train][WARNING][train.py>_train] ==> #21950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:26:27,466][train][INFO][log.py>_log] ==> #22000      Episodes Collected: 4790       Transitions Collected: 130659     Batch Size: 256   | NewEpisode Model(mean:21892     ) Reward(mean:1.57 , max:2.63 , min:0.33 , std:0.63 ) | total_loss: 1.431   reward_loss: 0.043   policy_loss: 2.367   value_loss: 0.358   consistency_loss: -0.534  lr: 0.000100  batch_future_return: 0.726   batch_model_diff: 8259.375target_model_diff: 200.000 Tp_perstep: 0.175   Tu_perstep: 0.049   
[2025-06-02 21:26:51,266][train][INFO][log.py>_log] ==> #22100      Episodes Collected: 4814       Transitions Collected: 131318     Batch Size: 256   | NewEpisode Model(mean:21991     ) Reward(mean:1.47 , max:2.63 , min:0.66 , std:0.57 ) | total_loss: 1.300   reward_loss: 0.037   policy_loss: 2.217   value_loss: 0.342   consistency_loss: -0.520  lr: 0.000100  batch_future_return: 0.682   batch_model_diff: 9336.328target_model_diff: 100.000 Tp_perstep: 0.175   Tu_perstep: 0.049   
[2025-06-02 21:26:51,268][train][WARNING][train.py>_train] ==> #22100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:27:15,542][train][INFO][log.py>_log] ==> #22200      Episodes Collected: 4838       Transitions Collected: 131979     Batch Size: 256   | NewEpisode Model(mean:22091     ) Reward(mean:1.42 , max:2.96 , min:0.00 , std:0.66 ) | total_loss: 2.260   reward_loss: 0.051   policy_loss: 3.830   value_loss: 0.562   consistency_loss: -0.881  lr: 0.000100  batch_future_return: 0.686   batch_model_diff: 8436.719target_model_diff: 200.000 Tp_perstep: 0.175   Tu_perstep: 0.049   
[2025-06-02 21:27:15,545][train][WARNING][train.py>_train] ==> #22200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:27:39,766][train][INFO][log.py>_log] ==> #22300      Episodes Collected: 4865       Transitions Collected: 132709     Batch Size: 256   | NewEpisode Model(mean:22190     ) Reward(mean:1.29 , max:2.96 , min:0.33 , std:0.59 ) | total_loss: 2.878   reward_loss: 0.080   policy_loss: 4.931   value_loss: 0.765   consistency_loss: -1.162  lr: 0.000100  batch_future_return: 0.692   batch_model_diff: 8558.203target_model_diff: 100.000 Tp_perstep: 0.175   Tu_perstep: 0.049   
[2025-06-02 21:28:03,753][train][INFO][log.py>_log] ==> #22400      Episodes Collected: 4888       Transitions Collected: 133397     Batch Size: 256   | NewEpisode Model(mean:22292     ) Reward(mean:1.33 , max:2.63 , min:0.66 , std:0.50 ) | total_loss: 1.892   reward_loss: 0.051   policy_loss: 3.462   value_loss: 0.550   consistency_loss: -0.880  lr: 0.000100  batch_future_return: 0.624   batch_model_diff: 9378.516target_model_diff: 200.000 Tp_perstep: 0.175   Tu_perstep: 0.049   
[2025-06-02 21:28:15,382][train][WARNING][train.py>_train] ==> #22450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:28:28,387][train][INFO][log.py>_log] ==> #22500      Episodes Collected: 4915       Transitions Collected: 134102     Batch Size: 256   | NewEpisode Model(mean:22395     ) Reward(mean:1.24 , max:2.96 , min:0.33 , std:0.59 ) | total_loss: 1.975   reward_loss: 0.056   policy_loss: 3.430   value_loss: 0.522   consistency_loss: -0.821  lr: 0.000100  batch_future_return: 0.686   batch_model_diff: 8500.391target_model_diff: 100.000 Tp_perstep: 0.175   Tu_perstep: 0.049   
[2025-06-02 21:28:40,451][train][WARNING][train.py>_train] ==> #22550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:28:53,018][train][INFO][log.py>_log] ==> #22600      Episodes Collected: 4941       Transitions Collected: 134842     Batch Size: 256   | NewEpisode Model(mean:22494     ) Reward(mean:1.30 , max:2.30 , min:0.00 , std:0.56 ) | total_loss: 1.900   reward_loss: 0.053   policy_loss: 3.234   value_loss: 0.493   consistency_loss: -0.755  lr: 0.000100  batch_future_return: 0.681   batch_model_diff: 9167.969target_model_diff: 200.000 Tp_perstep: 0.175   Tu_perstep: 0.049   
[2025-06-02 21:28:53,020][train][WARNING][train.py>_train] ==> #22600 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:29:17,298][train][INFO][log.py>_log] ==> #22700      Episodes Collected: 4967       Transitions Collected: 135531     Batch Size: 256   | NewEpisode Model(mean:22595     ) Reward(mean:1.44 , max:2.63 , min:0.33 , std:0.60 ) | total_loss: 2.980   reward_loss: 0.095   policy_loss: 5.206   value_loss: 0.838   consistency_loss: -1.265  lr: 0.000100  batch_future_return: 0.727   batch_model_diff: 9557.422target_model_diff: 100.000 Tp_perstep: 0.176   Tu_perstep: 0.049   
[2025-06-02 21:29:17,300][train][WARNING][train.py>_train] ==> #22700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:29:41,633][train][INFO][log.py>_log] ==> #22800      Episodes Collected: 4993       Transitions Collected: 136172     Batch Size: 256   | NewEpisode Model(mean:22695     ) Reward(mean:1.32 , max:2.96 , min:0.33 , std:0.54 ) | total_loss: 2.243   reward_loss: 0.054   policy_loss: 3.659   value_loss: 0.544   consistency_loss: -0.803  lr: 0.000100  batch_future_return: 0.762   batch_model_diff: 9230.469target_model_diff: 200.000 Tp_perstep: 0.176   Tu_perstep: 0.049   
[2025-06-02 21:30:05,490][train][INFO][log.py>_log] ==> #22900      Episodes Collected: 5015       Transitions Collected: 136796     Batch Size: 256   | NewEpisode Model(mean:22794     ) Reward(mean:1.51 , max:2.63 , min:0.33 , std:0.58 ) | total_loss: 2.552   reward_loss: 0.073   policy_loss: 4.241   value_loss: 0.639   consistency_loss: -0.961  lr: 0.000100  batch_future_return: 0.737   batch_model_diff: 8471.484target_model_diff: 100.000 Tp_perstep: 0.176   Tu_perstep: 0.049   
[2025-06-02 21:30:29,357][train][INFO][log.py>_log] ==> #23000      Episodes Collected: 5042       Transitions Collected: 137538     Batch Size: 256   | NewEpisode Model(mean:22890     ) Reward(mean:1.35 , max:2.63 , min:0.33 , std:0.55 ) | total_loss: 3.661   reward_loss: 0.098   policy_loss: 6.166   value_loss: 0.955   consistency_loss: -1.421  lr: 0.000100  batch_future_return: 0.772   batch_model_diff: 9844.922target_model_diff: 200.000 Tp_perstep: 0.176   Tu_perstep: 0.049   
[2025-06-02 21:30:53,051][train][INFO][log.py>_log] ==> #23100      Episodes Collected: 5067       Transitions Collected: 138190     Batch Size: 256   | NewEpisode Model(mean:22992     ) Reward(mean:1.29 , max:2.63 , min:0.33 , std:0.55 ) | total_loss: 2.551   reward_loss: 0.060   policy_loss: 4.126   value_loss: 0.616   consistency_loss: -0.895  lr: 0.000100  batch_future_return: 0.733   batch_model_diff: 9017.578target_model_diff: 100.000 Tp_perstep: 0.176   Tu_perstep: 0.049   
[2025-06-02 21:30:53,053][train][WARNING][train.py>_train] ==> #23100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:31:17,454][train][INFO][log.py>_log] ==> #23200      Episodes Collected: 5097       Transitions Collected: 138915     Batch Size: 256   | NewEpisode Model(mean:23092     ) Reward(mean:1.48 , max:2.30 , min:0.66 , std:0.44 ) | total_loss: 2.300   reward_loss: 0.077   policy_loss: 4.095   value_loss: 0.641   consistency_loss: -1.016  lr: 0.000100  batch_future_return: 0.701   batch_model_diff: 9072.266target_model_diff: 200.000 Tp_perstep: 0.176   Tu_perstep: 0.049   
[2025-06-02 21:31:42,072][train][INFO][log.py>_log] ==> #23300      Episodes Collected: 5124       Transitions Collected: 139593     Batch Size: 256   | NewEpisode Model(mean:23197     ) Reward(mean:1.29 , max:2.30 , min:0.33 , std:0.43 ) | total_loss: 1.941   reward_loss: 0.049   policy_loss: 3.358   value_loss: 0.515   consistency_loss: -0.798  lr: 0.000100  batch_future_return: 0.669   batch_model_diff: 8966.016target_model_diff: 100.000 Tp_perstep: 0.176   Tu_perstep: 0.049   
[2025-06-02 21:32:05,784][train][INFO][log.py>_log] ==> #23400      Episodes Collected: 5146       Transitions Collected: 140230     Batch Size: 256   | NewEpisode Model(mean:23292     ) Reward(mean:1.18 , max:2.30 , min:0.00 , std:0.51 ) | total_loss: 1.773   reward_loss: 0.051   policy_loss: 3.181   value_loss: 0.505   consistency_loss: -0.793  lr: 0.000100  batch_future_return: 0.686   batch_model_diff: 9233.203target_model_diff: 200.000 Tp_perstep: 0.176   Tu_perstep: 0.049   
[2025-06-02 21:32:30,240][train][INFO][log.py>_log] ==> #23500      Episodes Collected: 5174       Transitions Collected: 140956     Batch Size: 256   | NewEpisode Model(mean:23391     ) Reward(mean:1.37 , max:2.63 , min:0.33 , std:0.51 ) | total_loss: 2.964   reward_loss: 0.079   policy_loss: 5.118   value_loss: 0.815   consistency_loss: -1.218  lr: 0.000100  batch_future_return: 0.733   batch_model_diff: 8637.500target_model_diff: 100.000 Tp_perstep: 0.176   Tu_perstep: 0.049   
[2025-06-02 21:32:41,804][train][WARNING][train.py>_train] ==> #23550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:32:54,030][train][INFO][log.py>_log] ==> #23600      Episodes Collected: 5198       Transitions Collected: 141617     Batch Size: 256   | NewEpisode Model(mean:23491     ) Reward(mean:1.42 , max:2.96 , min:0.66 , std:0.57 ) | total_loss: 2.804   reward_loss: 0.071   policy_loss: 4.800   value_loss: 0.740   consistency_loss: -1.127  lr: 0.000100  batch_future_return: 0.720   batch_model_diff: 8724.609target_model_diff: 200.000 Tp_perstep: 0.176   Tu_perstep: 0.049   
[2025-06-02 21:33:17,780][train][INFO][log.py>_log] ==> #23700      Episodes Collected: 5224       Transitions Collected: 142266     Batch Size: 256   | NewEpisode Model(mean:23592     ) Reward(mean:1.42 , max:3.29 , min:0.66 , std:0.64 ) | total_loss: 1.987   reward_loss: 0.052   policy_loss: 3.279   value_loss: 0.502   consistency_loss: -0.735  lr: 0.000100  batch_future_return: 0.783   batch_model_diff: 9528.516target_model_diff: 100.000 Tp_perstep: 0.176   Tu_perstep: 0.049   
[2025-06-02 21:33:29,430][train][WARNING][train.py>_train] ==> #23750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:33:41,738][train][INFO][log.py>_log] ==> #23800      Episodes Collected: 5248       Transitions Collected: 142923     Batch Size: 256   | NewEpisode Model(mean:23692     ) Reward(mean:1.34 , max:4.33 , min:0.00 , std:0.84 ) | total_loss: 2.937   reward_loss: 0.074   policy_loss: 5.021   value_loss: 0.761   consistency_loss: -1.175  lr: 0.000100  batch_future_return: 0.688   batch_model_diff: 9361.328target_model_diff: 200.000 Tp_perstep: 0.176   Tu_perstep: 0.049   
[2025-06-02 21:34:06,222][train][INFO][log.py>_log] ==> #23900      Episodes Collected: 5274       Transitions Collected: 143646     Batch Size: 256   | NewEpisode Model(mean:23791     ) Reward(mean:1.52 , max:3.62 , min:0.33 , std:0.72 ) | total_loss: 2.111   reward_loss: 0.054   policy_loss: 3.404   value_loss: 0.493   consistency_loss: -0.735  lr: 0.000100  batch_future_return: 0.749   batch_model_diff: 9153.906target_model_diff: 100.000 Tp_perstep: 0.176   Tu_perstep: 0.049   
[2025-06-02 21:34:31,378][train][INFO][log.py>_log] ==> #24000      Episodes Collected: 5302       Transitions Collected: 144386     Batch Size: 256   | NewEpisode Model(mean:23892     ) Reward(mean:1.51 , max:4.60 , min:0.33 , std:0.84 ) | total_loss: 2.356   reward_loss: 0.062   policy_loss: 3.987   value_loss: 0.599   consistency_loss: -0.921  lr: 0.000100  batch_future_return: 0.702   batch_model_diff: 9313.672target_model_diff: 200.000 Tp_perstep: 0.176   Tu_perstep: 0.049   
[2025-06-02 21:34:55,209][train][INFO][log.py>_log] ==> #24100      Episodes Collected: 5328       Transitions Collected: 145056     Batch Size: 256   | NewEpisode Model(mean:23993     ) Reward(mean:1.43 , max:2.30 , min:0.66 , std:0.47 ) | total_loss: 1.717   reward_loss: 0.044   policy_loss: 2.764   value_loss: 0.386   consistency_loss: -0.593  lr: 0.000100  batch_future_return: 0.749   batch_model_diff: 8726.172target_model_diff: 100.000 Tp_perstep: 0.176   Tu_perstep: 0.049   
[2025-06-02 21:35:19,025][train][INFO][log.py>_log] ==> #24200      Episodes Collected: 5352       Transitions Collected: 145670     Batch Size: 256   | NewEpisode Model(mean:24093     ) Reward(mean:1.48 , max:2.63 , min:0.66 , std:0.45 ) | total_loss: 2.630   reward_loss: 0.072   policy_loss: 4.520   value_loss: 0.701   consistency_loss: -1.069  lr: 0.000100  batch_future_return: 0.777   batch_model_diff: 10148.828target_model_diff: 200.000 Tp_perstep: 0.177   Tu_perstep: 0.049   
[2025-06-02 21:35:19,028][train][WARNING][train.py>_train] ==> #24200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:35:43,151][train][INFO][log.py>_log] ==> #24300      Episodes Collected: 5375       Transitions Collected: 146350     Batch Size: 256   | NewEpisode Model(mean:24188     ) Reward(mean:1.27 , max:2.30 , min:0.33 , std:0.46 ) | total_loss: 2.562   reward_loss: 0.076   policy_loss: 4.547   value_loss: 0.709   consistency_loss: -1.119  lr: 0.000100  batch_future_return: 0.656   batch_model_diff: 8848.047target_model_diff: 100.000 Tp_perstep: 0.177   Tu_perstep: 0.049   
[2025-06-02 21:36:07,403][train][INFO][log.py>_log] ==> #24400      Episodes Collected: 5401       Transitions Collected: 147089     Batch Size: 256   | NewEpisode Model(mean:24289     ) Reward(mean:1.26 , max:2.63 , min:0.33 , std:0.49 ) | total_loss: 1.646   reward_loss: 0.046   policy_loss: 2.849   value_loss: 0.449   consistency_loss: -0.681  lr: 0.000100  batch_future_return: 0.701   batch_model_diff: 9677.344target_model_diff: 200.000 Tp_perstep: 0.177   Tu_perstep: 0.049   
[2025-06-02 21:36:19,456][train][WARNING][train.py>_train] ==> #24450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:36:32,111][train][INFO][log.py>_log] ==> #24500      Episodes Collected: 5427       Transitions Collected: 147788     Batch Size: 256   | NewEpisode Model(mean:24393     ) Reward(mean:1.34 , max:2.63 , min:0.33 , std:0.52 ) | total_loss: 2.430   reward_loss: 0.061   policy_loss: 3.923   value_loss: 0.593   consistency_loss: -0.851  lr: 0.000100  batch_future_return: 0.732   batch_model_diff: 9639.453target_model_diff: 100.000 Tp_perstep: 0.177   Tu_perstep: 0.049   
[2025-06-02 21:36:43,822][train][WARNING][train.py>_train] ==> #24550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:36:56,031][train][INFO][log.py>_log] ==> #24600      Episodes Collected: 5453       Transitions Collected: 148498     Batch Size: 256   | NewEpisode Model(mean:24489     ) Reward(mean:1.43 , max:3.62 , min:0.33 , std:0.79 ) | total_loss: 2.318   reward_loss: 0.061   policy_loss: 3.860   value_loss: 0.591   consistency_loss: -0.875  lr: 0.000100  batch_future_return: 0.735   batch_model_diff: 9766.016target_model_diff: 200.000 Tp_perstep: 0.177   Tu_perstep: 0.049   
[2025-06-02 21:37:20,412][train][INFO][log.py>_log] ==> #24700      Episodes Collected: 5476       Transitions Collected: 149154     Batch Size: 256   | NewEpisode Model(mean:24596     ) Reward(mean:1.33 , max:2.30 , min:0.00 , std:0.47 ) | total_loss: 1.419   reward_loss: 0.034   policy_loss: 2.244   value_loss: 0.326   consistency_loss: -0.471  lr: 0.000100  batch_future_return: 0.750   batch_model_diff: 10210.547target_model_diff: 100.000 Tp_perstep: 0.177   Tu_perstep: 0.049   
[2025-06-02 21:37:44,685][train][INFO][log.py>_log] ==> #24800      Episodes Collected: 5503       Transitions Collected: 149860     Batch Size: 256   | NewEpisode Model(mean:24689     ) Reward(mean:1.52 , max:2.63 , min:0.66 , std:0.49 ) | total_loss: 2.120   reward_loss: 0.055   policy_loss: 3.456   value_loss: 0.501   consistency_loss: -0.758  lr: 0.000100  batch_future_return: 0.750   batch_model_diff: 10532.031target_model_diff: 200.000 Tp_perstep: 0.177   Tu_perstep: 0.049   
[2025-06-02 21:37:44,687][train][WARNING][train.py>_train] ==> #24800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:38:08,542][train][INFO][log.py>_log] ==> #24900      Episodes Collected: 5525       Transitions Collected: 150525     Batch Size: 256   | NewEpisode Model(mean:24791     ) Reward(mean:1.20 , max:2.63 , min:0.66 , std:0.40 ) | total_loss: 2.059   reward_loss: 0.053   policy_loss: 3.453   value_loss: 0.534   consistency_loss: -0.791  lr: 0.000100  batch_future_return: 0.738   batch_model_diff: 10134.375target_model_diff: 100.000 Tp_perstep: 0.177   Tu_perstep: 0.049   
[2025-06-02 21:38:20,561][train][WARNING][train.py>_train] ==> #24950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:38:32,664][train][INFO][log.py>_log] ==> #25000      Episodes Collected: 5549       Transitions Collected: 151226     Batch Size: 256   | NewEpisode Model(mean:24892     ) Reward(mean:1.38 , max:2.30 , min:0.66 , std:0.42 ) | total_loss: 1.730   reward_loss: 0.048   policy_loss: 2.998   value_loss: 0.458   consistency_loss: -0.715  lr: 0.000100  batch_future_return: 0.679   batch_model_diff: 10278.125target_model_diff: 200.000 Tp_perstep: 0.177   Tu_perstep: 0.049   
[2025-06-02 21:38:44,942][train][WARNING][train.py>_train] ==> #25050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:38:57,103][train][INFO][log.py>_log] ==> #25100      Episodes Collected: 5575       Transitions Collected: 151908     Batch Size: 256   | NewEpisode Model(mean:24994     ) Reward(mean:1.39 , max:2.30 , min:0.66 , std:0.46 ) | total_loss: 3.030   reward_loss: 0.078   policy_loss: 5.188   value_loss: 0.812   consistency_loss: -1.219  lr: 0.000100  batch_future_return: 0.726   batch_model_diff: 9839.453target_model_diff: 100.000 Tp_perstep: 0.177   Tu_perstep: 0.049   
[2025-06-02 21:39:09,651][train][WARNING][train.py>_train] ==> #25150 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:39:22,213][train][INFO][log.py>_log] ==> #25200      Episodes Collected: 5601       Transitions Collected: 152638     Batch Size: 256   | NewEpisode Model(mean:25092     ) Reward(mean:1.34 , max:2.63 , min:0.33 , std:0.46 ) | total_loss: 2.593   reward_loss: 0.069   policy_loss: 4.380   value_loss: 0.659   consistency_loss: -1.010  lr: 0.000100  batch_future_return: 0.720   batch_model_diff: 9557.031target_model_diff: 200.000 Tp_perstep: 0.177   Tu_perstep: 0.049   
[2025-06-02 21:39:46,493][train][INFO][log.py>_log] ==> #25300      Episodes Collected: 5624       Transitions Collected: 153319     Batch Size: 256   | NewEpisode Model(mean:25190     ) Reward(mean:1.34 , max:2.63 , min:0.66 , std:0.52 ) | total_loss: 1.857   reward_loss: 0.048   policy_loss: 3.160   value_loss: 0.494   consistency_loss: -0.737  lr: 0.000100  batch_future_return: 0.682   batch_model_diff: 9775.000target_model_diff: 100.000 Tp_perstep: 0.177   Tu_perstep: 0.049   
[2025-06-02 21:40:10,627][train][INFO][log.py>_log] ==> #25400      Episodes Collected: 5650       Transitions Collected: 153977     Batch Size: 256   | NewEpisode Model(mean:25291     ) Reward(mean:1.37 , max:2.96 , min:0.33 , std:0.51 ) | total_loss: 1.386   reward_loss: 0.035   policy_loss: 2.309   value_loss: 0.343   consistency_loss: -0.522  lr: 0.000100  batch_future_return: 0.677   batch_model_diff: 10887.500target_model_diff: 200.000 Tp_perstep: 0.177   Tu_perstep: 0.049   
[2025-06-02 21:40:10,629][train][WARNING][train.py>_train] ==> #25400 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:40:35,022][train][INFO][log.py>_log] ==> #25500      Episodes Collected: 5676       Transitions Collected: 154685     Batch Size: 256   | NewEpisode Model(mean:25390     ) Reward(mean:1.14 , max:2.96 , min:0.33 , std:0.55 ) | total_loss: 1.993   reward_loss: 0.053   policy_loss: 3.268   value_loss: 0.489   consistency_loss: -0.726  lr: 0.000100  batch_future_return: 0.717   batch_model_diff: 10057.422target_model_diff: 100.000 Tp_perstep: 0.177   Tu_perstep: 0.049   
[2025-06-02 21:40:58,825][train][INFO][log.py>_log] ==> #25600      Episodes Collected: 5701       Transitions Collected: 155368     Batch Size: 256   | NewEpisode Model(mean:25490     ) Reward(mean:1.42 , max:2.96 , min:0.00 , std:0.62 ) | total_loss: 2.887   reward_loss: 0.066   policy_loss: 4.585   value_loss: 0.686   consistency_loss: -0.968  lr: 0.000100  batch_future_return: 0.798   batch_model_diff: 10347.656target_model_diff: 200.000 Tp_perstep: 0.177   Tu_perstep: 0.049   
[2025-06-02 21:41:23,178][train][INFO][log.py>_log] ==> #25700      Episodes Collected: 5725       Transitions Collected: 156016     Batch Size: 256   | NewEpisode Model(mean:25591     ) Reward(mean:1.33 , max:2.30 , min:0.33 , std:0.55 ) | total_loss: 0.711   reward_loss: 0.018   policy_loss: 1.204   value_loss: 0.195   consistency_loss: -0.280  lr: 0.000100  batch_future_return: 0.751   batch_model_diff: 9916.406target_model_diff: 100.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:41:47,299][train][INFO][log.py>_log] ==> #25800      Episodes Collected: 5751       Transitions Collected: 156730     Batch Size: 256   | NewEpisode Model(mean:25688     ) Reward(mean:1.45 , max:2.63 , min:0.33 , std:0.60 ) | total_loss: 2.763   reward_loss: 0.075   policy_loss: 4.655   value_loss: 0.717   consistency_loss: -1.073  lr: 0.000100  batch_future_return: 0.737   batch_model_diff: 10735.156target_model_diff: 200.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:41:47,302][train][WARNING][train.py>_train] ==> #25800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:42:12,219][train][INFO][log.py>_log] ==> #25900      Episodes Collected: 5778       Transitions Collected: 157436     Batch Size: 256   | NewEpisode Model(mean:25791     ) Reward(mean:1.28 , max:2.30 , min:0.00 , std:0.56 ) | total_loss: 1.732   reward_loss: 0.039   policy_loss: 2.879   value_loss: 0.444   consistency_loss: -0.648  lr: 0.000100  batch_future_return: 0.745   batch_model_diff: 9692.188target_model_diff: 100.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:42:12,223][train][WARNING][train.py>_train] ==> #25900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:42:36,333][train][INFO][log.py>_log] ==> #26000      Episodes Collected: 5805       Transitions Collected: 158153     Batch Size: 256   | NewEpisode Model(mean:25889     ) Reward(mean:1.21 , max:2.63 , min:0.33 , std:0.51 ) | total_loss: 0.260   reward_loss: 0.006   policy_loss: 0.459   value_loss: 0.077   consistency_loss: -0.112  lr: 0.000100  batch_future_return: 0.728   batch_model_diff: 10149.609target_model_diff: 200.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:42:48,111][train][WARNING][train.py>_train] ==> #26050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:43:00,236][train][INFO][log.py>_log] ==> #26100      Episodes Collected: 5828       Transitions Collected: 158798     Batch Size: 256   | NewEpisode Model(mean:25992     ) Reward(mean:1.29 , max:2.63 , min:0.66 , std:0.56 ) | total_loss: 2.989   reward_loss: 0.092   policy_loss: 4.964   value_loss: 0.763   consistency_loss: -1.129  lr: 0.000100  batch_future_return: 0.717   batch_model_diff: 10755.469target_model_diff: 100.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:43:23,974][train][INFO][log.py>_log] ==> #26200      Episodes Collected: 5851       Transitions Collected: 159500     Batch Size: 256   | NewEpisode Model(mean:26091     ) Reward(mean:1.56 , max:2.63 , min:0.66 , std:0.60 ) | total_loss: 0.841   reward_loss: 0.019   policy_loss: 1.394   value_loss: 0.215   consistency_loss: -0.313  lr: 0.000100  batch_future_return: 0.735   batch_model_diff: 11410.156target_model_diff: 200.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:43:23,977][train][WARNING][train.py>_train] ==> #26200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:43:36,076][train][WARNING][train.py>_train] ==> #26250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:43:48,437][train][INFO][log.py>_log] ==> #26300      Episodes Collected: 5875       Transitions Collected: 160163     Batch Size: 256   | NewEpisode Model(mean:26192     ) Reward(mean:1.48 , max:2.96 , min:0.66 , std:0.65 ) | total_loss: 2.113   reward_loss: 0.055   policy_loss: 3.556   value_loss: 0.557   consistency_loss: -0.818  lr: 0.000100  batch_future_return: 0.731   batch_model_diff: 10262.109target_model_diff: 100.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:44:12,830][train][INFO][log.py>_log] ==> #26400      Episodes Collected: 5901       Transitions Collected: 160896     Batch Size: 256   | NewEpisode Model(mean:26291     ) Reward(mean:1.56 , max:4.33 , min:0.33 , std:0.86 ) | total_loss: 1.467   reward_loss: 0.041   policy_loss: 2.527   value_loss: 0.415   consistency_loss: -0.603  lr: 0.000100  batch_future_return: 0.711   batch_model_diff: 10393.359target_model_diff: 200.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:44:12,833][train][WARNING][train.py>_train] ==> #26400 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:44:24,934][train][WARNING][train.py>_train] ==> #26450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:44:37,267][train][INFO][log.py>_log] ==> #26500      Episodes Collected: 5923       Transitions Collected: 161538     Batch Size: 256   | NewEpisode Model(mean:26392     ) Reward(mean:1.18 , max:2.30 , min:0.00 , std:0.52 ) | total_loss: 1.811   reward_loss: 0.041   policy_loss: 2.909   value_loss: 0.428   consistency_loss: -0.623  lr: 0.000100  batch_future_return: 0.778   batch_model_diff: 10741.016target_model_diff: 100.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:44:48,897][train][WARNING][train.py>_train] ==> #26550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:45:01,036][train][INFO][log.py>_log] ==> #26600      Episodes Collected: 5949       Transitions Collected: 162238     Batch Size: 256   | NewEpisode Model(mean:26490     ) Reward(mean:1.29 , max:2.30 , min:0.33 , std:0.56 ) | total_loss: 2.606   reward_loss: 0.066   policy_loss: 4.381   value_loss: 0.686   consistency_loss: -1.006  lr: 0.000100  batch_future_return: 0.660   batch_model_diff: 10332.812target_model_diff: 200.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:45:25,341][train][INFO][log.py>_log] ==> #26700      Episodes Collected: 5972       Transitions Collected: 162916     Batch Size: 256   | NewEpisode Model(mean:26593     ) Reward(mean:1.37 , max:2.30 , min:0.66 , std:0.49 ) | total_loss: 1.643   reward_loss: 0.042   policy_loss: 2.750   value_loss: 0.415   consistency_loss: -0.627  lr: 0.000100  batch_future_return: 0.693   batch_model_diff: 10759.766target_model_diff: 100.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:45:37,083][train][WARNING][train.py>_train] ==> #26750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:45:49,125][train][INFO][log.py>_log] ==> #26800      Episodes Collected: 5995       Transitions Collected: 163610     Batch Size: 256   | NewEpisode Model(mean:26690     ) Reward(mean:1.29 , max:1.97 , min:0.66 , std:0.39 ) | total_loss: 1.270   reward_loss: 0.029   policy_loss: 2.141   value_loss: 0.331   consistency_loss: -0.492  lr: 0.000100  batch_future_return: 0.646   batch_model_diff: 10482.422target_model_diff: 200.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:46:13,836][train][INFO][log.py>_log] ==> #26900      Episodes Collected: 6020       Transitions Collected: 164308     Batch Size: 256   | NewEpisode Model(mean:26791     ) Reward(mean:1.39 , max:2.30 , min:0.33 , std:0.50 ) | total_loss: 1.700   reward_loss: 0.050   policy_loss: 2.912   value_loss: 0.445   consistency_loss: -0.687  lr: 0.000100  batch_future_return: 0.681   batch_model_diff: 11711.328target_model_diff: 100.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:46:38,585][train][INFO][log.py>_log] ==> #27000      Episodes Collected: 6048       Transitions Collected: 165025     Batch Size: 256   | NewEpisode Model(mean:26892     ) Reward(mean:1.55 , max:2.63 , min:0.33 , std:0.55 ) | total_loss: 0.840   reward_loss: 0.023   policy_loss: 1.410   value_loss: 0.228   consistency_loss: -0.325  lr: 0.000100  batch_future_return: 0.745   batch_model_diff: 11033.594target_model_diff: 200.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:47:03,184][train][INFO][log.py>_log] ==> #27100      Episodes Collected: 6072       Transitions Collected: 165723     Batch Size: 256   | NewEpisode Model(mean:26992     ) Reward(mean:1.12 , max:1.97 , min:0.33 , std:0.38 ) | total_loss: 1.457   reward_loss: 0.040   policy_loss: 2.443   value_loss: 0.382   consistency_loss: -0.561  lr: 0.000100  batch_future_return: 0.722   batch_model_diff: 10416.016target_model_diff: 100.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:47:15,331][train][WARNING][train.py>_train] ==> #27150 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:47:27,446][train][INFO][log.py>_log] ==> #27200      Episodes Collected: 6099       Transitions Collected: 166417     Batch Size: 256   | NewEpisode Model(mean:27091     ) Reward(mean:1.25 , max:2.30 , min:0.33 , std:0.52 ) | total_loss: 2.460   reward_loss: 0.054   policy_loss: 3.943   value_loss: 0.589   consistency_loss: -0.842  lr: 0.000100  batch_future_return: 0.726   batch_model_diff: 11168.359target_model_diff: 200.000 Tp_perstep: 0.178   Tu_perstep: 0.049   
[2025-06-02 21:47:52,159][train][INFO][log.py>_log] ==> #27300      Episodes Collected: 6127       Transitions Collected: 167136     Batch Size: 256   | NewEpisode Model(mean:27192     ) Reward(mean:1.39 , max:2.63 , min:0.33 , std:0.47 ) | total_loss: 1.643   reward_loss: 0.039   policy_loss: 2.667   value_loss: 0.400   consistency_loss: -0.582  lr: 0.000100  batch_future_return: 0.790   batch_model_diff: 10601.172target_model_diff: 100.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:48:16,071][train][INFO][log.py>_log] ==> #27400      Episodes Collected: 6153       Transitions Collected: 167792     Batch Size: 256   | NewEpisode Model(mean:27291     ) Reward(mean:1.37 , max:3.62 , min:0.33 , std:0.72 ) | total_loss: 1.444   reward_loss: 0.036   policy_loss: 2.329   value_loss: 0.341   consistency_loss: -0.503  lr: 0.000100  batch_future_return: 0.746   batch_model_diff: 10242.969target_model_diff: 200.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:48:16,074][train][WARNING][train.py>_train] ==> #27400 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:48:40,231][train][INFO][log.py>_log] ==> #27500      Episodes Collected: 6179       Transitions Collected: 168494     Batch Size: 256   | NewEpisode Model(mean:27389     ) Reward(mean:1.49 , max:2.96 , min:0.33 , std:0.65 ) | total_loss: 1.500   reward_loss: 0.047   policy_loss: 2.616   value_loss: 0.426   consistency_loss: -0.635  lr: 0.000100  batch_future_return: 0.678   batch_model_diff: 11824.609target_model_diff: 100.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:48:40,233][train][WARNING][train.py>_train] ==> #27500 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:49:04,492][train][INFO][log.py>_log] ==> #27600      Episodes Collected: 6204       Transitions Collected: 169206     Batch Size: 256   | NewEpisode Model(mean:27494     ) Reward(mean:1.45 , max:2.63 , min:0.33 , std:0.61 ) | total_loss: 2.071   reward_loss: 0.053   policy_loss: 3.350   value_loss: 0.514   consistency_loss: -0.731  lr: 0.000100  batch_future_return: 0.762   batch_model_diff: 11718.750target_model_diff: 200.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:49:04,495][train][WARNING][train.py>_train] ==> #27600 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:49:28,380][train][INFO][log.py>_log] ==> #27700      Episodes Collected: 6229       Transitions Collected: 169854     Batch Size: 256   | NewEpisode Model(mean:27594     ) Reward(mean:1.58 , max:2.63 , min:0.66 , std:0.45 ) | total_loss: 1.435   reward_loss: 0.036   policy_loss: 2.410   value_loss: 0.373   consistency_loss: -0.552  lr: 0.000100  batch_future_return: 0.765   batch_model_diff: 11301.172target_model_diff: 100.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:49:28,382][train][WARNING][train.py>_train] ==> #27700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:49:40,472][train][WARNING][train.py>_train] ==> #27750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:49:52,567][train][INFO][log.py>_log] ==> #27800      Episodes Collected: 6253       Transitions Collected: 170496     Batch Size: 256   | NewEpisode Model(mean:27692     ) Reward(mean:1.54 , max:3.67 , min:0.66 , std:0.75 ) | total_loss: 3.848   reward_loss: 0.101   policy_loss: 5.979   value_loss: 0.864   consistency_loss: -1.224  lr: 0.000100  batch_future_return: 0.800   batch_model_diff: 10322.656target_model_diff: 200.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:50:04,280][train][WARNING][train.py>_train] ==> #27850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:50:16,381][train][INFO][log.py>_log] ==> #27900      Episodes Collected: 6279       Transitions Collected: 171208     Batch Size: 256   | NewEpisode Model(mean:27790     ) Reward(mean:1.54 , max:3.29 , min:0.66 , std:0.67 ) | total_loss: 2.348   reward_loss: 0.069   policy_loss: 3.908   value_loss: 0.619   consistency_loss: -0.892  lr: 0.000100  batch_future_return: 0.749   batch_model_diff: 11405.859target_model_diff: 100.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:50:40,662][train][INFO][log.py>_log] ==> #28000      Episodes Collected: 6304       Transitions Collected: 171899     Batch Size: 256   | NewEpisode Model(mean:27894     ) Reward(mean:1.38 , max:3.29 , min:0.33 , std:0.73 ) | total_loss: 2.079   reward_loss: 0.055   policy_loss: 3.400   value_loss: 0.521   consistency_loss: -0.753  lr: 0.000100  batch_future_return: 0.759   batch_model_diff: 10170.312target_model_diff: 200.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:50:40,665][train][WARNING][train.py>_train] ==> #28000 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:50:53,248][train][WARNING][train.py>_train] ==> #28050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:51:06,243][train][INFO][log.py>_log] ==> #28100      Episodes Collected: 6329       Transitions Collected: 172644     Batch Size: 256   | NewEpisode Model(mean:27991     ) Reward(mean:1.43 , max:2.30 , min:0.66 , std:0.38 ) | total_loss: 2.240   reward_loss: 0.069   policy_loss: 3.783   value_loss: 0.595   consistency_loss: -0.880  lr: 0.000100  batch_future_return: 0.742   batch_model_diff: 11042.578target_model_diff: 100.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:51:30,464][train][INFO][log.py>_log] ==> #28200      Episodes Collected: 6355       Transitions Collected: 173358     Batch Size: 256   | NewEpisode Model(mean:28091     ) Reward(mean:1.69 , max:3.29 , min:0.33 , std:0.67 ) | total_loss: 1.851   reward_loss: 0.055   policy_loss: 3.117   value_loss: 0.490   consistency_loss: -0.722  lr: 0.000100  batch_future_return: 0.759   batch_model_diff: 11226.172target_model_diff: 200.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:51:30,466][train][WARNING][train.py>_train] ==> #28200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:51:42,562][train][WARNING][train.py>_train] ==> #28250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:51:55,754][train][INFO][log.py>_log] ==> #28300      Episodes Collected: 6380       Transitions Collected: 174083     Batch Size: 256   | NewEpisode Model(mean:28194     ) Reward(mean:1.34 , max:1.97 , min:0.00 , std:0.51 ) | total_loss: 2.276   reward_loss: 0.064   policy_loss: 3.777   value_loss: 0.580   consistency_loss: -0.855  lr: 0.000100  batch_future_return: 0.751   batch_model_diff: 11219.531target_model_diff: 100.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:52:07,419][train][WARNING][train.py>_train] ==> #28350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:52:19,506][train][INFO][log.py>_log] ==> #28400      Episodes Collected: 6403       Transitions Collected: 174771     Batch Size: 256   | NewEpisode Model(mean:28289     ) Reward(mean:1.36 , max:1.97 , min:0.66 , std:0.45 ) | total_loss: 2.210   reward_loss: 0.056   policy_loss: 3.791   value_loss: 0.590   consistency_loss: -0.892  lr: 0.000100  batch_future_return: 0.694   batch_model_diff: 10953.125target_model_diff: 200.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:52:19,508][train][WARNING][train.py>_train] ==> #28400 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:52:44,668][train][INFO][log.py>_log] ==> #28500      Episodes Collected: 6432       Transitions Collected: 175527     Batch Size: 256   | NewEpisode Model(mean:28394     ) Reward(mean:1.60 , max:2.96 , min:0.66 , std:0.56 ) | total_loss: 2.536   reward_loss: 0.070   policy_loss: 4.256   value_loss: 0.648   consistency_loss: -0.976  lr: 0.000100  batch_future_return: 0.771   batch_model_diff: 10360.938target_model_diff: 100.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:52:56,238][train][WARNING][train.py>_train] ==> #28550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:53:08,941][train][INFO][log.py>_log] ==> #28600      Episodes Collected: 6458       Transitions Collected: 176180     Batch Size: 256   | NewEpisode Model(mean:28497     ) Reward(mean:1.38 , max:3.29 , min:0.66 , std:0.72 ) | total_loss: 1.592   reward_loss: 0.040   policy_loss: 2.614   value_loss: 0.410   consistency_loss: -0.583  lr: 0.000100  batch_future_return: 0.741   batch_model_diff: 11627.734target_model_diff: 200.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:53:33,215][train][INFO][log.py>_log] ==> #28700      Episodes Collected: 6483       Transitions Collected: 176881     Batch Size: 256   | NewEpisode Model(mean:28593     ) Reward(mean:1.67 , max:2.96 , min:0.33 , std:0.70 ) | total_loss: 3.334   reward_loss: 0.087   policy_loss: 5.494   value_loss: 0.846   consistency_loss: -1.229  lr: 0.000100  batch_future_return: 0.784   batch_model_diff: 11061.719target_model_diff: 100.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:53:44,883][train][WARNING][train.py>_train] ==> #28750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:53:56,951][train][INFO][log.py>_log] ==> #28800      Episodes Collected: 6509       Transitions Collected: 177549     Batch Size: 256   | NewEpisode Model(mean:28691     ) Reward(mean:1.45 , max:2.30 , min:0.33 , std:0.48 ) | total_loss: 2.567   reward_loss: 0.062   policy_loss: 3.974   value_loss: 0.585   consistency_loss: -0.808  lr: 0.000100  batch_future_return: 0.813   batch_model_diff: 11581.641target_model_diff: 200.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:54:09,201][train][WARNING][train.py>_train] ==> #28850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:54:21,453][train][INFO][log.py>_log] ==> #28900      Episodes Collected: 6535       Transitions Collected: 178239     Batch Size: 256   | NewEpisode Model(mean:28792     ) Reward(mean:1.38 , max:3.62 , min:0.33 , std:0.63 ) | total_loss: 2.599   reward_loss: 0.069   policy_loss: 4.425   value_loss: 0.691   consistency_loss: -1.034  lr: 0.000100  batch_future_return: 0.681   batch_model_diff: 11937.109target_model_diff: 100.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:54:34,085][train][WARNING][train.py>_train] ==> #28950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:54:46,632][train][INFO][log.py>_log] ==> #29000      Episodes Collected: 6561       Transitions Collected: 178970     Batch Size: 256   | NewEpisode Model(mean:28892     ) Reward(mean:1.32 , max:2.63 , min:0.33 , std:0.66 ) | total_loss: 1.801   reward_loss: 0.044   policy_loss: 2.930   value_loss: 0.428   consistency_loss: -0.640  lr: 0.000100  batch_future_return: 0.754   batch_model_diff: 12012.500target_model_diff: 200.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:55:10,408][train][INFO][log.py>_log] ==> #29100      Episodes Collected: 6586       Transitions Collected: 179604     Batch Size: 256   | NewEpisode Model(mean:28993     ) Reward(mean:1.59 , max:2.96 , min:0.33 , std:0.56 ) | total_loss: 2.446   reward_loss: 0.077   policy_loss: 3.951   value_loss: 0.616   consistency_loss: -0.868  lr: 0.000100  batch_future_return: 0.844   batch_model_diff: 11607.031target_model_diff: 100.000 Tp_perstep: 0.179   Tu_perstep: 0.049   
[2025-06-02 21:55:10,411][train][WARNING][train.py>_train] ==> #29100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:55:34,543][train][INFO][log.py>_log] ==> #29200      Episodes Collected: 6611       Transitions Collected: 180311     Batch Size: 256   | NewEpisode Model(mean:29089     ) Reward(mean:1.28 , max:3.29 , min:0.33 , std:0.63 ) | total_loss: 1.623   reward_loss: 0.038   policy_loss: 2.645   value_loss: 0.412   consistency_loss: -0.582  lr: 0.000100  batch_future_return: 0.726   batch_model_diff: 12291.797target_model_diff: 200.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 21:55:59,228][train][INFO][log.py>_log] ==> #29300      Episodes Collected: 6635       Transitions Collected: 180964     Batch Size: 256   | NewEpisode Model(mean:29194     ) Reward(mean:1.36 , max:2.30 , min:0.00 , std:0.56 ) | total_loss: 1.797   reward_loss: 0.050   policy_loss: 3.072   value_loss: 0.489   consistency_loss: -0.724  lr: 0.000100  batch_future_return: 0.763   batch_model_diff: 12234.375target_model_diff: 100.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 21:56:10,769][train][WARNING][train.py>_train] ==> #29350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:56:22,939][train][INFO][log.py>_log] ==> #29400      Episodes Collected: 6660       Transitions Collected: 181679     Batch Size: 256   | NewEpisode Model(mean:29290     ) Reward(mean:1.56 , max:2.63 , min:0.66 , std:0.49 ) | total_loss: 1.958   reward_loss: 0.059   policy_loss: 3.324   value_loss: 0.513   consistency_loss: -0.777  lr: 0.000100  batch_future_return: 0.778   batch_model_diff: 12610.547target_model_diff: 200.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 21:56:22,942][train][WARNING][train.py>_train] ==> #29400 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:56:47,175][train][INFO][log.py>_log] ==> #29500      Episodes Collected: 6684       Transitions Collected: 182339     Batch Size: 256   | NewEpisode Model(mean:29393     ) Reward(mean:1.33 , max:1.97 , min:0.00 , std:0.49 ) | total_loss: 1.652   reward_loss: 0.047   policy_loss: 2.750   value_loss: 0.435   consistency_loss: -0.627  lr: 0.000100  batch_future_return: 0.775   batch_model_diff: 12468.750target_model_diff: 100.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 21:57:11,485][train][INFO][log.py>_log] ==> #29600      Episodes Collected: 6710       Transitions Collected: 183016     Batch Size: 256   | NewEpisode Model(mean:29492     ) Reward(mean:1.38 , max:2.63 , min:0.33 , std:0.61 ) | total_loss: 2.101   reward_loss: 0.052   policy_loss: 3.366   value_loss: 0.517   consistency_loss: -0.723  lr: 0.000100  batch_future_return: 0.759   batch_model_diff: 12629.688target_model_diff: 200.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 21:57:23,115][train][WARNING][train.py>_train] ==> #29650 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:57:35,312][train][INFO][log.py>_log] ==> #29700      Episodes Collected: 6735       Transitions Collected: 183673     Batch Size: 256   | NewEpisode Model(mean:29590     ) Reward(mean:1.43 , max:1.97 , min:0.33 , std:0.43 ) | total_loss: 2.226   reward_loss: 0.057   policy_loss: 3.539   value_loss: 0.549   consistency_loss: -0.754  lr: 0.000100  batch_future_return: 0.837   batch_model_diff: 11676.953target_model_diff: 100.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 21:57:35,314][train][WARNING][train.py>_train] ==> #29700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:57:47,614][train][WARNING][train.py>_train] ==> #29750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:58:00,565][train][INFO][log.py>_log] ==> #29800      Episodes Collected: 6759       Transitions Collected: 184441     Batch Size: 256   | NewEpisode Model(mean:29691     ) Reward(mean:1.33 , max:2.63 , min:0.33 , std:0.60 ) | total_loss: 1.855   reward_loss: 0.052   policy_loss: 3.241   value_loss: 0.518   consistency_loss: -0.784  lr: 0.000100  batch_future_return: 0.686   batch_model_diff: 12628.516target_model_diff: 200.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 21:58:24,837][train][INFO][log.py>_log] ==> #29900      Episodes Collected: 6784       Transitions Collected: 185132     Batch Size: 256   | NewEpisode Model(mean:29791     ) Reward(mean:1.49 , max:3.62 , min:0.00 , std:0.74 ) | total_loss: 1.784   reward_loss: 0.054   policy_loss: 3.113   value_loss: 0.520   consistency_loss: -0.757  lr: 0.000100  batch_future_return: 0.708   batch_model_diff: 12484.375target_model_diff: 100.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 21:58:24,839][train][WARNING][train.py>_train] ==> #29900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:58:50,644][train][INFO][log.py>_log] ==> #30000      Episodes Collected: 6811       Transitions Collected: 185865     Batch Size: 256   | NewEpisode Model(mean:29893     ) Reward(mean:1.47 , max:3.29 , min:0.33 , std:0.65 ) | total_loss: 1.312   reward_loss: 0.031   policy_loss: 2.132   value_loss: 0.320   consistency_loss: -0.465  lr: 0.000100  batch_future_return: 0.704   batch_model_diff: 12415.234target_model_diff: 200.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 21:59:02,324][train][WARNING][train.py>_train] ==> #30050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:59:14,494][train][INFO][log.py>_log] ==> #30100      Episodes Collected: 6835       Transitions Collected: 186526     Batch Size: 256   | NewEpisode Model(mean:29990     ) Reward(mean:1.32 , max:1.97 , min:0.66 , std:0.41 ) | total_loss: 2.465   reward_loss: 0.066   policy_loss: 4.036   value_loss: 0.627   consistency_loss: -0.897  lr: 0.000100  batch_future_return: 0.787   batch_model_diff: 12621.484target_model_diff: 100.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 21:59:14,495][train][WARNING][train.py>_train] ==> #30100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 21:59:39,153][train][INFO][log.py>_log] ==> #30200      Episodes Collected: 6860       Transitions Collected: 187236     Batch Size: 256   | NewEpisode Model(mean:30088     ) Reward(mean:1.45 , max:2.96 , min:0.33 , std:0.57 ) | total_loss: 0.887   reward_loss: 0.024   policy_loss: 1.482   value_loss: 0.238   consistency_loss: -0.339  lr: 0.000100  batch_future_return: 0.758   batch_model_diff: 12711.328target_model_diff: 200.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 22:00:03,579][train][INFO][log.py>_log] ==> #30300      Episodes Collected: 6887       Transitions Collected: 187960     Batch Size: 256   | NewEpisode Model(mean:30190     ) Reward(mean:1.41 , max:3.29 , min:0.66 , std:0.63 ) | total_loss: 2.920   reward_loss: 0.085   policy_loss: 5.039   value_loss: 0.784   consistency_loss: -1.200  lr: 0.000100  batch_future_return: 0.700   batch_model_diff: 12188.672target_model_diff: 100.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 22:00:27,955][train][INFO][log.py>_log] ==> #30400      Episodes Collected: 6911       Transitions Collected: 188636     Batch Size: 256   | NewEpisode Model(mean:30293     ) Reward(mean:1.29 , max:2.30 , min:0.66 , std:0.39 ) | total_loss: 2.261   reward_loss: 0.056   policy_loss: 3.753   value_loss: 0.584   consistency_loss: -0.847  lr: 0.000100  batch_future_return: 0.706   batch_model_diff: 12751.953target_model_diff: 200.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 22:00:27,957][train][WARNING][train.py>_train] ==> #30400 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:00:53,142][train][INFO][log.py>_log] ==> #30500      Episodes Collected: 6939       Transitions Collected: 189381     Batch Size: 256   | NewEpisode Model(mean:30390     ) Reward(mean:1.57 , max:2.63 , min:0.66 , std:0.52 ) | total_loss: 1.753   reward_loss: 0.048   policy_loss: 2.885   value_loss: 0.456   consistency_loss: -0.647  lr: 0.000100  batch_future_return: 0.817   batch_model_diff: 12278.516target_model_diff: 100.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 22:01:17,457][train][INFO][log.py>_log] ==> #30600      Episodes Collected: 6966       Transitions Collected: 190076     Batch Size: 256   | NewEpisode Model(mean:30492     ) Reward(mean:1.69 , max:2.96 , min:0.33 , std:0.58 ) | total_loss: 1.435   reward_loss: 0.044   policy_loss: 2.522   value_loss: 0.420   consistency_loss: -0.618  lr: 0.000100  batch_future_return: 0.731   batch_model_diff: 13080.469target_model_diff: 200.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 22:01:41,687][train][INFO][log.py>_log] ==> #30700      Episodes Collected: 6990       Transitions Collected: 190746     Batch Size: 256   | NewEpisode Model(mean:30591     ) Reward(mean:1.15 , max:1.97 , min:0.00 , std:0.46 ) | total_loss: 2.176   reward_loss: 0.048   policy_loss: 3.619   value_loss: 0.557   consistency_loss: -0.815  lr: 0.000100  batch_future_return: 0.680   batch_model_diff: 12201.172target_model_diff: 100.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 22:02:06,031][train][INFO][log.py>_log] ==> #30800      Episodes Collected: 7016       Transitions Collected: 191480     Batch Size: 256   | NewEpisode Model(mean:30689     ) Reward(mean:1.33 , max:2.30 , min:0.33 , std:0.52 ) | total_loss: 1.534   reward_loss: 0.039   policy_loss: 2.559   value_loss: 0.390   consistency_loss: -0.581  lr: 0.000100  batch_future_return: 0.717   batch_model_diff: 11783.203target_model_diff: 200.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 22:02:06,033][train][WARNING][train.py>_train] ==> #30800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:02:18,182][train][WARNING][train.py>_train] ==> #30850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:02:31,181][train][INFO][log.py>_log] ==> #30900      Episodes Collected: 7042       Transitions Collected: 192154     Batch Size: 256   | NewEpisode Model(mean:30795     ) Reward(mean:1.45 , max:2.30 , min:0.33 , std:0.53 ) | total_loss: 3.101   reward_loss: 0.083   policy_loss: 5.253   value_loss: 0.834   consistency_loss: -1.221  lr: 0.000100  batch_future_return: 0.753   batch_model_diff: 12125.000target_model_diff: 100.000 Tp_perstep: 0.180   Tu_perstep: 0.049   
[2025-06-02 22:02:55,965][train][INFO][log.py>_log] ==> #31000      Episodes Collected: 7066       Transitions Collected: 192901     Batch Size: 256   | NewEpisode Model(mean:30888     ) Reward(mean:1.59 , max:2.63 , min:0.66 , std:0.55 ) | total_loss: 0.734   reward_loss: 0.015   policy_loss: 1.178   value_loss: 0.177   consistency_loss: -0.251  lr: 0.000100  batch_future_return: 0.676   batch_model_diff: 12657.031target_model_diff: 200.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:03:20,185][train][INFO][log.py>_log] ==> #31100      Episodes Collected: 7090       Transitions Collected: 193571     Batch Size: 256   | NewEpisode Model(mean:30994     ) Reward(mean:1.38 , max:2.63 , min:0.33 , std:0.64 ) | total_loss: 1.119   reward_loss: 0.027   policy_loss: 1.827   value_loss: 0.280   consistency_loss: -0.402  lr: 0.000100  batch_future_return: 0.758   batch_model_diff: 13113.672target_model_diff: 100.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:03:44,989][train][INFO][log.py>_log] ==> #31200      Episodes Collected: 7116       Transitions Collected: 194306     Batch Size: 256   | NewEpisode Model(mean:31091     ) Reward(mean:1.32 , max:2.63 , min:0.33 , std:0.59 ) | total_loss: 1.665   reward_loss: 0.044   policy_loss: 2.751   value_loss: 0.409   consistency_loss: -0.616  lr: 0.000100  batch_future_return: 0.710   batch_model_diff: 11778.516target_model_diff: 200.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:04:09,678][train][INFO][log.py>_log] ==> #31300      Episodes Collected: 7141       Transitions Collected: 195034     Batch Size: 256   | NewEpisode Model(mean:31194     ) Reward(mean:1.33 , max:2.63 , min:0.66 , std:0.51 ) | total_loss: 1.217   reward_loss: 0.035   policy_loss: 2.139   value_loss: 0.340   consistency_loss: -0.521  lr: 0.000100  batch_future_return: 0.713   batch_model_diff: 12568.359target_model_diff: 100.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:04:21,915][train][WARNING][train.py>_train] ==> #31350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:04:34,949][train][INFO][log.py>_log] ==> #31400      Episodes Collected: 7165       Transitions Collected: 195767     Batch Size: 256   | NewEpisode Model(mean:31293     ) Reward(mean:1.49 , max:2.96 , min:0.66 , std:0.50 ) | total_loss: 1.067   reward_loss: 0.029   policy_loss: 1.779   value_loss: 0.276   consistency_loss: -0.405  lr: 0.000100  batch_future_return: 0.746   batch_model_diff: 12090.625target_model_diff: 200.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:04:58,880][train][INFO][log.py>_log] ==> #31500      Episodes Collected: 7191       Transitions Collected: 196457     Batch Size: 256   | NewEpisode Model(mean:31392     ) Reward(mean:1.42 , max:2.30 , min:0.33 , std:0.61 ) | total_loss: 1.215   reward_loss: 0.034   policy_loss: 2.078   value_loss: 0.338   consistency_loss: -0.491  lr: 0.000100  batch_future_return: 0.764   batch_model_diff: 12564.453target_model_diff: 100.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:04:58,881][train][WARNING][train.py>_train] ==> #31500 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:05:23,100][train][INFO][log.py>_log] ==> #31600      Episodes Collected: 7215       Transitions Collected: 197109     Batch Size: 256   | NewEpisode Model(mean:31494     ) Reward(mean:1.30 , max:2.30 , min:0.33 , std:0.54 ) | total_loss: 2.400   reward_loss: 0.055   policy_loss: 4.028   value_loss: 0.627   consistency_loss: -0.920  lr: 0.000100  batch_future_return: 0.714   batch_model_diff: 12617.969target_model_diff: 200.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:05:34,787][train][WARNING][train.py>_train] ==> #31650 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:05:47,414][train][INFO][log.py>_log] ==> #31700      Episodes Collected: 7241       Transitions Collected: 197780     Batch Size: 256   | NewEpisode Model(mean:31591     ) Reward(mean:1.28 , max:2.30 , min:0.33 , std:0.51 ) | total_loss: 2.113   reward_loss: 0.055   policy_loss: 3.397   value_loss: 0.515   consistency_loss: -0.733  lr: 0.000100  batch_future_return: 0.785   batch_model_diff: 13137.500target_model_diff: 100.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:06:11,615][train][INFO][log.py>_log] ==> #31800      Episodes Collected: 7268       Transitions Collected: 198489     Batch Size: 256   | NewEpisode Model(mean:31691     ) Reward(mean:1.29 , max:3.29 , min:0.33 , std:0.69 ) | total_loss: 1.676   reward_loss: 0.040   policy_loss: 2.691   value_loss: 0.416   consistency_loss: -0.579  lr: 0.000100  batch_future_return: 0.821   batch_model_diff: 12097.266target_model_diff: 200.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:06:11,618][train][WARNING][train.py>_train] ==> #31800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:06:36,388][train][INFO][log.py>_log] ==> #31900      Episodes Collected: 7294       Transitions Collected: 199198     Batch Size: 256   | NewEpisode Model(mean:31792     ) Reward(mean:1.40 , max:2.63 , min:0.33 , std:0.50 ) | total_loss: 0.609   reward_loss: 0.017   policy_loss: 1.010   value_loss: 0.158   consistency_loss: -0.229  lr: 0.000100  batch_future_return: 0.686   batch_model_diff: 11408.984target_model_diff: 100.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:07:00,688][train][INFO][log.py>_log] ==> #32000      Episodes Collected: 7321       Transitions Collected: 199907     Batch Size: 256   | NewEpisode Model(mean:31891     ) Reward(mean:1.46 , max:2.30 , min:0.66 , std:0.53 ) | total_loss: 2.032   reward_loss: 0.048   policy_loss: 3.236   value_loss: 0.480   consistency_loss: -0.686  lr: 0.000100  batch_future_return: 0.840   batch_model_diff: 13358.984target_model_diff: 200.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:07:25,453][train][INFO][log.py>_log] ==> #32100      Episodes Collected: 7346       Transitions Collected: 200595     Batch Size: 256   | NewEpisode Model(mean:31994     ) Reward(mean:1.24 , max:2.96 , min:0.33 , std:0.65 ) | total_loss: 2.954   reward_loss: 0.075   policy_loss: 4.793   value_loss: 0.742   consistency_loss: -1.050  lr: 0.000100  batch_future_return: 0.782   batch_model_diff: 12394.531target_model_diff: 100.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:07:49,789][train][INFO][log.py>_log] ==> #32200      Episodes Collected: 7370       Transitions Collected: 201291     Batch Size: 256   | NewEpisode Model(mean:32089     ) Reward(mean:1.55 , max:2.30 , min:0.33 , std:0.62 ) | total_loss: 2.279   reward_loss: 0.052   policy_loss: 3.690   value_loss: 0.564   consistency_loss: -0.802  lr: 0.000100  batch_future_return: 0.708   batch_model_diff: 12427.734target_model_diff: 200.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:08:01,942][train][WARNING][train.py>_train] ==> #32250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:08:14,165][train][INFO][log.py>_log] ==> #32300      Episodes Collected: 7397       Transitions Collected: 201949     Batch Size: 256   | NewEpisode Model(mean:32190     ) Reward(mean:1.42 , max:2.63 , min:0.66 , std:0.45 ) | total_loss: 1.623   reward_loss: 0.050   policy_loss: 2.810   value_loss: 0.460   consistency_loss: -0.676  lr: 0.000100  batch_future_return: 0.714   batch_model_diff: 13639.844target_model_diff: 100.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:08:38,549][train][INFO][log.py>_log] ==> #32400      Episodes Collected: 7423       Transitions Collected: 202678     Batch Size: 256   | NewEpisode Model(mean:32287     ) Reward(mean:1.40 , max:2.30 , min:0.33 , std:0.55 ) | total_loss: 1.280   reward_loss: 0.039   policy_loss: 2.233   value_loss: 0.371   consistency_loss: -0.543  lr: 0.000100  batch_future_return: 0.750   batch_model_diff: 12774.219target_model_diff: 200.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:09:02,297][train][INFO][log.py>_log] ==> #32500      Episodes Collected: 7447       Transitions Collected: 203362     Batch Size: 256   | NewEpisode Model(mean:32395     ) Reward(mean:1.33 , max:3.29 , min:0.33 , std:0.69 ) | total_loss: 1.808   reward_loss: 0.048   policy_loss: 3.070   value_loss: 0.491   consistency_loss: -0.716  lr: 0.000100  batch_future_return: 0.691   batch_model_diff: 12464.844target_model_diff: 100.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:09:02,300][train][WARNING][train.py>_train] ==> #32500 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:09:14,440][train][WARNING][train.py>_train] ==> #32550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:09:26,657][train][INFO][log.py>_log] ==> #32600      Episodes Collected: 7474       Transitions Collected: 204024     Batch Size: 256   | NewEpisode Model(mean:32495     ) Reward(mean:1.46 , max:1.97 , min:0.66 , std:0.40 ) | total_loss: 3.122   reward_loss: 0.077   policy_loss: 5.004   value_loss: 0.764   consistency_loss: -1.075  lr: 0.000100  batch_future_return: 0.743   batch_model_diff: 13581.641target_model_diff: 200.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:09:38,791][train][WARNING][train.py>_train] ==> #32650 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:09:50,913][train][INFO][log.py>_log] ==> #32700      Episodes Collected: 7500       Transitions Collected: 204720     Batch Size: 256   | NewEpisode Model(mean:32590     ) Reward(mean:1.43 , max:2.30 , min:0.66 , std:0.47 ) | total_loss: 2.161   reward_loss: 0.057   policy_loss: 3.670   value_loss: 0.567   consistency_loss: -0.854  lr: 0.000100  batch_future_return: 0.709   batch_model_diff: 13215.625target_model_diff: 100.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:10:02,939][train][WARNING][train.py>_train] ==> #32750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:10:15,946][train][INFO][log.py>_log] ==> #32800      Episodes Collected: 7525       Transitions Collected: 205407     Batch Size: 256   | NewEpisode Model(mean:32692     ) Reward(mean:1.43 , max:2.30 , min:0.33 , std:0.49 ) | total_loss: 2.391   reward_loss: 0.067   policy_loss: 4.105   value_loss: 0.660   consistency_loss: -0.973  lr: 0.000100  batch_future_return: 0.695   batch_model_diff: 14114.844target_model_diff: 200.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:10:15,948][train][WARNING][train.py>_train] ==> #32800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:10:40,540][train][INFO][log.py>_log] ==> #32900      Episodes Collected: 7551       Transitions Collected: 206162     Batch Size: 256   | NewEpisode Model(mean:32788     ) Reward(mean:1.48 , max:2.96 , min:0.33 , std:0.57 ) | total_loss: 1.786   reward_loss: 0.049   policy_loss: 2.821   value_loss: 0.438   consistency_loss: -0.597  lr: 0.000100  batch_future_return: 0.812   batch_model_diff: 13838.281target_model_diff: 100.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:11:04,639][train][INFO][log.py>_log] ==> #33000      Episodes Collected: 7574       Transitions Collected: 206817     Batch Size: 256   | NewEpisode Model(mean:32893     ) Reward(mean:1.49 , max:4.00 , min:0.33 , std:0.68 ) | total_loss: 2.322   reward_loss: 0.059   policy_loss: 3.860   value_loss: 0.609   consistency_loss: -0.875  lr: 0.000100  batch_future_return: 0.745   batch_model_diff: 13267.188target_model_diff: 200.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:11:28,949][train][INFO][log.py>_log] ==> #33100      Episodes Collected: 7600       Transitions Collected: 207508     Batch Size: 256   | NewEpisode Model(mean:32993     ) Reward(mean:1.39 , max:2.96 , min:0.66 , std:0.63 ) | total_loss: 1.548   reward_loss: 0.036   policy_loss: 2.602   value_loss: 0.405   consistency_loss: -0.596  lr: 0.000100  batch_future_return: 0.672   batch_model_diff: 13876.562target_model_diff: 100.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:11:28,951][train][WARNING][train.py>_train] ==> #33100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:11:53,705][train][INFO][log.py>_log] ==> #33200      Episodes Collected: 7627       Transitions Collected: 208257     Batch Size: 256   | NewEpisode Model(mean:33091     ) Reward(mean:1.29 , max:2.30 , min:0.33 , std:0.51 ) | total_loss: 2.848   reward_loss: 0.074   policy_loss: 4.889   value_loss: 0.774   consistency_loss: -1.154  lr: 0.000100  batch_future_return: 0.691   batch_model_diff: 13263.672target_model_diff: 200.000 Tp_perstep: 0.181   Tu_perstep: 0.049   
[2025-06-02 22:12:05,349][train][WARNING][train.py>_train] ==> #33250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:12:17,456][train][INFO][log.py>_log] ==> #33300      Episodes Collected: 7651       Transitions Collected: 208907     Batch Size: 256   | NewEpisode Model(mean:33194     ) Reward(mean:1.25 , max:2.63 , min:0.33 , std:0.57 ) | total_loss: 1.774   reward_loss: 0.046   policy_loss: 2.920   value_loss: 0.458   consistency_loss: -0.653  lr: 0.000100  batch_future_return: 0.713   batch_model_diff: 14075.000target_model_diff: 100.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:12:41,676][train][INFO][log.py>_log] ==> #33400      Episodes Collected: 7670       Transitions Collected: 209575     Batch Size: 256   | NewEpisode Model(mean:33290     ) Reward(mean:1.35 , max:1.97 , min:0.66 , std:0.43 ) | total_loss: 1.202   reward_loss: 0.030   policy_loss: 2.014   value_loss: 0.322   consistency_loss: -0.461  lr: 0.000100  batch_future_return: 0.717   batch_model_diff: 14076.172target_model_diff: 200.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:13:06,519][train][INFO][log.py>_log] ==> #33500      Episodes Collected: 7695       Transitions Collected: 210266     Batch Size: 256   | NewEpisode Model(mean:33393     ) Reward(mean:1.37 , max:1.97 , min:0.33 , std:0.43 ) | total_loss: 1.673   reward_loss: 0.043   policy_loss: 2.762   value_loss: 0.437   consistency_loss: -0.621  lr: 0.000100  batch_future_return: 0.791   batch_model_diff: 13680.469target_model_diff: 100.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:13:18,181][train][WARNING][train.py>_train] ==> #33550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:13:30,353][train][INFO][log.py>_log] ==> #33600      Episodes Collected: 7721       Transitions Collected: 210930     Batch Size: 256   | NewEpisode Model(mean:33488     ) Reward(mean:1.44 , max:3.62 , min:0.33 , std:0.61 ) | total_loss: 2.074   reward_loss: 0.055   policy_loss: 3.512   value_loss: 0.565   consistency_loss: -0.817  lr: 0.000100  batch_future_return: 0.731   batch_model_diff: 14441.406target_model_diff: 200.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:13:30,356][train][WARNING][train.py>_train] ==> #33600 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:13:42,446][train][WARNING][train.py>_train] ==> #33650 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:13:54,659][train][INFO][log.py>_log] ==> #33700      Episodes Collected: 7746       Transitions Collected: 211667     Batch Size: 256   | NewEpisode Model(mean:33588     ) Reward(mean:1.34 , max:2.63 , min:0.33 , std:0.59 ) | total_loss: 2.351   reward_loss: 0.066   policy_loss: 3.839   value_loss: 0.599   consistency_loss: -0.852  lr: 0.000100  batch_future_return: 0.766   batch_model_diff: 14687.891target_model_diff: 100.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:14:06,314][train][WARNING][train.py>_train] ==> #33750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:14:19,002][train][INFO][log.py>_log] ==> #33800      Episodes Collected: 7772       Transitions Collected: 212373     Batch Size: 256   | NewEpisode Model(mean:33693     ) Reward(mean:1.62 , max:2.96 , min:0.99 , std:0.51 ) | total_loss: 2.551   reward_loss: 0.070   policy_loss: 4.132   value_loss: 0.613   consistency_loss: -0.902  lr: 0.000100  batch_future_return: 0.759   batch_model_diff: 13564.062target_model_diff: 200.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:14:43,298][train][INFO][log.py>_log] ==> #33900      Episodes Collected: 7796       Transitions Collected: 213033     Batch Size: 256   | NewEpisode Model(mean:33795     ) Reward(mean:1.47 , max:2.96 , min:0.66 , std:0.51 ) | total_loss: 2.449   reward_loss: 0.078   policy_loss: 4.115   value_loss: 0.648   consistency_loss: -0.953  lr: 0.000100  batch_future_return: 0.714   batch_model_diff: 13900.391target_model_diff: 100.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:14:55,378][train][WARNING][train.py>_train] ==> #33950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:15:07,713][train][INFO][log.py>_log] ==> #34000      Episodes Collected: 7822       Transitions Collected: 213714     Batch Size: 256   | NewEpisode Model(mean:33891     ) Reward(mean:1.33 , max:2.30 , min:0.33 , std:0.45 ) | total_loss: 1.230   reward_loss: 0.032   policy_loss: 2.097   value_loss: 0.342   consistency_loss: -0.492  lr: 0.000100  batch_future_return: 0.760   batch_model_diff: 13449.609target_model_diff: 200.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:15:31,663][train][INFO][log.py>_log] ==> #34100      Episodes Collected: 7849       Transitions Collected: 214438     Batch Size: 256   | NewEpisode Model(mean:33988     ) Reward(mean:1.23 , max:2.30 , min:0.33 , std:0.56 ) | total_loss: 2.023   reward_loss: 0.051   policy_loss: 3.239   value_loss: 0.498   consistency_loss: -0.696  lr: 0.000100  batch_future_return: 0.782   batch_model_diff: 13740.625target_model_diff: 100.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:15:31,664][train][WARNING][train.py>_train] ==> #34100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:15:56,918][train][INFO][log.py>_log] ==> #34200      Episodes Collected: 7876       Transitions Collected: 215133     Batch Size: 256   | NewEpisode Model(mean:34093     ) Reward(mean:1.52 , max:2.96 , min:0.33 , std:0.61 ) | total_loss: 2.319   reward_loss: 0.063   policy_loss: 3.725   value_loss: 0.563   consistency_loss: -0.805  lr: 0.000100  batch_future_return: 0.729   batch_model_diff: 14850.781target_model_diff: 200.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:16:09,498][train][WARNING][train.py>_train] ==> #34250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:16:21,591][train][INFO][log.py>_log] ==> #34300      Episodes Collected: 7900       Transitions Collected: 215894     Batch Size: 256   | NewEpisode Model(mean:34192     ) Reward(mean:1.26 , max:1.97 , min:0.33 , std:0.53 ) | total_loss: 0.737   reward_loss: 0.022   policy_loss: 1.281   value_loss: 0.210   consistency_loss: -0.309  lr: 0.000100  batch_future_return: 0.705   batch_model_diff: 13508.594target_model_diff: 100.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:16:21,594][train][WARNING][train.py>_train] ==> #34300 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:16:46,445][train][INFO][log.py>_log] ==> #34400      Episodes Collected: 7927       Transitions Collected: 216606     Batch Size: 256   | NewEpisode Model(mean:34294     ) Reward(mean:1.39 , max:2.63 , min:0.33 , std:0.56 ) | total_loss: 0.592   reward_loss: 0.017   policy_loss: 1.077   value_loss: 0.178   consistency_loss: -0.273  lr: 0.000100  batch_future_return: 0.693   batch_model_diff: 14532.031target_model_diff: 200.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:17:11,524][train][INFO][log.py>_log] ==> #34500      Episodes Collected: 7950       Transitions Collected: 217314     Batch Size: 256   | NewEpisode Model(mean:34391     ) Reward(mean:1.30 , max:2.63 , min:0.66 , std:0.48 ) | total_loss: 2.637   reward_loss: 0.056   policy_loss: 4.327   value_loss: 0.673   consistency_loss: -0.957  lr: 0.000100  batch_future_return: 0.732   batch_model_diff: 15153.516target_model_diff: 100.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:17:23,167][train][WARNING][train.py>_train] ==> #34550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:17:36,320][train][INFO][log.py>_log] ==> #34600      Episodes Collected: 7978       Transitions Collected: 218043     Batch Size: 256   | NewEpisode Model(mean:34492     ) Reward(mean:1.53 , max:2.96 , min:0.66 , std:0.47 ) | total_loss: 2.664   reward_loss: 0.075   policy_loss: 4.512   value_loss: 0.739   consistency_loss: -1.054  lr: 0.000100  batch_future_return: 0.764   batch_model_diff: 14288.281target_model_diff: 200.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:18:01,691][train][INFO][log.py>_log] ==> #34700      Episodes Collected: 8006       Transitions Collected: 218807     Batch Size: 256   | NewEpisode Model(mean:34594     ) Reward(mean:1.48 , max:2.30 , min:0.33 , std:0.50 ) | total_loss: 1.346   reward_loss: 0.029   policy_loss: 2.244   value_loss: 0.360   consistency_loss: -0.508  lr: 0.000100  batch_future_return: 0.762   batch_model_diff: 14841.406target_model_diff: 100.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:18:25,825][train][INFO][log.py>_log] ==> #34800      Episodes Collected: 8031       Transitions Collected: 219459     Batch Size: 256   | NewEpisode Model(mean:34696     ) Reward(mean:1.47 , max:2.96 , min:0.66 , std:0.65 ) | total_loss: 1.406   reward_loss: 0.031   policy_loss: 2.248   value_loss: 0.334   consistency_loss: -0.478  lr: 0.000100  batch_future_return: 0.751   batch_model_diff: 14565.625target_model_diff: 200.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:18:50,947][train][INFO][log.py>_log] ==> #34900      Episodes Collected: 8058       Transitions Collected: 220205     Batch Size: 256   | NewEpisode Model(mean:34791     ) Reward(mean:1.39 , max:2.30 , min:0.33 , std:0.59 ) | total_loss: 1.181   reward_loss: 0.028   policy_loss: 2.017   value_loss: 0.321   consistency_loss: -0.472  lr: 0.000100  batch_future_return: 0.747   batch_model_diff: 15136.719target_model_diff: 100.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:19:15,285][train][INFO][log.py>_log] ==> #35000      Episodes Collected: 8083       Transitions Collected: 220912     Batch Size: 256   | NewEpisode Model(mean:34893     ) Reward(mean:1.37 , max:2.30 , min:0.66 , std:0.52 ) | total_loss: 2.219   reward_loss: 0.057   policy_loss: 3.668   value_loss: 0.578   consistency_loss: -0.826  lr: 0.000100  batch_future_return: 0.727   batch_model_diff: 14128.906target_model_diff: 200.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:19:27,381][train][WARNING][train.py>_train] ==> #35050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:19:40,101][train][INFO][log.py>_log] ==> #35100      Episodes Collected: 8110       Transitions Collected: 221609     Batch Size: 256   | NewEpisode Model(mean:34994     ) Reward(mean:1.73 , max:2.96 , min:0.66 , std:0.46 ) | total_loss: 1.879   reward_loss: 0.043   policy_loss: 3.076   value_loss: 0.452   consistency_loss: -0.677  lr: 0.000100  batch_future_return: 0.736   batch_model_diff: 13893.750target_model_diff: 100.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:20:03,902][train][INFO][log.py>_log] ==> #35200      Episodes Collected: 8135       Transitions Collected: 222268     Batch Size: 256   | NewEpisode Model(mean:35094     ) Reward(mean:1.46 , max:1.97 , min:0.66 , std:0.35 ) | total_loss: 1.720   reward_loss: 0.050   policy_loss: 2.869   value_loss: 0.431   consistency_loss: -0.653  lr: 0.000100  batch_future_return: 0.699   batch_model_diff: 14116.406target_model_diff: 200.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:20:28,278][train][INFO][log.py>_log] ==> #35300      Episodes Collected: 8159       Transitions Collected: 222953     Batch Size: 256   | NewEpisode Model(mean:35192     ) Reward(mean:1.49 , max:2.63 , min:0.33 , std:0.55 ) | total_loss: 1.623   reward_loss: 0.048   policy_loss: 2.862   value_loss: 0.454   consistency_loss: -0.700  lr: 0.000100  batch_future_return: 0.713   batch_model_diff: 14469.922target_model_diff: 100.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:20:39,901][train][WARNING][train.py>_train] ==> #35350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:20:52,664][train][INFO][log.py>_log] ==> #35400      Episodes Collected: 8184       Transitions Collected: 223658     Batch Size: 256   | NewEpisode Model(mean:35292     ) Reward(mean:1.26 , max:1.97 , min:0.66 , std:0.38 ) | total_loss: 2.587   reward_loss: 0.071   policy_loss: 4.320   value_loss: 0.678   consistency_loss: -0.986  lr: 0.000100  batch_future_return: 0.759   batch_model_diff: 13466.406target_model_diff: 200.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:21:17,035][train][INFO][log.py>_log] ==> #35500      Episodes Collected: 8210       Transitions Collected: 224342     Batch Size: 256   | NewEpisode Model(mean:35393     ) Reward(mean:1.53 , max:3.62 , min:0.00 , std:0.73 ) | total_loss: 1.419   reward_loss: 0.037   policy_loss: 2.291   value_loss: 0.350   consistency_loss: -0.498  lr: 0.000100  batch_future_return: 0.747   batch_model_diff: 15435.156target_model_diff: 100.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:21:29,095][train][WARNING][train.py>_train] ==> #35550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:21:41,422][train][INFO][log.py>_log] ==> #35600      Episodes Collected: 8236       Transitions Collected: 225020     Batch Size: 256   | NewEpisode Model(mean:35493     ) Reward(mean:1.42 , max:2.30 , min:0.66 , std:0.44 ) | total_loss: 0.838   reward_loss: 0.025   policy_loss: 1.451   value_loss: 0.240   consistency_loss: -0.349  lr: 0.000100  batch_future_return: 0.814   batch_model_diff: 14362.891target_model_diff: 200.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:22:06,166][train][INFO][log.py>_log] ==> #35700      Episodes Collected: 8265       Transitions Collected: 225759     Batch Size: 256   | NewEpisode Model(mean:35590     ) Reward(mean:1.47 , max:2.63 , min:0.33 , std:0.58 ) | total_loss: 0.530   reward_loss: 0.016   policy_loss: 0.895   value_loss: 0.146   consistency_loss: -0.209  lr: 0.000100  batch_future_return: 0.705   batch_model_diff: 15789.453target_model_diff: 100.000 Tp_perstep: 0.182   Tu_perstep: 0.049   
[2025-06-02 22:22:31,021][train][INFO][log.py>_log] ==> #35800      Episodes Collected: 8289       Transitions Collected: 226442     Batch Size: 256   | NewEpisode Model(mean:35695     ) Reward(mean:1.36 , max:2.30 , min:0.66 , std:0.58 ) | total_loss: 2.027   reward_loss: 0.059   policy_loss: 3.459   value_loss: 0.561   consistency_loss: -0.816  lr: 0.000100  batch_future_return: 0.764   batch_model_diff: 14229.688target_model_diff: 200.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:22:43,092][train][WARNING][train.py>_train] ==> #35850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:22:55,428][train][INFO][log.py>_log] ==> #35900      Episodes Collected: 8315       Transitions Collected: 227142     Batch Size: 256   | NewEpisode Model(mean:35790     ) Reward(mean:1.53 , max:2.63 , min:0.66 , std:0.46 ) | total_loss: 1.650   reward_loss: 0.042   policy_loss: 2.607   value_loss: 0.396   consistency_loss: -0.549  lr: 0.000100  batch_future_return: 0.807   batch_model_diff: 14585.938target_model_diff: 100.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:22:55,431][train][WARNING][train.py>_train] ==> #35900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:23:20,277][train][INFO][log.py>_log] ==> #36000      Episodes Collected: 8340       Transitions Collected: 227845     Batch Size: 256   | NewEpisode Model(mean:35888     ) Reward(mean:1.38 , max:2.30 , min:0.33 , std:0.51 ) | total_loss: 2.994   reward_loss: 0.066   policy_loss: 4.796   value_loss: 0.722   consistency_loss: -1.024  lr: 0.000100  batch_future_return: 0.759   batch_model_diff: 15233.594target_model_diff: 200.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:23:44,274][train][INFO][log.py>_log] ==> #36100      Episodes Collected: 8367       Transitions Collected: 228561     Batch Size: 256   | NewEpisode Model(mean:35990     ) Reward(mean:1.50 , max:2.63 , min:0.66 , std:0.60 ) | total_loss: 1.453   reward_loss: 0.038   policy_loss: 2.395   value_loss: 0.385   consistency_loss: -0.538  lr: 0.000100  batch_future_return: 0.744   batch_model_diff: 15697.266target_model_diff: 100.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:23:44,276][train][WARNING][train.py>_train] ==> #36100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:24:08,697][train][INFO][log.py>_log] ==> #36200      Episodes Collected: 8392       Transitions Collected: 229213     Batch Size: 256   | NewEpisode Model(mean:36093     ) Reward(mean:1.55 , max:2.96 , min:0.99 , std:0.53 ) | total_loss: 1.782   reward_loss: 0.042   policy_loss: 3.057   value_loss: 0.487   consistency_loss: -0.719  lr: 0.000100  batch_future_return: 0.696   batch_model_diff: 15082.422target_model_diff: 200.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:24:20,793][train][WARNING][train.py>_train] ==> #36250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:24:32,975][train][INFO][log.py>_log] ==> #36300      Episodes Collected: 8418       Transitions Collected: 229912     Batch Size: 256   | NewEpisode Model(mean:36190     ) Reward(mean:1.25 , max:2.96 , min:0.33 , std:0.52 ) | total_loss: 1.843   reward_loss: 0.050   policy_loss: 3.104   value_loss: 0.487   consistency_loss: -0.716  lr: 0.000100  batch_future_return: 0.696   batch_model_diff: 14704.297target_model_diff: 100.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:24:57,244][train][INFO][log.py>_log] ==> #36400      Episodes Collected: 8440       Transitions Collected: 230615     Batch Size: 256   | NewEpisode Model(mean:36293     ) Reward(mean:1.36 , max:2.30 , min:0.66 , std:0.50 ) | total_loss: 0.649   reward_loss: 0.015   policy_loss: 1.074   value_loss: 0.171   consistency_loss: -0.241  lr: 0.000100  batch_future_return: 0.798   batch_model_diff: 14970.312target_model_diff: 200.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:25:09,820][train][WARNING][train.py>_train] ==> #36450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:25:22,038][train][INFO][log.py>_log] ==> #36500      Episodes Collected: 8469       Transitions Collected: 231342     Batch Size: 256   | NewEpisode Model(mean:36394     ) Reward(mean:1.49 , max:2.63 , min:0.66 , std:0.59 ) | total_loss: 2.433   reward_loss: 0.057   policy_loss: 3.969   value_loss: 0.611   consistency_loss: -0.873  lr: 0.000100  batch_future_return: 0.701   batch_model_diff: 15192.578target_model_diff: 100.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:25:22,040][train][WARNING][train.py>_train] ==> #36500 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:25:47,030][train][INFO][log.py>_log] ==> #36600      Episodes Collected: 8496       Transitions Collected: 232066     Batch Size: 256   | NewEpisode Model(mean:36494     ) Reward(mean:1.23 , max:2.96 , min:0.33 , std:0.64 ) | total_loss: 1.602   reward_loss: 0.041   policy_loss: 2.686   value_loss: 0.416   consistency_loss: -0.615  lr: 0.000100  batch_future_return: 0.746   batch_model_diff: 15654.297target_model_diff: 200.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:26:10,897][train][INFO][log.py>_log] ==> #36700      Episodes Collected: 8521       Transitions Collected: 232735     Batch Size: 256   | NewEpisode Model(mean:36595     ) Reward(mean:1.55 , max:3.29 , min:0.33 , std:0.68 ) | total_loss: 1.509   reward_loss: 0.037   policy_loss: 2.514   value_loss: 0.394   consistency_loss: -0.570  lr: 0.000100  batch_future_return: 0.726   batch_model_diff: 15133.594target_model_diff: 100.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:26:23,012][train][WARNING][train.py>_train] ==> #36750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:26:35,630][train][INFO][log.py>_log] ==> #36800      Episodes Collected: 8545       Transitions Collected: 233386     Batch Size: 256   | NewEpisode Model(mean:36696     ) Reward(mean:1.40 , max:2.30 , min:0.66 , std:0.43 ) | total_loss: 1.509   reward_loss: 0.036   policy_loss: 2.445   value_loss: 0.384   consistency_loss: -0.534  lr: 0.000100  batch_future_return: 0.768   batch_model_diff: 14493.359target_model_diff: 200.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:27:00,274][train][INFO][log.py>_log] ==> #36900      Episodes Collected: 8573       Transitions Collected: 234152     Batch Size: 256   | NewEpisode Model(mean:36791     ) Reward(mean:1.33 , max:2.30 , min:0.33 , std:0.47 ) | total_loss: 1.698   reward_loss: 0.048   policy_loss: 2.796   value_loss: 0.446   consistency_loss: -0.629  lr: 0.000100  batch_future_return: 0.762   batch_model_diff: 14955.469target_model_diff: 100.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:27:00,277][train][WARNING][train.py>_train] ==> #36900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:27:25,107][train][INFO][log.py>_log] ==> #37000      Episodes Collected: 8598       Transitions Collected: 234833     Batch Size: 256   | NewEpisode Model(mean:36894     ) Reward(mean:1.49 , max:2.63 , min:0.33 , std:0.64 ) | total_loss: 0.300   reward_loss: 0.005   policy_loss: 0.477   value_loss: 0.072   consistency_loss: -0.100  lr: 0.000100  batch_future_return: 0.732   batch_model_diff: 15080.469target_model_diff: 200.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:27:49,780][train][INFO][log.py>_log] ==> #37100      Episodes Collected: 8621       Transitions Collected: 235536     Batch Size: 256   | NewEpisode Model(mean:36991     ) Reward(mean:1.34 , max:3.29 , min:0.33 , std:0.61 ) | total_loss: 2.151   reward_loss: 0.057   policy_loss: 3.737   value_loss: 0.605   consistency_loss: -0.897  lr: 0.000100  batch_future_return: 0.724   batch_model_diff: 14589.062target_model_diff: 100.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:28:14,025][train][INFO][log.py>_log] ==> #37200      Episodes Collected: 8644       Transitions Collected: 236235     Batch Size: 256   | NewEpisode Model(mean:37087     ) Reward(mean:1.11 , max:2.30 , min:0.33 , std:0.52 ) | total_loss: 0.901   reward_loss: 0.024   policy_loss: 1.512   value_loss: 0.248   consistency_loss: -0.349  lr: 0.000100  batch_future_return: 0.769   batch_model_diff: 16062.500target_model_diff: 200.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:28:38,342][train][INFO][log.py>_log] ==> #37300      Episodes Collected: 8669       Transitions Collected: 236912     Batch Size: 256   | NewEpisode Model(mean:37190     ) Reward(mean:1.64 , max:3.62 , min:0.66 , std:0.71 ) | total_loss: 1.256   reward_loss: 0.023   policy_loss: 1.974   value_loss: 0.289   consistency_loss: -0.407  lr: 0.000100  batch_future_return: 0.755   batch_model_diff: 14971.484target_model_diff: 100.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:28:49,975][train][WARNING][train.py>_train] ==> #37350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:29:02,206][train][INFO][log.py>_log] ==> #37400      Episodes Collected: 8693       Transitions Collected: 237601     Batch Size: 256   | NewEpisode Model(mean:37293     ) Reward(mean:1.29 , max:3.29 , min:0.00 , std:0.74 ) | total_loss: 1.520   reward_loss: 0.038   policy_loss: 2.511   value_loss: 0.389   consistency_loss: -0.563  lr: 0.000100  batch_future_return: 0.686   batch_model_diff: 14403.125target_model_diff: 200.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:29:02,208][train][WARNING][train.py>_train] ==> #37400 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:29:26,513][train][INFO][log.py>_log] ==> #37500      Episodes Collected: 8718       Transitions Collected: 238277     Batch Size: 256   | NewEpisode Model(mean:37395     ) Reward(mean:1.35 , max:2.30 , min:0.66 , std:0.49 ) | total_loss: 1.347   reward_loss: 0.038   policy_loss: 2.340   value_loss: 0.392   consistency_loss: -0.564  lr: 0.000100  batch_future_return: 0.710   batch_model_diff: 15680.469target_model_diff: 100.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:29:51,843][train][INFO][log.py>_log] ==> #37600      Episodes Collected: 8743       Transitions Collected: 239024     Batch Size: 256   | NewEpisode Model(mean:37494     ) Reward(mean:1.55 , max:3.29 , min:0.66 , std:0.64 ) | total_loss: 0.534   reward_loss: 0.016   policy_loss: 0.876   value_loss: 0.144   consistency_loss: -0.197  lr: 0.000100  batch_future_return: 0.781   batch_model_diff: 15878.516target_model_diff: 200.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:30:17,066][train][INFO][log.py>_log] ==> #37700      Episodes Collected: 8773       Transitions Collected: 239761     Batch Size: 256   | NewEpisode Model(mean:37593     ) Reward(mean:1.40 , max:2.96 , min:0.33 , std:0.61 ) | total_loss: 2.123   reward_loss: 0.061   policy_loss: 3.727   value_loss: 0.616   consistency_loss: -0.910  lr: 0.000100  batch_future_return: 0.704   batch_model_diff: 15856.641target_model_diff: 100.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:30:41,365][train][INFO][log.py>_log] ==> #37800      Episodes Collected: 8799       Transitions Collected: 240438     Batch Size: 256   | NewEpisode Model(mean:37693     ) Reward(mean:1.48 , max:2.63 , min:0.66 , std:0.59 ) | total_loss: 0.970   reward_loss: 0.027   policy_loss: 1.621   value_loss: 0.260   consistency_loss: -0.372  lr: 0.000100  batch_future_return: 0.754   batch_model_diff: 15454.688target_model_diff: 200.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:31:05,832][train][INFO][log.py>_log] ==> #37900      Episodes Collected: 8825       Transitions Collected: 241135     Batch Size: 256   | NewEpisode Model(mean:37792     ) Reward(mean:1.57 , max:2.96 , min:0.66 , std:0.67 ) | total_loss: 1.718   reward_loss: 0.042   policy_loss: 2.885   value_loss: 0.447   consistency_loss: -0.660  lr: 0.000100  batch_future_return: 0.719   batch_model_diff: 15564.062target_model_diff: 100.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:31:18,438][train][WARNING][train.py>_train] ==> #37950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:31:31,115][train][INFO][log.py>_log] ==> #38000      Episodes Collected: 8853       Transitions Collected: 241856     Batch Size: 256   | NewEpisode Model(mean:37891     ) Reward(mean:1.41 , max:2.96 , min:0.33 , std:0.71 ) | total_loss: 1.074   reward_loss: 0.036   policy_loss: 1.794   value_loss: 0.281   consistency_loss: -0.413  lr: 0.000100  batch_future_return: 0.733   batch_model_diff: 14344.922target_model_diff: 200.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:31:55,849][train][INFO][log.py>_log] ==> #38100      Episodes Collected: 8878       Transitions Collected: 242630     Batch Size: 256   | NewEpisode Model(mean:37990     ) Reward(mean:1.17 , max:2.96 , min:0.33 , std:0.58 ) | total_loss: 2.102   reward_loss: 0.050   policy_loss: 3.632   value_loss: 0.594   consistency_loss: -0.864  lr: 0.000100  batch_future_return: 0.679   batch_model_diff: 16675.000target_model_diff: 100.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:32:21,096][train][INFO][log.py>_log] ==> #38200      Episodes Collected: 8901       Transitions Collected: 243325     Batch Size: 256   | NewEpisode Model(mean:38095     ) Reward(mean:1.32 , max:2.63 , min:0.33 , std:0.61 ) | total_loss: 0.544   reward_loss: 0.010   policy_loss: 0.851   value_loss: 0.127   consistency_loss: -0.174  lr: 0.000100  batch_future_return: 0.800   batch_model_diff: 15578.906target_model_diff: 200.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:32:21,100][train][WARNING][train.py>_train] ==> #38200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:32:33,270][train][WARNING][train.py>_train] ==> #38250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:32:45,985][train][INFO][log.py>_log] ==> #38300      Episodes Collected: 8926       Transitions Collected: 244039     Batch Size: 256   | NewEpisode Model(mean:38187     ) Reward(mean:1.42 , max:2.63 , min:0.33 , std:0.59 ) | total_loss: 3.259   reward_loss: 0.094   policy_loss: 5.623   value_loss: 0.895   consistency_loss: -1.341  lr: 0.000100  batch_future_return: 0.715   batch_model_diff: 16214.453target_model_diff: 100.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:32:57,833][train][WARNING][train.py>_train] ==> #38350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:33:10,000][train][INFO][log.py>_log] ==> #38400      Episodes Collected: 8952       Transitions Collected: 244734     Batch Size: 256   | NewEpisode Model(mean:38288     ) Reward(mean:1.19 , max:2.30 , min:0.00 , std:0.48 ) | total_loss: 2.077   reward_loss: 0.060   policy_loss: 3.599   value_loss: 0.588   consistency_loss: -0.865  lr: 0.000100  batch_future_return: 0.702   batch_model_diff: 15255.469target_model_diff: 200.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:33:21,842][train][WARNING][train.py>_train] ==> #38450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:33:34,077][train][INFO][log.py>_log] ==> #38500      Episodes Collected: 8976       Transitions Collected: 245399     Batch Size: 256   | NewEpisode Model(mean:38391     ) Reward(mean:1.53 , max:2.96 , min:0.33 , std:0.63 ) | total_loss: 0.957   reward_loss: 0.028   policy_loss: 1.639   value_loss: 0.271   consistency_loss: -0.389  lr: 0.000100  batch_future_return: 0.715   batch_model_diff: 15567.578target_model_diff: 100.000 Tp_perstep: 0.183   Tu_perstep: 0.049   
[2025-06-02 22:33:34,080][train][WARNING][train.py>_train] ==> #38500 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:33:46,242][train][WARNING][train.py>_train] ==> #38550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:33:58,663][train][INFO][log.py>_log] ==> #38600      Episodes Collected: 9000       Transitions Collected: 246094     Batch Size: 256   | NewEpisode Model(mean:38491     ) Reward(mean:1.21 , max:1.97 , min:0.33 , std:0.41 ) | total_loss: 2.332   reward_loss: 0.064   policy_loss: 4.095   value_loss: 0.654   consistency_loss: -0.995  lr: 0.000100  batch_future_return: 0.668   batch_model_diff: 15290.234target_model_diff: 200.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:33:58,665][train][WARNING][train.py>_train] ==> #38600 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:34:24,063][train][INFO][log.py>_log] ==> #38700      Episodes Collected: 9029       Transitions Collected: 246854     Batch Size: 256   | NewEpisode Model(mean:38589     ) Reward(mean:1.42 , max:2.63 , min:0.33 , std:0.58 ) | total_loss: 0.371   reward_loss: 0.010   policy_loss: 0.578   value_loss: 0.088   consistency_loss: -0.120  lr: 0.000100  batch_future_return: 0.848   batch_model_diff: 16519.922target_model_diff: 100.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:34:35,746][train][WARNING][train.py>_train] ==> #38750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:34:48,067][train][INFO][log.py>_log] ==> #38800      Episodes Collected: 9052       Transitions Collected: 247527     Batch Size: 256   | NewEpisode Model(mean:38695     ) Reward(mean:1.30 , max:2.63 , min:0.33 , std:0.47 ) | total_loss: 1.094   reward_loss: 0.025   policy_loss: 1.807   value_loss: 0.289   consistency_loss: -0.405  lr: 0.000100  batch_future_return: 0.754   batch_model_diff: 16033.203target_model_diff: 200.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:34:48,069][train][WARNING][train.py>_train] ==> #38800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:35:12,413][train][INFO][log.py>_log] ==> #38900      Episodes Collected: 9080       Transitions Collected: 248206     Batch Size: 256   | NewEpisode Model(mean:38791     ) Reward(mean:1.51 , max:3.95 , min:0.33 , std:0.68 ) | total_loss: 3.148   reward_loss: 0.067   policy_loss: 4.976   value_loss: 0.747   consistency_loss: -1.041  lr: 0.000100  batch_future_return: 0.786   batch_model_diff: 16567.578target_model_diff: 100.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:35:12,415][train][WARNING][train.py>_train] ==> #38900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:35:37,363][train][INFO][log.py>_log] ==> #39000      Episodes Collected: 9107       Transitions Collected: 248941     Batch Size: 256   | NewEpisode Model(mean:38888     ) Reward(mean:1.40 , max:2.63 , min:0.66 , std:0.43 ) | total_loss: 0.585   reward_loss: 0.014   policy_loss: 0.948   value_loss: 0.150   consistency_loss: -0.207  lr: 0.000100  batch_future_return: 0.711   batch_model_diff: 15760.938target_model_diff: 200.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:35:37,365][train][WARNING][train.py>_train] ==> #39000 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:36:02,139][train][INFO][log.py>_log] ==> #39100      Episodes Collected: 9130       Transitions Collected: 249643     Batch Size: 256   | NewEpisode Model(mean:38991     ) Reward(mean:1.54 , max:2.96 , min:0.66 , std:0.62 ) | total_loss: 1.910   reward_loss: 0.046   policy_loss: 3.112   value_loss: 0.481   consistency_loss: -0.684  lr: 0.000100  batch_future_return: 0.727   batch_model_diff: 16610.938target_model_diff: 100.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:36:14,326][train][WARNING][train.py>_train] ==> #39150 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:36:26,400][train][INFO][log.py>_log] ==> #39200      Episodes Collected: 9157       Transitions Collected: 250342     Batch Size: 256   | NewEpisode Model(mean:39091     ) Reward(mean:1.25 , max:2.30 , min:0.66 , std:0.46 ) | total_loss: 1.685   reward_loss: 0.044   policy_loss: 2.765   value_loss: 0.432   consistency_loss: -0.616  lr: 0.000100  batch_future_return: 0.721   batch_model_diff: 17851.562target_model_diff: 200.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:36:26,402][train][WARNING][train.py>_train] ==> #39200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:36:51,620][train][INFO][log.py>_log] ==> #39300      Episodes Collected: 9186       Transitions Collected: 251084     Batch Size: 256   | NewEpisode Model(mean:39190     ) Reward(mean:1.24 , max:2.96 , min:0.33 , std:0.67 ) | total_loss: 0.499   reward_loss: 0.013   policy_loss: 0.849   value_loss: 0.140   consistency_loss: -0.199  lr: 0.000100  batch_future_return: 0.749   batch_model_diff: 16261.719target_model_diff: 100.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:37:16,976][train][INFO][log.py>_log] ==> #39400      Episodes Collected: 9214       Transitions Collected: 251807     Batch Size: 256   | NewEpisode Model(mean:39294     ) Reward(mean:1.56 , max:2.63 , min:0.66 , std:0.54 ) | total_loss: 0.593   reward_loss: 0.013   policy_loss: 0.950   value_loss: 0.147   consistency_loss: -0.203  lr: 0.000100  batch_future_return: 0.827   batch_model_diff: 16934.766target_model_diff: 200.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:37:42,293][train][INFO][log.py>_log] ==> #39500      Episodes Collected: 9241       Transitions Collected: 252552     Batch Size: 256   | NewEpisode Model(mean:39390     ) Reward(mean:1.52 , max:3.62 , min:0.66 , std:0.62 ) | total_loss: 0.885   reward_loss: 0.022   policy_loss: 1.395   value_loss: 0.215   consistency_loss: -0.293  lr: 0.000100  batch_future_return: 0.790   batch_model_diff: 16718.750target_model_diff: 100.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:37:53,942][train][WARNING][train.py>_train] ==> #39550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:38:07,117][train][INFO][log.py>_log] ==> #39600      Episodes Collected: 9269       Transitions Collected: 253302     Batch Size: 256   | NewEpisode Model(mean:39490     ) Reward(mean:1.44 , max:3.29 , min:0.66 , std:0.62 ) | total_loss: 1.735   reward_loss: 0.050   policy_loss: 2.910   value_loss: 0.468   consistency_loss: -0.671  lr: 0.000100  batch_future_return: 0.737   batch_model_diff: 16392.578target_model_diff: 200.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:38:31,500][train][INFO][log.py>_log] ==> #39700      Episodes Collected: 9295       Transitions Collected: 253993     Batch Size: 256   | NewEpisode Model(mean:39595     ) Reward(mean:1.53 , max:3.29 , min:0.33 , std:0.69 ) | total_loss: 2.369   reward_loss: 0.055   policy_loss: 3.877   value_loss: 0.598   consistency_loss: -0.856  lr: 0.000100  batch_future_return: 0.719   batch_model_diff: 16484.766target_model_diff: 100.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:38:31,502][train][WARNING][train.py>_train] ==> #39700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:38:44,012][train][WARNING][train.py>_train] ==> #39750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:38:56,678][train][INFO][log.py>_log] ==> #39800      Episodes Collected: 9322       Transitions Collected: 254719     Batch Size: 256   | NewEpisode Model(mean:39694     ) Reward(mean:1.28 , max:2.63 , min:0.00 , std:0.52 ) | total_loss: 1.106   reward_loss: 0.025   policy_loss: 1.801   value_loss: 0.279   consistency_loss: -0.395  lr: 0.000100  batch_future_return: 0.727   batch_model_diff: 16309.375target_model_diff: 200.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:38:56,680][train][WARNING][train.py>_train] ==> #39800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:39:08,959][train][WARNING][train.py>_train] ==> #39850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:39:22,042][train][INFO][log.py>_log] ==> #39900      Episodes Collected: 9350       Transitions Collected: 255474     Batch Size: 256   | NewEpisode Model(mean:39792     ) Reward(mean:1.28 , max:2.63 , min:0.33 , std:0.54 ) | total_loss: 1.282   reward_loss: 0.023   policy_loss: 1.969   value_loss: 0.283   consistency_loss: -0.390  lr: 0.000100  batch_future_return: 0.722   batch_model_diff: 18124.609target_model_diff: 100.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:39:22,044][train][WARNING][train.py>_train] ==> #39900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:39:47,347][train][INFO][log.py>_log] ==> #40000      Episodes Collected: 9373       Transitions Collected: 256185     Batch Size: 256   | NewEpisode Model(mean:39892     ) Reward(mean:1.40 , max:2.63 , min:0.33 , std:0.51 ) | total_loss: 0.912   reward_loss: 0.018   policy_loss: 1.447   value_loss: 0.219   consistency_loss: -0.304  lr: 0.000100  batch_future_return: 0.711   batch_model_diff: 16506.641target_model_diff: 200.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:39:47,349][train][WARNING][train.py>_train] ==> #40000 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:40:12,566][train][INFO][log.py>_log] ==> #40100      Episodes Collected: 9398       Transitions Collected: 256915     Batch Size: 256   | NewEpisode Model(mean:39990     ) Reward(mean:1.39 , max:2.63 , min:0.33 , std:0.60 ) | total_loss: 1.565   reward_loss: 0.040   policy_loss: 2.584   value_loss: 0.396   consistency_loss: -0.579  lr: 0.000100  batch_future_return: 0.718   batch_model_diff: 16808.984target_model_diff: 100.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:40:24,657][train][WARNING][train.py>_train] ==> #40150 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:40:36,942][train][INFO][log.py>_log] ==> #40200      Episodes Collected: 9424       Transitions Collected: 257626     Batch Size: 256   | NewEpisode Model(mean:40088     ) Reward(mean:1.33 , max:2.30 , min:0.33 , std:0.52 ) | total_loss: 0.655   reward_loss: 0.020   policy_loss: 1.024   value_loss: 0.157   consistency_loss: -0.214  lr: 0.000100  batch_future_return: 0.813   batch_model_diff: 18414.062target_model_diff: 200.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:41:01,724][train][INFO][log.py>_log] ==> #40300      Episodes Collected: 9452       Transitions Collected: 258359     Batch Size: 256   | NewEpisode Model(mean:40189     ) Reward(mean:1.42 , max:2.96 , min:0.33 , std:0.66 ) | total_loss: 1.344   reward_loss: 0.039   policy_loss: 2.198   value_loss: 0.345   consistency_loss: -0.490  lr: 0.000100  batch_future_return: 0.810   batch_model_diff: 16173.047target_model_diff: 100.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:41:14,337][train][WARNING][train.py>_train] ==> #40350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:41:27,149][train][INFO][log.py>_log] ==> #40400      Episodes Collected: 9477       Transitions Collected: 259097     Batch Size: 256   | NewEpisode Model(mean:40293     ) Reward(mean:1.45 , max:2.30 , min:0.33 , std:0.50 ) | total_loss: 0.727   reward_loss: 0.019   policy_loss: 1.224   value_loss: 0.195   consistency_loss: -0.282  lr: 0.000100  batch_future_return: 0.697   batch_model_diff: 16497.266target_model_diff: 200.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:41:51,236][train][INFO][log.py>_log] ==> #40500      Episodes Collected: 9500       Transitions Collected: 259652     Batch Size: 256   | NewEpisode Model(mean:40390     ) Reward(mean:1.52 , max:2.96 , min:0.33 , std:0.60 ) | total_loss: 2.000   reward_loss: 0.062   policy_loss: 3.236   value_loss: 0.506   consistency_loss: -0.712  lr: 0.000100  batch_future_return: 0.782   batch_model_diff: 16344.531target_model_diff: 100.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:42:15,245][train][INFO][log.py>_log] ==> #40600      Episodes Collected: 9512       Transitions Collected: 259937     Batch Size: 256   | NewEpisode Model(mean:40485     ) Reward(mean:1.12 , max:1.97 , min:0.00 , std:0.49 ) | total_loss: 1.993   reward_loss: 0.046   policy_loss: 3.260   value_loss: 0.521   consistency_loss: -0.722  lr: 0.000100  batch_future_return: 0.731   batch_model_diff: 17835.547target_model_diff: 200.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:42:15,247][train][WARNING][train.py>_train] ==> #40600 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:42:27,304][train][WARNING][train.py>_train] ==> #40650 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:42:39,526][train][INFO][log.py>_log] ==> #40700      Episodes Collected: 9538       Transitions Collected: 260632     Batch Size: 256   | NewEpisode Model(mean:40593     ) Reward(mean:1.66 , max:2.63 , min:0.99 , std:0.43 ) | total_loss: 1.924   reward_loss: 0.050   policy_loss: 3.215   value_loss: 0.505   consistency_loss: -0.734  lr: 0.000100  batch_future_return: 0.740   batch_model_diff: 16680.859target_model_diff: 100.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:42:51,758][train][WARNING][train.py>_train] ==> #40750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:43:04,479][train][INFO][log.py>_log] ==> #40800      Episodes Collected: 9564       Transitions Collected: 261351     Batch Size: 256   | NewEpisode Model(mean:40695     ) Reward(mean:1.57 , max:2.96 , min:0.00 , std:0.66 ) | total_loss: 0.593   reward_loss: 0.015   policy_loss: 1.035   value_loss: 0.178   consistency_loss: -0.251  lr: 0.000100  batch_future_return: 0.733   batch_model_diff: 17324.609target_model_diff: 200.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:43:29,322][train][INFO][log.py>_log] ==> #40900      Episodes Collected: 9590       Transitions Collected: 262047     Batch Size: 256   | NewEpisode Model(mean:40796     ) Reward(mean:1.49 , max:2.63 , min:0.00 , std:0.62 ) | total_loss: 2.315   reward_loss: 0.057   policy_loss: 3.723   value_loss: 0.572   consistency_loss: -0.804  lr: 0.000100  batch_future_return: 0.736   batch_model_diff: 17952.734target_model_diff: 100.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:43:29,326][train][WARNING][train.py>_train] ==> #40900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:43:42,048][train][WARNING][train.py>_train] ==> #40950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:43:54,667][train][INFO][log.py>_log] ==> #41000      Episodes Collected: 9617       Transitions Collected: 262798     Batch Size: 256   | NewEpisode Model(mean:40893     ) Reward(mean:1.56 , max:3.95 , min:0.66 , std:0.64 ) | total_loss: 0.775   reward_loss: 0.016   policy_loss: 1.313   value_loss: 0.213   consistency_loss: -0.304  lr: 0.000100  batch_future_return: 0.702   batch_model_diff: 15975.391target_model_diff: 200.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:44:06,926][train][WARNING][train.py>_train] ==> #41050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:44:19,514][train][INFO][log.py>_log] ==> #41100      Episodes Collected: 9641       Transitions Collected: 263505     Batch Size: 256   | NewEpisode Model(mean:40991     ) Reward(mean:1.40 , max:2.63 , min:0.33 , std:0.58 ) | total_loss: 1.182   reward_loss: 0.035   policy_loss: 2.141   value_loss: 0.363   consistency_loss: -0.542  lr: 0.000100  batch_future_return: 0.709   batch_model_diff: 16946.094target_model_diff: 100.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:44:19,516][train][WARNING][train.py>_train] ==> #41100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:44:31,530][train][WARNING][train.py>_train] ==> #41150 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:44:44,685][train][INFO][log.py>_log] ==> #41200      Episodes Collected: 9669       Transitions Collected: 264269     Batch Size: 256   | NewEpisode Model(mean:41091     ) Reward(mean:1.49 , max:2.30 , min:0.66 , std:0.46 ) | total_loss: 1.599   reward_loss: 0.051   policy_loss: 2.666   value_loss: 0.429   consistency_loss: -0.612  lr: 0.000100  batch_future_return: 0.786   batch_model_diff: 17256.250target_model_diff: 200.000 Tp_perstep: 0.184   Tu_perstep: 0.049   
[2025-06-02 22:44:44,687][train][WARNING][train.py>_train] ==> #41200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:45:09,407][train][INFO][log.py>_log] ==> #41300      Episodes Collected: 9694       Transitions Collected: 264943     Batch Size: 256   | NewEpisode Model(mean:41192     ) Reward(mean:1.41 , max:2.63 , min:0.33 , std:0.51 ) | total_loss: 1.327   reward_loss: 0.036   policy_loss: 2.267   value_loss: 0.365   consistency_loss: -0.533  lr: 0.000100  batch_future_return: 0.729   batch_model_diff: 17279.297target_model_diff: 100.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:45:22,029][train][WARNING][train.py>_train] ==> #41350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:45:34,251][train][INFO][log.py>_log] ==> #41400      Episodes Collected: 9720       Transitions Collected: 265658     Batch Size: 256   | NewEpisode Model(mean:41291     ) Reward(mean:1.58 , max:4.27 , min:0.33 , std:0.94 ) | total_loss: 1.808   reward_loss: 0.037   policy_loss: 3.009   value_loss: 0.478   consistency_loss: -0.679  lr: 0.000100  batch_future_return: 0.697   batch_model_diff: 16904.297target_model_diff: 200.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:45:46,450][train][WARNING][train.py>_train] ==> #41450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:45:59,102][train][INFO][log.py>_log] ==> #41500      Episodes Collected: 9748       Transitions Collected: 266413     Batch Size: 256   | NewEpisode Model(mean:41391     ) Reward(mean:1.54 , max:2.96 , min:0.66 , std:0.60 ) | total_loss: 0.653   reward_loss: 0.015   policy_loss: 1.069   value_loss: 0.167   consistency_loss: -0.237  lr: 0.000100  batch_future_return: 0.760   batch_model_diff: 17469.531target_model_diff: 100.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:46:23,526][train][INFO][log.py>_log] ==> #41600      Episodes Collected: 9771       Transitions Collected: 267044     Batch Size: 256   | NewEpisode Model(mean:41491     ) Reward(mean:1.49 , max:2.63 , min:0.33 , std:0.58 ) | total_loss: 1.603   reward_loss: 0.043   policy_loss: 2.715   value_loss: 0.437   consistency_loss: -0.632  lr: 0.000100  batch_future_return: 0.744   batch_model_diff: 16942.578target_model_diff: 200.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:46:35,632][train][WARNING][train.py>_train] ==> #41650 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:46:48,333][train][INFO][log.py>_log] ==> #41700      Episodes Collected: 9796       Transitions Collected: 267819     Batch Size: 256   | NewEpisode Model(mean:41589     ) Reward(mean:1.45 , max:2.30 , min:0.66 , std:0.54 ) | total_loss: 0.147   reward_loss: 0.004   policy_loss: 0.271   value_loss: 0.048   consistency_loss: -0.070  lr: 0.000100  batch_future_return: 0.728   batch_model_diff: 17982.031target_model_diff: 100.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:47:00,111][train][WARNING][train.py>_train] ==> #41750 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:47:12,782][train][INFO][log.py>_log] ==> #41800      Episodes Collected: 9820       Transitions Collected: 268472     Batch Size: 256   | NewEpisode Model(mean:41692     ) Reward(mean:1.40 , max:2.63 , min:0.66 , std:0.46 ) | total_loss: 1.022   reward_loss: 0.033   policy_loss: 1.769   value_loss: 0.293   consistency_loss: -0.427  lr: 0.000100  batch_future_return: 0.704   batch_model_diff: 16479.297target_model_diff: 200.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:47:12,785][train][WARNING][train.py>_train] ==> #41800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:47:37,739][train][INFO][log.py>_log] ==> #41900      Episodes Collected: 9846       Transitions Collected: 269214     Batch Size: 256   | NewEpisode Model(mean:41788     ) Reward(mean:1.37 , max:2.63 , min:0.33 , std:0.63 ) | total_loss: 0.790   reward_loss: 0.017   policy_loss: 1.261   value_loss: 0.194   consistency_loss: -0.269  lr: 0.000100  batch_future_return: 0.768   batch_model_diff: 16255.859target_model_diff: 100.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:47:49,862][train][WARNING][train.py>_train] ==> #41950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:48:02,621][train][INFO][log.py>_log] ==> #42000      Episodes Collected: 9871       Transitions Collected: 269892     Batch Size: 256   | NewEpisode Model(mean:41892     ) Reward(mean:1.35 , max:2.96 , min:0.33 , std:0.65 ) | total_loss: 1.143   reward_loss: 0.026   policy_loss: 1.827   value_loss: 0.282   consistency_loss: -0.390  lr: 0.000100  batch_future_return: 0.780   batch_model_diff: 16734.766target_model_diff: 200.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:48:27,909][train][INFO][log.py>_log] ==> #42100      Episodes Collected: 9898       Transitions Collected: 270682     Batch Size: 256   | NewEpisode Model(mean:41989     ) Reward(mean:1.47 , max:3.62 , min:0.33 , std:0.75 ) | total_loss: 1.922   reward_loss: 0.045   policy_loss: 3.063   value_loss: 0.461   consistency_loss: -0.651  lr: 0.000100  batch_future_return: 0.790   batch_model_diff: 16139.453target_model_diff: 100.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:48:39,596][train][WARNING][train.py>_train] ==> #42150 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:48:52,613][train][INFO][log.py>_log] ==> #42200      Episodes Collected: 9925       Transitions Collected: 271388     Batch Size: 256   | NewEpisode Model(mean:42095     ) Reward(mean:1.64 , max:2.96 , min:0.66 , std:0.58 ) | total_loss: 2.278   reward_loss: 0.065   policy_loss: 3.654   value_loss: 0.563   consistency_loss: -0.791  lr: 0.000100  batch_future_return: 0.840   batch_model_diff: 17425.391target_model_diff: 200.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:48:52,616][train][WARNING][train.py>_train] ==> #42200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:49:05,772][train][WARNING][train.py>_train] ==> #42250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:49:18,823][train][INFO][log.py>_log] ==> #42300      Episodes Collected: 9955       Transitions Collected: 272177     Batch Size: 256   | NewEpisode Model(mean:42190     ) Reward(mean:1.46 , max:2.63 , min:0.66 , std:0.60 ) | total_loss: 1.606   reward_loss: 0.039   policy_loss: 2.597   value_loss: 0.409   consistency_loss: -0.566  lr: 0.000100  batch_future_return: 0.720   batch_model_diff: 18506.250target_model_diff: 100.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:49:43,078][train][INFO][log.py>_log] ==> #42400      Episodes Collected: 9980       Transitions Collected: 272853     Batch Size: 256   | NewEpisode Model(mean:42294     ) Reward(mean:1.39 , max:2.30 , min:0.33 , std:0.45 ) | total_loss: 1.838   reward_loss: 0.039   policy_loss: 2.979   value_loss: 0.458   consistency_loss: -0.647  lr: 0.000100  batch_future_return: 0.777   batch_model_diff: 18348.438target_model_diff: 200.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:49:43,088][train][WARNING][train.py>_train] ==> #42400 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:50:08,888][train][INFO][log.py>_log] ==> #42500      Episodes Collected: 10007      Transitions Collected: 273625     Batch Size: 256   | NewEpisode Model(mean:42391     ) Reward(mean:1.35 , max:1.97 , min:0.33 , std:0.46 ) | total_loss: 1.824   reward_loss: 0.052   policy_loss: 3.187   value_loss: 0.526   consistency_loss: -0.773  lr: 0.000100  batch_future_return: 0.701   batch_model_diff: 16553.906target_model_diff: 100.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:50:34,137][train][INFO][log.py>_log] ==> #42600      Episodes Collected: 10033      Transitions Collected: 274356     Batch Size: 256   | NewEpisode Model(mean:42491     ) Reward(mean:1.29 , max:2.96 , min:0.66 , std:0.55 ) | total_loss: 0.917   reward_loss: 0.023   policy_loss: 1.499   value_loss: 0.237   consistency_loss: -0.332  lr: 0.000100  batch_future_return: 0.726   batch_model_diff: 17761.719target_model_diff: 200.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:50:59,254][train][INFO][log.py>_log] ==> #42700      Episodes Collected: 10060      Transitions Collected: 275089     Batch Size: 256   | NewEpisode Model(mean:42594     ) Reward(mean:1.36 , max:2.30 , min:0.66 , std:0.44 ) | total_loss: 1.155   reward_loss: 0.025   policy_loss: 1.802   value_loss: 0.264   consistency_loss: -0.369  lr: 0.000100  batch_future_return: 0.783   batch_model_diff: 18583.984target_model_diff: 100.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:51:24,182][train][INFO][log.py>_log] ==> #42800      Episodes Collected: 10088      Transitions Collected: 275802     Batch Size: 256   | NewEpisode Model(mean:42691     ) Reward(mean:1.30 , max:2.63 , min:0.33 , std:0.49 ) | total_loss: 0.993   reward_loss: 0.022   policy_loss: 1.627   value_loss: 0.256   consistency_loss: -0.360  lr: 0.000100  batch_future_return: 0.720   batch_model_diff: 19318.359target_model_diff: 200.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:51:24,186][train][WARNING][train.py>_train] ==> #42800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:51:50,056][train][INFO][log.py>_log] ==> #42900      Episodes Collected: 10114      Transitions Collected: 276578     Batch Size: 256   | NewEpisode Model(mean:42794     ) Reward(mean:1.48 , max:2.63 , min:0.33 , std:0.59 ) | total_loss: 2.884   reward_loss: 0.096   policy_loss: 4.968   value_loss: 0.784   consistency_loss: -1.188  lr: 0.000100  batch_future_return: 0.754   batch_model_diff: 17350.391target_model_diff: 100.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:51:50,059][train][WARNING][train.py>_train] ==> #42900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:52:03,169][train][WARNING][train.py>_train] ==> #42950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:52:15,790][train][INFO][log.py>_log] ==> #43000      Episodes Collected: 10141      Transitions Collected: 277360     Batch Size: 256   | NewEpisode Model(mean:42891     ) Reward(mean:1.60 , max:4.27 , min:0.66 , std:0.73 ) | total_loss: 0.836   reward_loss: 0.019   policy_loss: 1.439   value_loss: 0.239   consistency_loss: -0.341  lr: 0.000100  batch_future_return: 0.764   batch_model_diff: 18868.750target_model_diff: 200.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:52:41,037][train][INFO][log.py>_log] ==> #43100      Episodes Collected: 10171      Transitions Collected: 278097     Batch Size: 256   | NewEpisode Model(mean:42993     ) Reward(mean:1.64 , max:3.95 , min:0.66 , std:0.69 ) | total_loss: 0.357   reward_loss: 0.012   policy_loss: 0.628   value_loss: 0.108   consistency_loss: -0.155  lr: 0.000100  batch_future_return: 0.679   batch_model_diff: 18582.031target_model_diff: 100.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:53:06,656][train][INFO][log.py>_log] ==> #43200      Episodes Collected: 10199      Transitions Collected: 278884     Batch Size: 256   | NewEpisode Model(mean:43091     ) Reward(mean:1.55 , max:3.62 , min:0.33 , std:0.77 ) | total_loss: 0.972   reward_loss: 0.023   policy_loss: 1.616   value_loss: 0.255   consistency_loss: -0.366  lr: 0.000100  batch_future_return: 0.706   batch_model_diff: 18107.031target_model_diff: 200.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:53:32,042][train][INFO][log.py>_log] ==> #43300      Episodes Collected: 10225      Transitions Collected: 279594     Batch Size: 256   | NewEpisode Model(mean:43194     ) Reward(mean:1.38 , max:2.63 , min:0.66 , std:0.49 ) | total_loss: 1.683   reward_loss: 0.039   policy_loss: 2.605   value_loss: 0.398   consistency_loss: -0.530  lr: 0.000100  batch_future_return: 0.794   batch_model_diff: 16500.000target_model_diff: 100.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:53:32,045][train][WARNING][train.py>_train] ==> #43300 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:53:57,736][train][INFO][log.py>_log] ==> #43400      Episodes Collected: 10252      Transitions Collected: 280377     Batch Size: 256   | NewEpisode Model(mean:43291     ) Reward(mean:1.35 , max:3.29 , min:0.33 , std:0.70 ) | total_loss: 1.854   reward_loss: 0.055   policy_loss: 3.126   value_loss: 0.497   consistency_loss: -0.726  lr: 0.000100  batch_future_return: 0.731   batch_model_diff: 18302.344target_model_diff: 200.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:54:10,486][train][WARNING][train.py>_train] ==> #43450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:54:23,128][train][INFO][log.py>_log] ==> #43500      Episodes Collected: 10279      Transitions Collected: 281096     Batch Size: 256   | NewEpisode Model(mean:43396     ) Reward(mean:1.39 , max:2.63 , min:0.33 , std:0.61 ) | total_loss: 0.525   reward_loss: 0.014   policy_loss: 0.909   value_loss: 0.150   consistency_loss: -0.218  lr: 0.000100  batch_future_return: 0.696   batch_model_diff: 18192.188target_model_diff: 100.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:54:47,998][train][INFO][log.py>_log] ==> #43600      Episodes Collected: 10301      Transitions Collected: 281829     Batch Size: 256   | NewEpisode Model(mean:43493     ) Reward(mean:1.18 , max:2.30 , min:0.33 , std:0.45 ) | total_loss: 2.534   reward_loss: 0.053   policy_loss: 3.867   value_loss: 0.564   consistency_loss: -0.764  lr: 0.000100  batch_future_return: 0.785   batch_model_diff: 17305.859target_model_diff: 200.000 Tp_perstep: 0.185   Tu_perstep: 0.049   
[2025-06-02 22:55:13,251][train][INFO][log.py>_log] ==> #43700      Episodes Collected: 10331      Transitions Collected: 282584     Batch Size: 256   | NewEpisode Model(mean:43593     ) Reward(mean:1.53 , max:2.63 , min:0.66 , std:0.50 ) | total_loss: 1.451   reward_loss: 0.030   policy_loss: 2.276   value_loss: 0.348   consistency_loss: -0.471  lr: 0.000100  batch_future_return: 0.778   batch_model_diff: 16813.672target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 22:55:37,027][train][INFO][log.py>_log] ==> #43800      Episodes Collected: 10353      Transitions Collected: 283232     Batch Size: 256   | NewEpisode Model(mean:43695     ) Reward(mean:1.36 , max:2.30 , min:0.33 , std:0.54 ) | total_loss: 0.779   reward_loss: 0.020   policy_loss: 1.296   value_loss: 0.203   consistency_loss: -0.294  lr: 0.000100  batch_future_return: 0.758   batch_model_diff: 18578.125target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 22:55:37,030][train][WARNING][train.py>_train] ==> #43800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:56:02,872][train][INFO][log.py>_log] ==> #43900      Episodes Collected: 10382      Transitions Collected: 283976     Batch Size: 256   | NewEpisode Model(mean:43792     ) Reward(mean:1.53 , max:2.96 , min:0.66 , std:0.57 ) | total_loss: 0.917   reward_loss: 0.022   policy_loss: 1.524   value_loss: 0.248   consistency_loss: -0.345  lr: 0.000100  batch_future_return: 0.804   batch_model_diff: 18254.688target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 22:56:14,992][train][WARNING][train.py>_train] ==> #43950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:56:27,186][train][INFO][log.py>_log] ==> #44000      Episodes Collected: 10410      Transitions Collected: 284695     Batch Size: 256   | NewEpisode Model(mean:43889     ) Reward(mean:1.50 , max:2.30 , min:0.33 , std:0.47 ) | total_loss: 1.275   reward_loss: 0.031   policy_loss: 2.257   value_loss: 0.368   consistency_loss: -0.553  lr: 0.000100  batch_future_return: 0.731   batch_model_diff: 18189.453target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 22:56:27,189][train][WARNING][train.py>_train] ==> #44000 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:56:52,340][train][INFO][log.py>_log] ==> #44100      Episodes Collected: 10436      Transitions Collected: 285410     Batch Size: 256   | NewEpisode Model(mean:43991     ) Reward(mean:1.37 , max:2.30 , min:0.33 , std:0.59 ) | total_loss: 1.580   reward_loss: 0.043   policy_loss: 2.668   value_loss: 0.436   consistency_loss: -0.620  lr: 0.000100  batch_future_return: 0.740   batch_model_diff: 19158.203target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 22:57:04,556][train][WARNING][train.py>_train] ==> #44150 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:57:17,146][train][INFO][log.py>_log] ==> #44200      Episodes Collected: 10458      Transitions Collected: 286087     Batch Size: 256   | NewEpisode Model(mean:44091     ) Reward(mean:1.30 , max:2.63 , min:0.00 , std:0.58 ) | total_loss: 0.894   reward_loss: 0.022   policy_loss: 1.509   value_loss: 0.240   consistency_loss: -0.349  lr: 0.000100  batch_future_return: 0.719   batch_model_diff: 18341.406target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 22:57:29,343][train][WARNING][train.py>_train] ==> #44250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:57:42,014][train][INFO][log.py>_log] ==> #44300      Episodes Collected: 10483      Transitions Collected: 286799     Batch Size: 256   | NewEpisode Model(mean:44186     ) Reward(mean:1.21 , max:2.63 , min:0.66 , std:0.45 ) | total_loss: 0.926   reward_loss: 0.021   policy_loss: 1.546   value_loss: 0.252   consistency_loss: -0.352  lr: 0.000100  batch_future_return: 0.729   batch_model_diff: 18721.094target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 22:58:06,576][train][INFO][log.py>_log] ==> #44400      Episodes Collected: 10506      Transitions Collected: 287512     Batch Size: 256   | NewEpisode Model(mean:44286     ) Reward(mean:1.43 , max:2.30 , min:0.66 , std:0.53 ) | total_loss: 0.702   reward_loss: 0.017   policy_loss: 1.149   value_loss: 0.186   consistency_loss: -0.255  lr: 0.000100  batch_future_return: 0.769   batch_model_diff: 18596.094target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 22:58:31,440][train][INFO][log.py>_log] ==> #44500      Episodes Collected: 10533      Transitions Collected: 288268     Batch Size: 256   | NewEpisode Model(mean:44390     ) Reward(mean:1.45 , max:2.63 , min:0.66 , std:0.52 ) | total_loss: 0.614   reward_loss: 0.016   policy_loss: 0.995   value_loss: 0.161   consistency_loss: -0.219  lr: 0.000100  batch_future_return: 0.746   batch_model_diff: 18684.375target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 22:58:43,666][train][WARNING][train.py>_train] ==> #44550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:58:56,344][train][INFO][log.py>_log] ==> #44600      Episodes Collected: 10560      Transitions Collected: 288995     Batch Size: 256   | NewEpisode Model(mean:44493     ) Reward(mean:1.45 , max:3.29 , min:0.66 , std:0.56 ) | total_loss: 0.705   reward_loss: 0.013   policy_loss: 1.091   value_loss: 0.161   consistency_loss: -0.220  lr: 0.000100  batch_future_return: 0.833   batch_model_diff: 17555.469target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 22:59:21,095][train][INFO][log.py>_log] ==> #44700      Episodes Collected: 10584      Transitions Collected: 289673     Batch Size: 256   | NewEpisode Model(mean:44592     ) Reward(mean:1.34 , max:2.30 , min:0.66 , std:0.49 ) | total_loss: 1.721   reward_loss: 0.039   policy_loss: 2.789   value_loss: 0.442   consistency_loss: -0.609  lr: 0.000100  batch_future_return: 0.754   batch_model_diff: 18245.703target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 22:59:45,943][train][INFO][log.py>_log] ==> #44800      Episodes Collected: 10607      Transitions Collected: 290379     Batch Size: 256   | NewEpisode Model(mean:44689     ) Reward(mean:1.54 , max:2.63 , min:0.33 , std:0.50 ) | total_loss: 1.648   reward_loss: 0.042   policy_loss: 2.652   value_loss: 0.402   consistency_loss: -0.573  lr: 0.000100  batch_future_return: 0.753   batch_model_diff: 19562.500target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 22:59:45,946][train][WARNING][train.py>_train] ==> #44800 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 22:59:58,632][train][WARNING][train.py>_train] ==> #44850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:00:11,281][train][INFO][log.py>_log] ==> #44900      Episodes Collected: 10635      Transitions Collected: 291168     Batch Size: 256   | NewEpisode Model(mean:44790     ) Reward(mean:1.32 , max:2.30 , min:0.66 , std:0.45 ) | total_loss: 1.069   reward_loss: 0.024   policy_loss: 1.694   value_loss: 0.254   consistency_loss: -0.356  lr: 0.000100  batch_future_return: 0.745   batch_model_diff: 19120.312target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:00:36,635][train][INFO][log.py>_log] ==> #45000      Episodes Collected: 10664      Transitions Collected: 291929     Batch Size: 256   | NewEpisode Model(mean:44895     ) Reward(mean:1.52 , max:2.30 , min:0.33 , std:0.54 ) | total_loss: 2.180   reward_loss: 0.061   policy_loss: 3.576   value_loss: 0.555   consistency_loss: -0.798  lr: 0.000100  batch_future_return: 0.751   batch_model_diff: 17928.516target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:00:48,759][train][WARNING][train.py>_train] ==> #45050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:01:01,385][train][INFO][log.py>_log] ==> #45100      Episodes Collected: 10691      Transitions Collected: 292597     Batch Size: 256   | NewEpisode Model(mean:44994     ) Reward(mean:1.32 , max:2.63 , min:0.33 , std:0.53 ) | total_loss: 0.854   reward_loss: 0.016   policy_loss: 1.315   value_loss: 0.192   consistency_loss: -0.262  lr: 0.000100  batch_future_return: 0.737   batch_model_diff: 19108.984target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:01:25,679][train][INFO][log.py>_log] ==> #45200      Episodes Collected: 10715      Transitions Collected: 293286     Batch Size: 256   | NewEpisode Model(mean:45091     ) Reward(mean:1.47 , max:2.30 , min:0.66 , std:0.42 ) | total_loss: 0.796   reward_loss: 0.024   policy_loss: 1.373   value_loss: 0.224   consistency_loss: -0.329  lr: 0.000100  batch_future_return: 0.762   batch_model_diff: 19033.203target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:01:37,861][train][WARNING][train.py>_train] ==> #45250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:01:50,604][train][INFO][log.py>_log] ==> #45300      Episodes Collected: 10741      Transitions Collected: 294047     Batch Size: 256   | NewEpisode Model(mean:45189     ) Reward(mean:1.38 , max:2.63 , min:0.66 , std:0.57 ) | total_loss: 1.233   reward_loss: 0.033   policy_loss: 2.110   value_loss: 0.340   consistency_loss: -0.498  lr: 0.000100  batch_future_return: 0.682   batch_model_diff: 19437.891target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:02:03,210][train][WARNING][train.py>_train] ==> #45350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:02:16,769][train][INFO][log.py>_log] ==> #45400      Episodes Collected: 10769      Transitions Collected: 294810     Batch Size: 256   | NewEpisode Model(mean:45295     ) Reward(mean:1.37 , max:2.30 , min:0.33 , std:0.51 ) | total_loss: 1.230   reward_loss: 0.021   policy_loss: 1.979   value_loss: 0.305   consistency_loss: -0.423  lr: 0.000100  batch_future_return: 0.727   batch_model_diff: 19799.609target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:02:40,959][train][INFO][log.py>_log] ==> #45500      Episodes Collected: 10792      Transitions Collected: 295452     Batch Size: 256   | NewEpisode Model(mean:45391     ) Reward(mean:1.59 , max:2.63 , min:0.66 , std:0.47 ) | total_loss: 0.647   reward_loss: 0.012   policy_loss: 1.031   value_loss: 0.161   consistency_loss: -0.218  lr: 0.000100  batch_future_return: 0.726   batch_model_diff: 19792.969target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:02:40,963][train][WARNING][train.py>_train] ==> #45500 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:03:05,907][train][INFO][log.py>_log] ==> #45600      Episodes Collected: 10817      Transitions Collected: 296207     Batch Size: 256   | NewEpisode Model(mean:45488     ) Reward(mean:1.51 , max:3.29 , min:0.00 , std:0.79 ) | total_loss: 1.386   reward_loss: 0.037   policy_loss: 2.366   value_loss: 0.385   consistency_loss: -0.556  lr: 0.000100  batch_future_return: 0.768   batch_model_diff: 18794.531target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:03:30,476][train][INFO][log.py>_log] ==> #45700      Episodes Collected: 10840      Transitions Collected: 296929     Batch Size: 256   | NewEpisode Model(mean:45593     ) Reward(mean:1.30 , max:1.97 , min:0.66 , std:0.49 ) | total_loss: 1.221   reward_loss: 0.037   policy_loss: 2.001   value_loss: 0.315   consistency_loss: -0.448  lr: 0.000100  batch_future_return: 0.795   batch_model_diff: 17870.703target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:03:56,282][train][INFO][log.py>_log] ==> #45800      Episodes Collected: 10867      Transitions Collected: 297665     Batch Size: 256   | NewEpisode Model(mean:45692     ) Reward(mean:1.41 , max:3.95 , min:0.33 , std:0.74 ) | total_loss: 0.908   reward_loss: 0.020   policy_loss: 1.432   value_loss: 0.225   consistency_loss: -0.300  lr: 0.000100  batch_future_return: 0.809   batch_model_diff: 19516.797target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:04:08,476][train][WARNING][train.py>_train] ==> #45850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:04:21,192][train][INFO][log.py>_log] ==> #45900      Episodes Collected: 10894      Transitions Collected: 298389     Batch Size: 256   | NewEpisode Model(mean:45790     ) Reward(mean:1.27 , max:2.63 , min:0.33 , std:0.58 ) | total_loss: 1.067   reward_loss: 0.036   policy_loss: 1.891   value_loss: 0.320   consistency_loss: -0.470  lr: 0.000100  batch_future_return: 0.726   batch_model_diff: 19161.328target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:04:33,803][train][WARNING][train.py>_train] ==> #45950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:04:46,544][train][INFO][log.py>_log] ==> #46000      Episodes Collected: 10924      Transitions Collected: 299152     Batch Size: 256   | NewEpisode Model(mean:45892     ) Reward(mean:1.56 , max:2.63 , min:0.66 , std:0.44 ) | total_loss: 1.224   reward_loss: 0.029   policy_loss: 2.106   value_loss: 0.361   consistency_loss: -0.500  lr: 0.000100  batch_future_return: 0.727   batch_model_diff: 18659.375target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:05:11,444][train][INFO][log.py>_log] ==> #46100      Episodes Collected: 10949      Transitions Collected: 299819     Batch Size: 256   | NewEpisode Model(mean:45994     ) Reward(mean:1.42 , max:2.63 , min:0.33 , std:0.57 ) | total_loss: 1.071   reward_loss: 0.031   policy_loss: 1.876   value_loss: 0.321   consistency_loss: -0.458  lr: 0.000100  batch_future_return: 0.742   batch_model_diff: 19870.312target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:05:36,471][train][INFO][log.py>_log] ==> #46200      Episodes Collected: 10974      Transitions Collected: 300601     Batch Size: 256   | NewEpisode Model(mean:46092     ) Reward(mean:1.54 , max:2.63 , min:0.99 , std:0.41 ) | total_loss: 0.489   reward_loss: 0.010   policy_loss: 0.783   value_loss: 0.117   consistency_loss: -0.166  lr: 0.000100  batch_future_return: 0.736   batch_model_diff: 20047.656target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:06:01,424][train][INFO][log.py>_log] ==> #46300      Episodes Collected: 10998      Transitions Collected: 301321     Batch Size: 256   | NewEpisode Model(mean:46192     ) Reward(mean:1.27 , max:2.30 , min:0.00 , std:0.51 ) | total_loss: 1.768   reward_loss: 0.035   policy_loss: 2.732   value_loss: 0.398   consistency_loss: -0.549  lr: 0.000100  batch_future_return: 0.742   batch_model_diff: 18885.938target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:06:13,654][train][WARNING][train.py>_train] ==> #46350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:06:26,317][train][INFO][log.py>_log] ==> #46400      Episodes Collected: 11026      Transitions Collected: 302022     Batch Size: 256   | NewEpisode Model(mean:46293     ) Reward(mean:1.39 , max:2.30 , min:0.33 , std:0.45 ) | total_loss: 1.227   reward_loss: 0.031   policy_loss: 1.987   value_loss: 0.302   consistency_loss: -0.433  lr: 0.000100  batch_future_return: 0.737   batch_model_diff: 19407.422target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:06:51,166][train][INFO][log.py>_log] ==> #46500      Episodes Collected: 11052      Transitions Collected: 302756     Batch Size: 256   | NewEpisode Model(mean:46389     ) Reward(mean:1.45 , max:2.63 , min:0.33 , std:0.53 ) | total_loss: 1.134   reward_loss: 0.024   policy_loss: 1.945   value_loss: 0.302   consistency_loss: -0.455  lr: 0.000100  batch_future_return: 0.701   batch_model_diff: 19805.078target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:07:16,152][train][INFO][log.py>_log] ==> #46600      Episodes Collected: 11077      Transitions Collected: 303454     Batch Size: 256   | NewEpisode Model(mean:46492     ) Reward(mean:1.54 , max:3.62 , min:0.33 , std:0.66 ) | total_loss: 0.877   reward_loss: 0.014   policy_loss: 1.362   value_loss: 0.202   consistency_loss: -0.275  lr: 0.000100  batch_future_return: 0.699   batch_model_diff: 18855.859target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:07:41,292][train][INFO][log.py>_log] ==> #46700      Episodes Collected: 11105      Transitions Collected: 304213     Batch Size: 256   | NewEpisode Model(mean:46591     ) Reward(mean:1.57 , max:2.96 , min:0.66 , std:0.63 ) | total_loss: 0.460   reward_loss: 0.008   policy_loss: 0.696   value_loss: 0.097   consistency_loss: -0.134  lr: 0.000100  batch_future_return: 0.740   batch_model_diff: 18783.984target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:07:41,294][train][WARNING][train.py>_train] ==> #46700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:08:06,308][train][INFO][log.py>_log] ==> #46800      Episodes Collected: 11133      Transitions Collected: 304940     Batch Size: 256   | NewEpisode Model(mean:46693     ) Reward(mean:1.57 , max:2.96 , min:0.66 , std:0.60 ) | total_loss: 1.170   reward_loss: 0.036   policy_loss: 2.019   value_loss: 0.330   consistency_loss: -0.484  lr: 0.000100  batch_future_return: 0.710   batch_model_diff: 17826.172target_model_diff: 200.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:08:31,224][train][INFO][log.py>_log] ==> #46900      Episodes Collected: 11160      Transitions Collected: 305643     Batch Size: 256   | NewEpisode Model(mean:46793     ) Reward(mean:1.55 , max:2.96 , min:0.33 , std:0.73 ) | total_loss: 1.103   reward_loss: 0.034   policy_loss: 1.786   value_loss: 0.280   consistency_loss: -0.393  lr: 0.000100  batch_future_return: 0.713   batch_model_diff: 19343.359target_model_diff: 100.000 Tp_perstep: 0.186   Tu_perstep: 0.049   
[2025-06-02 23:08:43,895][train][WARNING][train.py>_train] ==> #46950 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:08:57,012][train][INFO][log.py>_log] ==> #47000      Episodes Collected: 11187      Transitions Collected: 306420     Batch Size: 256   | NewEpisode Model(mean:46895     ) Reward(mean:1.53 , max:2.63 , min:0.33 , std:0.54 ) | total_loss: 1.745   reward_loss: 0.045   policy_loss: 2.758   value_loss: 0.414   consistency_loss: -0.581  lr: 0.000100  batch_future_return: 0.800   batch_model_diff: 18033.984target_model_diff: 200.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:09:21,798][train][INFO][log.py>_log] ==> #47100      Episodes Collected: 11212      Transitions Collected: 307124     Batch Size: 256   | NewEpisode Model(mean:46994     ) Reward(mean:1.43 , max:2.63 , min:0.33 , std:0.53 ) | total_loss: 1.014   reward_loss: 0.026   policy_loss: 1.709   value_loss: 0.277   consistency_loss: -0.395  lr: 0.000100  batch_future_return: 0.750   batch_model_diff: 19646.875target_model_diff: 100.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:09:46,680][train][INFO][log.py>_log] ==> #47200      Episodes Collected: 11240      Transitions Collected: 307817     Batch Size: 256   | NewEpisode Model(mean:47092     ) Reward(mean:1.27 , max:2.63 , min:0.33 , std:0.57 ) | total_loss: 1.123   reward_loss: 0.030   policy_loss: 1.836   value_loss: 0.297   consistency_loss: -0.409  lr: 0.000100  batch_future_return: 0.717   batch_model_diff: 19898.828target_model_diff: 200.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:10:11,110][train][INFO][log.py>_log] ==> #47300      Episodes Collected: 11264      Transitions Collected: 308488     Batch Size: 256   | NewEpisode Model(mean:47192     ) Reward(mean:1.37 , max:2.30 , min:0.33 , std:0.53 ) | total_loss: 1.292   reward_loss: 0.031   policy_loss: 2.144   value_loss: 0.354   consistency_loss: -0.485  lr: 0.000100  batch_future_return: 0.695   batch_model_diff: 19456.250target_model_diff: 100.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:10:23,306][train][WARNING][train.py>_train] ==> #47350 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:10:36,358][train][INFO][log.py>_log] ==> #47400      Episodes Collected: 11293      Transitions Collected: 309306     Batch Size: 256   | NewEpisode Model(mean:47290     ) Reward(mean:1.39 , max:2.30 , min:0.66 , std:0.47 ) | total_loss: 0.531   reward_loss: 0.012   policy_loss: 0.850   value_loss: 0.130   consistency_loss: -0.182  lr: 0.000100  batch_future_return: 0.728   batch_model_diff: 19777.344target_model_diff: 200.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:10:49,032][train][WARNING][train.py>_train] ==> #47450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:11:01,207][train][INFO][log.py>_log] ==> #47500      Episodes Collected: 11320      Transitions Collected: 310004     Batch Size: 256   | NewEpisode Model(mean:47394     ) Reward(mean:1.57 , max:2.96 , min:0.66 , std:0.58 ) | total_loss: 1.316   reward_loss: 0.034   policy_loss: 2.193   value_loss: 0.338   consistency_loss: -0.497  lr: 0.000100  batch_future_return: 0.692   batch_model_diff: 20172.266target_model_diff: 100.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:11:01,209][train][WARNING][train.py>_train] ==> #47500 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:11:13,304][train][WARNING][train.py>_train] ==> #47550 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:11:26,014][train][INFO][log.py>_log] ==> #47600      Episodes Collected: 11345      Transitions Collected: 310722     Batch Size: 256   | NewEpisode Model(mean:47493     ) Reward(mean:1.57 , max:4.99 , min:0.66 , std:0.83 ) | total_loss: 1.345   reward_loss: 0.033   policy_loss: 2.107   value_loss: 0.319   consistency_loss: -0.438  lr: 0.000100  batch_future_return: 0.799   batch_model_diff: 20735.156target_model_diff: 200.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:11:38,227][train][WARNING][train.py>_train] ==> #47650 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:11:50,467][train][INFO][log.py>_log] ==> #47700      Episodes Collected: 11372      Transitions Collected: 311394     Batch Size: 256   | NewEpisode Model(mean:47594     ) Reward(mean:1.40 , max:2.30 , min:0.00 , std:0.56 ) | total_loss: 1.535   reward_loss: 0.038   policy_loss: 2.458   value_loss: 0.361   consistency_loss: -0.525  lr: 0.000100  batch_future_return: 0.773   batch_model_diff: 18320.312target_model_diff: 100.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:12:15,787][train][INFO][log.py>_log] ==> #47800      Episodes Collected: 11398      Transitions Collected: 312163     Batch Size: 256   | NewEpisode Model(mean:47693     ) Reward(mean:1.58 , max:2.63 , min:0.66 , std:0.45 ) | total_loss: 2.196   reward_loss: 0.047   policy_loss: 3.436   value_loss: 0.511   consistency_loss: -0.707  lr: 0.000100  batch_future_return: 0.780   batch_model_diff: 20918.359target_model_diff: 200.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:12:40,619][train][INFO][log.py>_log] ==> #47900      Episodes Collected: 11424      Transitions Collected: 312868     Batch Size: 256   | NewEpisode Model(mean:47794     ) Reward(mean:1.53 , max:3.95 , min:0.66 , std:0.77 ) | total_loss: 0.296   reward_loss: 0.007   policy_loss: 0.518   value_loss: 0.088   consistency_loss: -0.126  lr: 0.000100  batch_future_return: 0.769   batch_model_diff: 18302.734target_model_diff: 100.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:12:40,621][train][WARNING][train.py>_train] ==> #47900 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:13:05,447][train][INFO][log.py>_log] ==> #48000      Episodes Collected: 11449      Transitions Collected: 313558     Batch Size: 256   | NewEpisode Model(mean:47891     ) Reward(mean:1.51 , max:3.29 , min:0.99 , std:0.51 ) | total_loss: 0.722   reward_loss: 0.021   policy_loss: 1.161   value_loss: 0.179   consistency_loss: -0.252  lr: 0.000100  batch_future_return: 0.749   batch_model_diff: 20619.531target_model_diff: 200.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:13:05,449][train][WARNING][train.py>_train] ==> #48000 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:13:18,019][train][WARNING][train.py>_train] ==> #48050 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:13:30,189][train][INFO][log.py>_log] ==> #48100      Episodes Collected: 11475      Transitions Collected: 314258     Batch Size: 256   | NewEpisode Model(mean:47990     ) Reward(mean:1.47 , max:2.96 , min:0.33 , std:0.65 ) | total_loss: 1.672   reward_loss: 0.041   policy_loss: 2.726   value_loss: 0.431   consistency_loss: -0.601  lr: 0.000100  batch_future_return: 0.713   batch_model_diff: 20085.547target_model_diff: 100.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:13:30,191][train][WARNING][train.py>_train] ==> #48100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:13:42,463][train][WARNING][train.py>_train] ==> #48150 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:13:55,117][train][INFO][log.py>_log] ==> #48200      Episodes Collected: 11503      Transitions Collected: 315016     Batch Size: 256   | NewEpisode Model(mean:48091     ) Reward(mean:1.46 , max:2.96 , min:0.66 , std:0.59 ) | total_loss: 1.871   reward_loss: 0.039   policy_loss: 2.922   value_loss: 0.431   consistency_loss: -0.599  lr: 0.000100  batch_future_return: 0.767   batch_model_diff: 20582.031target_model_diff: 200.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:13:55,119][train][WARNING][train.py>_train] ==> #48200 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:14:19,975][train][INFO][log.py>_log] ==> #48300      Episodes Collected: 11529      Transitions Collected: 315733     Batch Size: 256   | NewEpisode Model(mean:48193     ) Reward(mean:1.53 , max:2.96 , min:0.66 , std:0.68 ) | total_loss: 1.221   reward_loss: 0.031   policy_loss: 1.969   value_loss: 0.291   consistency_loss: -0.426  lr: 0.000100  batch_future_return: 0.722   batch_model_diff: 19944.922target_model_diff: 100.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:14:19,977][train][WARNING][train.py>_train] ==> #48300 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:14:45,265][train][INFO][log.py>_log] ==> #48400      Episodes Collected: 11556      Transitions Collected: 316444     Batch Size: 256   | NewEpisode Model(mean:48294     ) Reward(mean:1.53 , max:2.63 , min:0.99 , std:0.44 ) | total_loss: 0.534   reward_loss: 0.016   policy_loss: 0.882   value_loss: 0.144   consistency_loss: -0.200  lr: 0.000100  batch_future_return: 0.738   batch_model_diff: 20032.031target_model_diff: 200.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:14:45,267][train][WARNING][train.py>_train] ==> #48400 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:14:58,423][train][WARNING][train.py>_train] ==> #48450 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:15:11,183][train][INFO][log.py>_log] ==> #48500      Episodes Collected: 11586      Transitions Collected: 317234     Batch Size: 256   | NewEpisode Model(mean:48391     ) Reward(mean:1.49 , max:2.96 , min:0.00 , std:0.51 ) | total_loss: 1.441   reward_loss: 0.039   policy_loss: 2.335   value_loss: 0.372   consistency_loss: -0.512  lr: 0.000100  batch_future_return: 0.740   batch_model_diff: 20842.969target_model_diff: 100.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:15:35,961][train][INFO][log.py>_log] ==> #48600      Episodes Collected: 11613      Transitions Collected: 317952     Batch Size: 256   | NewEpisode Model(mean:48493     ) Reward(mean:1.46 , max:2.96 , min:0.33 , std:0.56 ) | total_loss: 0.789   reward_loss: 0.021   policy_loss: 1.267   value_loss: 0.194   consistency_loss: -0.274  lr: 0.000100  batch_future_return: 0.808   batch_model_diff: 18927.734target_model_diff: 200.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:16:01,194][train][INFO][log.py>_log] ==> #48700      Episodes Collected: 11639      Transitions Collected: 318682     Batch Size: 256   | NewEpisode Model(mean:48593     ) Reward(mean:1.42 , max:2.30 , min:0.66 , std:0.51 ) | total_loss: 0.630   reward_loss: 0.012   policy_loss: 1.019   value_loss: 0.157   consistency_loss: -0.220  lr: 0.000100  batch_future_return: 0.795   batch_model_diff: 19097.656target_model_diff: 100.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:16:01,196][train][WARNING][train.py>_train] ==> #48700 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:16:26,972][train][INFO][log.py>_log] ==> #48800      Episodes Collected: 11669      Transitions Collected: 319471     Batch Size: 256   | NewEpisode Model(mean:48692     ) Reward(mean:1.32 , max:2.63 , min:0.33 , std:0.56 ) | total_loss: 0.884   reward_loss: 0.018   policy_loss: 1.401   value_loss: 0.220   consistency_loss: -0.295  lr: 0.000100  batch_future_return: 0.773   batch_model_diff: 21469.922target_model_diff: 200.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:16:52,621][train][INFO][log.py>_log] ==> #48900      Episodes Collected: 11698      Transitions Collected: 320257     Batch Size: 256   | NewEpisode Model(mean:48793     ) Reward(mean:1.34 , max:3.62 , min:0.33 , std:0.66 ) | total_loss: 1.311   reward_loss: 0.033   policy_loss: 2.122   value_loss: 0.329   consistency_loss: -0.463  lr: 0.000100  batch_future_return: 0.697   batch_model_diff: 21489.844target_model_diff: 100.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:17:18,067][train][INFO][log.py>_log] ==> #49000      Episodes Collected: 11727      Transitions Collected: 320989     Batch Size: 256   | NewEpisode Model(mean:48894     ) Reward(mean:1.50 , max:2.30 , min:0.33 , std:0.43 ) | total_loss: 1.038   reward_loss: 0.030   policy_loss: 1.825   value_loss: 0.302   consistency_loss: -0.446  lr: 0.000100  batch_future_return: 0.672   batch_model_diff: 20299.219target_model_diff: 200.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:17:43,109][train][INFO][log.py>_log] ==> #49100      Episodes Collected: 11753      Transitions Collected: 321701     Batch Size: 256   | NewEpisode Model(mean:48992     ) Reward(mean:1.50 , max:2.30 , min:0.66 , std:0.45 ) | total_loss: 1.871   reward_loss: 0.046   policy_loss: 3.030   value_loss: 0.467   consistency_loss: -0.661  lr: 0.000100  batch_future_return: 0.722   batch_model_diff: 19288.672target_model_diff: 100.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:17:43,111][train][WARNING][train.py>_train] ==> #49100 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:18:09,498][train][INFO][log.py>_log] ==> #49200      Episodes Collected: 11781      Transitions Collected: 322484     Batch Size: 256   | NewEpisode Model(mean:49091     ) Reward(mean:1.30 , max:2.30 , min:0.66 , std:0.49 ) | total_loss: 0.478   reward_loss: 0.010   policy_loss: 0.785   value_loss: 0.123   consistency_loss: -0.174  lr: 0.000100  batch_future_return: 0.748   batch_model_diff: 20496.875target_model_diff: 200.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:18:21,663][train][WARNING][train.py>_train] ==> #49250 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:18:34,371][train][INFO][log.py>_log] ==> #49300      Episodes Collected: 11807      Transitions Collected: 323227     Batch Size: 256   | NewEpisode Model(mean:49193     ) Reward(mean:1.28 , max:2.63 , min:0.00 , std:0.59 ) | total_loss: 0.905   reward_loss: 0.018   policy_loss: 1.433   value_loss: 0.226   consistency_loss: -0.301  lr: 0.000100  batch_future_return: 0.760   batch_model_diff: 20000.000target_model_diff: 100.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:18:59,520][train][INFO][log.py>_log] ==> #49400      Episodes Collected: 11835      Transitions Collected: 323957     Batch Size: 256   | NewEpisode Model(mean:49292     ) Reward(mean:1.42 , max:2.63 , min:0.33 , std:0.48 ) | total_loss: 0.631   reward_loss: 0.015   policy_loss: 1.074   value_loss: 0.178   consistency_loss: -0.251  lr: 0.000100  batch_future_return: 0.758   batch_model_diff: 21700.781target_model_diff: 200.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:18:59,523][train][WARNING][train.py>_train] ==> #49400 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:19:24,720][train][INFO][log.py>_log] ==> #49500      Episodes Collected: 11860      Transitions Collected: 324696     Batch Size: 256   | NewEpisode Model(mean:49391     ) Reward(mean:1.38 , max:2.30 , min:0.33 , std:0.47 ) | total_loss: 1.397   reward_loss: 0.030   policy_loss: 2.158   value_loss: 0.326   consistency_loss: -0.436  lr: 0.000100  batch_future_return: 0.808   batch_model_diff: 19017.578target_model_diff: 100.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:19:24,722][train][WARNING][train.py>_train] ==> #49500 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:19:51,041][train][INFO][log.py>_log] ==> #49600      Episodes Collected: 11887      Transitions Collected: 325514     Batch Size: 256   | NewEpisode Model(mean:49493     ) Reward(mean:1.34 , max:2.96 , min:0.33 , std:0.58 ) | total_loss: 2.518   reward_loss: 0.092   policy_loss: 4.247   value_loss: 0.676   consistency_loss: -0.995  lr: 0.000100  batch_future_return: 0.781   batch_model_diff: 20216.016target_model_diff: 200.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:20:16,196][train][INFO][log.py>_log] ==> #49700      Episodes Collected: 11912      Transitions Collected: 326211     Batch Size: 256   | NewEpisode Model(mean:49594     ) Reward(mean:1.38 , max:2.96 , min:0.33 , std:0.60 ) | total_loss: 1.293   reward_loss: 0.030   policy_loss: 2.064   value_loss: 0.326   consistency_loss: -0.441  lr: 0.000100  batch_future_return: 0.750   batch_model_diff: 20574.609target_model_diff: 100.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:20:41,604][train][INFO][log.py>_log] ==> #49800      Episodes Collected: 11941      Transitions Collected: 326964     Batch Size: 256   | NewEpisode Model(mean:49690     ) Reward(mean:1.35 , max:2.30 , min:0.00 , std:0.56 ) | total_loss: 1.274   reward_loss: 0.040   policy_loss: 2.061   value_loss: 0.319   consistency_loss: -0.454  lr: 0.000100  batch_future_return: 0.720   batch_model_diff: 21596.094target_model_diff: 200.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:20:53,741][train][WARNING][train.py>_train] ==> #49850 Batch Queue is empty (Require more reanalyze actors Or actor fails).
[2025-06-02 23:21:06,390][train][INFO][log.py>_log] ==> #49900      Episodes Collected: 11966      Transitions Collected: 327671     Batch Size: 256   | NewEpisode Model(mean:49790     ) Reward(mean:1.38 , max:2.30 , min:0.66 , std:0.48 ) | total_loss: 1.699   reward_loss: 0.034   policy_loss: 2.560   value_loss: 0.368   consistency_loss: -0.494  lr: 0.000100  batch_future_return: 0.839   batch_model_diff: 19443.359target_model_diff: 100.000 Tp_perstep: 0.187   Tu_perstep: 0.049   
[2025-06-02 23:22:01,197][train][INFO][train.py>train] ==> Training over...
